


################################################## vertexai_supervised_tuning_token_count_and_cost_estimation.md ##################################################


```
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# Vertex AI Supervised tuning token count and cost estimation.

<table align="left">
  <td style="text-align: center">
    <a href="https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/vertexai_supervised_tuning_token_count_and_cost_estimation.ipynb">
      <img src="https://cloud.google.com/ml-engine/images/colab-logo-32px.png" alt="Google Colaboratory logo"><br> Open in Colab
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Ftuning%2Fvertexai_supervised_tuning_token_count_and_cost_estimation.ipynb">
      <img width="32px" src="https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png" alt="Google Cloud Colab Enterprise logo"><br> Open in Colab Enterprise
    </a>
  </td>    
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/tuning/vertexai_supervised_tuning_token_count_and_cost_estimation.ipynb">
      <img src="https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32" alt="Vertex AI logo"><br> Open in Workbench
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/vertexai_supervised_tuning_token_count_and_cost_estimation.ipynb">
      <img src="https://cloud.google.com/ml-engine/images/github-logo-32px.png" alt="GitHub logo"><br> View on GitHub
    </a>
  </td>
</table>

| | |
|-|-|
| Author(s) | [Lehui Liu](https://github.com/liulehui), [Erwin Huizenga](https://github.com/Huize501) |

## Overview

This notebook serves as a tool to preprocess and estimate token counts for tuning costs for tuning [`gemini-1.5-pro-002`](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning).

At the end you will also find the code to preprocess and estimate token counts for tuning costs for tuning `gemini-1.0-pro-002`. If you get started please start with `gemini-1.5-pro-002`.

For how to prepare dataset for tuning gemini, please refer to this [tutorial](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-about).

## Get started

### Install Vertex AI SDK and other required packages



```
%pip install --upgrade --user --quiet google-cloud-aiplatform[tokenization] numpy==1.26.4 tensorflow
```

### Restart runtime

To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.

The restart might take a minute or longer. After it's restarted, continue to the next step.


```
import IPython

app = IPython.Application.instance()
app.kernel.do_shutdown(True)
```




    {'status': 'ok', 'restart': True}



<div class="alert alert-block alert-warning">
<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>
</div>


### Authenticate your notebook environment (Colab only)

If you're running this notebook on Google Colab, run the cell below to authenticate your environment.


```
import sys

if "google.colab" in sys.modules:
    from google.colab import auth

    auth.authenticate_user()
```

### Set Google Cloud project information and initialize Vertex AI SDK

To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).

Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).


```
PROJECT_ID = "[your-project-id]"  # @param {type:"string"}
LOCATION = "us-central1"  # @param {type:"string"}


import vertexai

vertexai.init(project=PROJECT_ID, location=LOCATION)
```

## Tuning token count and cost estimation: `Gemini 1.5 Pro` and `Gemini 1.5 Flash`

### Import libraries


```
from collections import defaultdict
import dataclasses
import json

from google.cloud import storage
import numpy as np
import tensorflow as tf
from vertexai.generative_models import Content, Part
from vertexai.preview.tokenization import get_tokenizer_for_model
```

### Load the dataset

This example is for text only. Define the Google Cloud Storage URIs pointing to your training and validation datasets or continue using the URIs provided.


```
BASE_MODEL = "gemini-1.5-pro-002"  # @param ['gemini-1.5-pro-002']{type:"string"}
training_dataset_uri = "gs://github-repo/generative-ai/gemini/tuning/train_sft_train_samples.jsonl"  # @param {type:"string"}
validation_dataset_uri = "gs://github-repo/generative-ai/gemini/tuning/val_sft_val_samples.jsonl"  # @param {type:"string"}

tokenizer = get_tokenizer_for_model("gemini-1.5-pro-001")
```

We'll now load the dataset and conduct some basic statistical analysis to understand its structure and content.



```
example_training_dataset = []
example_validation_dataset = []

try:
    with tf.io.gfile.GFile(training_dataset_uri) as dataset_jsonl_file:
        example_training_dataset = [
            json.loads(dataset_line) for dataset_line in dataset_jsonl_file
        ]
except KeyError as e:
    print(
        f"KeyError: Please check if your file '{training_dataset_uri}' is a JSONL file with correct JSON format. Error: {e}"
    )
    # Exit the script if there's an error in the training data
    import sys

    sys.exit(1)

print()

if validation_dataset_uri:
    try:
        with tf.io.gfile.GFile(validation_dataset_uri) as dataset_jsonl_file:
            example_validation_dataset = [
                json.loads(dataset_line) for dataset_line in dataset_jsonl_file
            ]
    except KeyError as e:
        print(
            f"KeyError: Please check if your file '{validation_dataset_uri}' is a JSONL file with correct JSON format. Error: {e}"
        )
        # Exit the script if there's an error in the validation data
        import sys

        sys.exit(1)

# Initial dataset stats
print("Num training examples:", len(example_training_dataset))
if example_training_dataset:  # Check if the list is not empty
    print("First example:")
    for item in example_training_dataset[0]["contents"]:
        print(item)
        text_content = item.get("parts", [{}])[0].get("text", "")
        print(tokenizer.count_tokens(text_content))  # Make sure 'tokenizer' is defined

if example_validation_dataset:
    print("Num validation examples:", len(example_validation_dataset))
```

    
    Num training examples: 500
    First example:
    {'role': 'user', 'parts': [{'text': 'Honesty is usually the best policy. It is disrespectful to lie to someone. If you don\'t want to date someone, you should say so.  Sometimes it is easy to be honest. For example, you might be able to truthfully say, "No, thank you, I already have a date for that party." Other times, you might need to find a kinder way to be nice. Maybe you are not attracted to the person. Instead of bluntly saying that, try saying, "No, thank you, I just don\'t think we would be a good fit." Avoid making up a phony excuse. For instance, don\'t tell someone you will be out of town this weekend if you won\'t be. There\'s a chance that you might then run into them at the movies, which would definitely cause hurt feelings. A compliment sandwich is a really effective way to provide feedback. Essentially, you "sandwich" your negative comment between two positive things. Try using this method when you need to reject someone.  An example of a compliment sandwich is to say something such as, "You\'re an awesome person. Unfortunately, I\'m not interested in dating you. Someone else is going to be really lucky to date someone with such a great personality!" You could also try, "You are a really nice person. I\'m only interested you as a friend. I like when we hang out in big groups together!" Be sincere. If you offer false compliments, the other person will likely be able to tell and feel hurt. If you do not want to date someone, it is best to be upfront about your feelings. Do not beat around the bush. If your mind is made up, it is best to clearly state your response.  If someone asks you to date them and you don\'t want to, you can be direct and kind at the same time. State your answer clearly. You can make your feelings clear without purposefully hurting someone else\'s feelings. Try smiling and saying, "That sounds fun, but no thank you. I\'m not interested in dating you." Don\'t beat around the bush. If you do not want to accept the date, there is no need to say, "Let me think about it." It is best to get the rejection over with. You don\'t want to give someone false hope. Avoid saying something like, "Let me check my schedule and get back to you." Try to treat the person the way you would want to be treated. This means that you should choose your words carefully. Be thoughtful in your response.  It\'s okay to pause before responding. You might be taken by surprise and need a moment to collect your thoughts. Say thank you. It is a compliment to be asked out. You can say, "I\'m flattered. Unfortunately, I can\'t accept." Don\'t laugh. Many people laugh nervously in awkward situations. Try to avoid giggling, as that is likely to result in hurt feelings. Sometimes it is not what you say, but how you say it. If you need to reject someone, think about factors other than your words. Non-verbal communication matters, too.  Use the right tone of voice. Try to sound gentle but firm. Make eye contact. This helps convey that you are being serious, and also shows respect for the other person. If you are in public, try not to speak too loudly. It is not necessary for everyone around you to know that you are turning down a date.\n\nProvide a summary of the article in two or three sentences:\n\n'}]}
    CountTokensResult(total_tokens=730)
    {'role': 'model', 'parts': [{'text': 'Tell the truth. Use a "compliment sandwich". Be direct. Treat the person with respect. Communicate effectively.'}]}
    CountTokensResult(total_tokens=23)
    Num validation examples: 100
    

You can perform various error checks to validate that each tuning example in the dataset adheres to the format expected by the tuning API. Errors are categorized based on their nature for easier debugging.  
  
For how to prepare dataset for tuning gemini, please refer to this [tutorial](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-about).

1. **Presence of System Instruction**: Checks if there is a system instruction and if its there for all rows. System instruction is optional. Warning type: `systemInstruction is missing in some rows`.
2. **Presence of Contents List:** Checks if a `contents` list is present in each entry. Error type: `missing_contents_list`.
3. **Content Item Format:** Validates that each item in the `contents` list is a dictionary. Error type: `invalid_content_item`.
4. **Content Item Format:** Validates that each item in the `contents` list is a dictionary. Error type: `invalid_content_item`.
5. **Role Validation:** Checks if the role is one of `user`, or `model` for `contents` list and system for `systemInstruction` list. Error type: `unrecognized_role`.
6. **Parts List Validation:** Verifies that the `parts` key contains a list. Error type: `missing_or_invalid_parts`.
7. **Part Format:** Checks if each part in the `parts` list is a dictionary and contains the key `text`. Error type: `invalid_part`.
8. **Text Validation:** Ensures that the `text` key has textual data and is a string. Error type: `missing_text`.
9. **Consecutive Turns:** For the chat history, it is enforced that the message roles alternate (user, then model, then user, etc.). Error type: `consecutive_turns`. This check is not applicable for systemInstruction.



```
from collections import defaultdict


def validate_dataset_format(dataset):
    """Validates the dataset.

    Args:
      dataset_uri: The dataset uri to be validated.
    """
    format_errors = defaultdict(list)
    system_instruction_missing = False  # Flag to track missing systemInstruction

    if not dataset or len(dataset) == 0:
        print("Input dataset file is empty or inaccessible.")
        return

    for row_idx, example in enumerate(dataset):
        # Verify presence of contents list
        if not isinstance(example, dict):
            format_errors["invalid_input"].append(row_idx)
            continue

        # Check for systemInstruction and validate if present
        system_instruction = example.get("systemInstruction", None)
        if system_instruction:
            try:
                # Validate the list within "parts"
                validate_contents(
                    system_instruction.get("parts", []),
                    format_errors,
                    row_idx,
                    is_system_instruction=True,
                )
            except (TypeError, AttributeError, KeyError) as e:
                print("Invalid input during system instruction validation: %s", e)
                format_errors["invalid_system_instruction"].append(row_idx)
        else:
            system_instruction_missing = True  # Set the flag if missing

        contents = example.get("contents", None)
        if not contents:
            format_errors["missing_contents_list"].append(row_idx)
            continue
        try:
            validate_contents(contents, format_errors, row_idx)
        except (TypeError, AttributeError, KeyError) as e:
            print("Invalid input during contents validation: %s", e)
            format_errors["invalid_input"].append(row_idx)

    if format_errors:
        print("Found errors for this dataset:")
        for k, v in format_errors.items():
            print(f"{k}: {v}")
    else:
        print("No errors found for this dataset.")

    # Print warning only once after processing all rows
    if system_instruction_missing:
        print("Warning: systemInstruction is missing in some rows.")


def validate_contents(contents, format_errors, row_index, is_system_instruction=False):
    """Validates contents list format."""

    if not isinstance(contents, list):
        format_errors["invalid_contents_list"].append(row_index)
        return

    prev_role = None
    for content_item in contents:  # Iterate over content items in the "contents" list
        if not isinstance(content_item, dict):
            format_errors["invalid_content_item"].append(row_index)
            return

        # Skip key checks for system instructions
        if not is_system_instruction and (
            "role" not in content_item or "parts" not in content_item
        ):
            format_errors["content_item_missing_key"].append(row_index)
            return

        # ... (rest of the validation logic remains the same)
```


```
validate_dataset_format(example_training_dataset)
if example_validation_dataset:
    validate_dataset_format(example_validation_dataset)
```

    No errors found for this dataset.
    Warning: systemInstruction is missing in some rows.
    No errors found for this dataset.
    Warning: systemInstruction is missing in some rows.
    

### Utils for dataset analysis and token counting

This section focuses on analyzing the structure and token counts of your datasets. You will also define some utility functions to streamline subsequent steps in the notebook.

* Load and inspect sample data from the training and validation datasets.
* Calculate token counts for messages to understand the dataset's characteristics.
* Define utility functions for calculating token distributions and dataset statistics. These will help assess the suitability of your data for supervised tuning and estimate potential costs.


```
@dataclasses.dataclass
class DatasetDistribution:
    """Dataset disbribution for given a population of values.

    It optionally contains a histogram consists of bucketized data representing
    the distribution of those values. The summary statistics are the sum, min,
    max, mean, median, p5, p95.

    Attributes:
      sum: Sum of the values in the population.
      max: Max of the values in the population.
      min: Min of the values in the population.
      mean: The arithmetic mean of the values in the population.
      median: The median of the values in the population.
      p5: P5 quantile of the values in the population.
      p95: P95 quantile of the values in the population.
    """

    sum: int | None = None
    max: float | None = None
    min: float | None = None
    mean: float | None = None
    median: float | None = None
    p5: float | None = None
    p95: float | None = None


@dataclasses.dataclass
class DatasetStatistics:
    """Dataset statistics used for dataset profiling.

    Attributes:
      total_number_of_dataset_examples: Number of tuning examples in the dataset.
      total_number_of_records_for_training: Number of tuning records after
        formatting. Each model turn in the chat message will be considered as a record for tuning.
      total_number_of_billable_tokens: Number of total billable tokens in the
        dataset.
      user_input_token_length_stats: Stats for input token length.
      user_output_token_length_stats: Stats for output token length.
    """

    total_number_of_dataset_examples: int | None = None
    total_number_of_records_for_training: int | None = None
    total_number_of_billable_tokens: int | None = None
    user_input_token_length_stats: DatasetDistribution | None = None
    user_output_token_length_stats: DatasetDistribution | None = None


MAX_TOKENS_PER_EXAMPLE = 32 * 1024
ESTIMATE_PADDING_TOKEN_PER_EXAMPLE = 8
```


```
def calculate_distribution_for_population(population) -> DatasetDistribution:
    """Calculates the distribution from the population of values.

    Args:
      population: The population of values to calculate distribution for.

    Returns:
      DatasetDistribution of the given population of values.
    """
    if not population:
        raise ValueError("population is empty")

    return DatasetDistribution(
        sum=np.sum(population),
        max=np.max(population),
        min=np.min(population),
        mean=np.mean(population),
        median=np.median(population),
        p5=np.percentile(population, 5, method="nearest"),
        p95=np.percentile(population, 95, method="nearest"),
    )


def get_token_distribution_for_one_tuning_dataset_example(example):
    model_turn_token_list = []
    input_token_list = []
    input = []
    n_too_long = 0
    number_of_records_for_training = 0  # each model turn in the chat message will be considered as a record for tuning

    # Handle optional systemInstruction
    system_instruction = example.get("systemInstruction")
    if system_instruction:
        text = system_instruction.get("parts")[0].get(
            "text"
        )  # Assuming single part in system instruction
        input.append(Content(role="system", parts=[Part.from_text(text)]))

    for content_item in example["contents"]:
        role = content_item.get("role").lower()
        text = content_item.get("parts")[0].get(
            "text"
        )  # Assuming single part in content item

        if role.lower() == "model":
            result = tokenizer.count_tokens(input)
            input_token_list.append(result.total_tokens)
            model_turn_token_list.append(tokenizer.count_tokens(text).total_tokens)
            number_of_records_for_training += 1
            if (
                result.total_tokens + tokenizer.count_tokens(text).total_tokens
                > MAX_TOKENS_PER_EXAMPLE
            ):
                n_too_long += 1
                break

        input.append(Content(role=role, parts=[Part.from_text(text)]))

    return (
        input_token_list,
        model_turn_token_list,
        number_of_records_for_training,
        np.sum(model_turn_token_list) + np.sum(input_token_list),
        n_too_long,
    )


def get_dataset_stats_for_dataset(dataset):
    results = map(get_token_distribution_for_one_tuning_dataset_example, dataset)
    user_input_token_list = []
    model_turn_token_list = []
    number_of_records_for_training = 0
    total_number_of_billable_tokens = 0
    n_too_long_for_dataset = 0
    for (
        input_token_list_per_example,
        model_turn_token_list_per_example,
        number_of_records_for_training_per_example,
        number_of_billable_token_per_example,
        n_too_long,
    ) in results:
        user_input_token_list.extend(input_token_list_per_example)
        model_turn_token_list.extend(model_turn_token_list_per_example)
        number_of_records_for_training += number_of_records_for_training_per_example
        total_number_of_billable_tokens += number_of_billable_token_per_example
        n_too_long_for_dataset += n_too_long

    print(
        f"\n{n_too_long_for_dataset} examples may be over the {MAX_TOKENS_PER_EXAMPLE} token limit, they will be truncated during tuning."
    )

    return DatasetStatistics(
        total_number_of_dataset_examples=len(dataset),
        total_number_of_records_for_training=number_of_records_for_training,
        total_number_of_billable_tokens=total_number_of_billable_tokens
        + number_of_records_for_training * ESTIMATE_PADDING_TOKEN_PER_EXAMPLE,
        user_input_token_length_stats=calculate_distribution_for_population(
            user_input_token_list
        ),
        user_output_token_length_stats=calculate_distribution_for_population(
            model_turn_token_list
        ),
    )


def print_dataset_stats(dataset):
    dataset_stats = get_dataset_stats_for_dataset(dataset)
    print("Below you can find the dataset statistics:")
    print(
        f"Total number of examples in the dataset: {dataset_stats.total_number_of_dataset_examples}"
    )
    print(
        f"Total number of records for training: {dataset_stats.total_number_of_records_for_training}"
    )
    print(
        f"Total number of billable tokens in the dataset: {dataset_stats.total_number_of_billable_tokens}"
    )
    print(
        f"User input token length distribution: {dataset_stats.user_input_token_length_stats}"
    )
    print(
        f"User output token length distribution: {dataset_stats.user_output_token_length_stats}"
    )
    return dataset_stats
```

Next you can analyze the structure and token counts of your datasets.


```
training_dataset_stats = print_dataset_stats(example_training_dataset)

if example_validation_dataset:
    validation_dataset_stats = print_dataset_stats(example_validation_dataset)
```

    
    0 examples may be over the 32768 token limit, they will be truncated during tuning.
    Below you can find the dataset statistics:
    Total number of examples in the dataset: 500
    Total number of records for training: 500
    Total number of billable tokens in the dataset: 259243
    User input token length distribution: DatasetDistribution(sum=233592, max=2932, min=25, mean=467.184, median=414.5, p5=101, p95=1002)
    User output token length distribution: DatasetDistribution(sum=21651, max=237, min=3, mean=43.302, median=37.0, p5=15, p95=89)
    
    0 examples may be over the 32768 token limit, they will be truncated during tuning.
    Below you can find the dataset statistics:
    Total number of examples in the dataset: 100
    Total number of records for training: 100
    Total number of billable tokens in the dataset: 50154
    User input token length distribution: DatasetDistribution(sum=45535, max=1418, min=29, mean=455.35, median=413.5, p5=145, p95=846)
    User output token length distribution: DatasetDistribution(sum=3819, max=165, min=8, mean=38.19, median=32.0, p5=17, p95=76)
    

### Cost Estimation for Supervised Fine-tuning
In this final section, you will estimate the total cost for supervised fine-tuning based on the number of tokens processed. The number of tokens used will be charged to you. Please refer to the [pricing page for the rate](https://cloud.google.com/vertex-ai/generative-ai/pricing#gemini-models).

**Important Note:** The final cost may vary slightly from this estimate due to dataset formatting and truncation logic during training.

The code calculates the total number of billable tokens by summing up the tokens from the training dataset and (if provided) the validation dataset. Then, it estimates the total cost by multiplying the total billable tokens with the number of training epochs (default is 4).


```
epoch_count = 4  # @param {type:"integer"}
if epoch_count is None:
    epoch_count = 4


total_number_of_billable_tokens = training_dataset_stats.total_number_of_billable_tokens


if validation_dataset_stats:
    total_number_of_billable_tokens += (
        validation_dataset_stats.total_number_of_billable_tokens
    )

print(f"Dataset has ~{total_number_of_billable_tokens} tokens that will be charged")
print(f"By default, you'll train for {epoch_count} epochs on this dataset.")
print(
    f"By default, you'll be charged for ~{epoch_count * total_number_of_billable_tokens} tokens."
)
```

    Dataset has ~309397 tokens that will be charged
    By default, you'll train for 4 epochs on this dataset.
    By default, you'll be charged for ~1237588 tokens.
    

## Convert `Gemini 1.0 Pro` fine-tuning dataset to `Gemini 1.5 Pro` dataset.


```
source_uri = (
    "gs://next-23-tuning-demo/example-fine-tuning.json"  # @param {type:"string"}
)
destination_uri = (
    "gs://next-23-tuning-demo/new-data-format.jsonl"  # @param {type:"string"}
)
system_instruction = "You are a helpful and friendly AI assistant"  # Optional
```


```
def convert_jsonl_format(
    source_uri: str,
    destination_uri: str,
    system_instruction: str = None,
):
    """Converts a JSONL file from the old format to the new format.

    Args:
        source_uri: Google Cloud Storage URI of the source JSONL file.
        destination_uri: Google Cloud Storage URI for the new JSONL file.
        system_instruction: Optional system instruction text.
                            If provided, it will be added as "systemInstruction" in the new format.
    """
    storage_client = storage.Client()

    # Extract bucket and file name from source URI
    source_bucket_name, source_blob_name = extract_bucket_and_blob_name(source_uri)
    source_bucket = storage_client.bucket(source_bucket_name)
    source_blob = source_bucket.blob(source_blob_name)

    # Extract bucket and file name from destination URI
    dest_bucket_name, dest_blob_name = extract_bucket_and_blob_name(destination_uri)
    dest_bucket = storage_client.bucket(dest_bucket_name)
    dest_blob = dest_bucket.blob(dest_blob_name)

    # Download the source JSONL file
    source_data = source_blob.download_as_string().decode("utf-8")

    new_data = []
    for line in source_data.splitlines():
        try:
            json_data = json.loads(line)
            new_json_data = convert_json_object(json_data, system_instruction)
            new_data.append(new_json_data)
        except json.JSONDecodeError as e:
            print(f"Skipping invalid JSON line: {line} - Error: {e}")

    # Upload the new JSONL file
    new_data_str = "\n".join([json.dumps(data) for data in new_data])
    dest_blob.upload_from_string(new_data_str)

    print(f"Successfully converted and uploaded to {destination_uri}")


def convert_json_object(json_data: dict, system_instruction: str = None) -> dict:
    """Converts a single JSON object from the old format to the new format.

    Args:
        json_data: The JSON object to convert.
        system_instruction: Optional system instruction text.

    Returns:
        The converted JSON object.
    """
    new_json_data = {}  # Create an empty dict instead of initializing with "contents"

    if system_instruction:
        new_json_data["systemInstruction"] = {
            "role": "system",
            "parts": [{"text": system_instruction}],
        }

    new_json_data["contents"] = []  # Initialize "contents" after "systemInstruction"

    for message in json_data.get("messages", []):
        new_message = {"role": message["role"], "parts": [{"text": message["content"]}]}
        new_json_data["contents"].append(new_message)

    return new_json_data


def extract_bucket_and_blob_name(gcs_uri: str) -> tuple:
    """Extracts the bucket name and blob name from a Google Cloud Storage URI.

    Args:
        gcs_uri: The Google Cloud Storage URI (e.g., "gs://my-bucket/my-file.jsonl")

    Returns:
        A tuple containing the bucket name and blob name.
    """
    if not gcs_uri.startswith("gs://"):
        raise ValueError("Invalid Google Cloud Storage URI")
    parts = gcs_uri[5:].split("/", 1)
    return parts[0], parts[1]
```


```
convert_jsonl_format(source_uri, destination_uri, system_instruction)
```

    Successfully converted and uploaded to gs://next-23-tuning-demo/new-data-format.jsonl
    

## Tuning token count and cost estimation for `Gemini 1.0 Pro` legacy users.

Only use this part if you still use `Gemini 1.0 Pro`. Its best to upgrade to using [`gemini-1.5-pro-002`](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning).

### Load the dataset

Define the Google Cloud Storage URIs pointing to your training and validation datasets or continue using the URIs provided.


```
BASE_MODEL = "gemini-1.0-pro-002"  # @param ['gemini-1.0-pro-002']{type:"string"}
training_dataset_uri = "gs://cloud-samples-data/ai-platform/generative_ai/sft_train_data.jsonl"  # @param {type:"string"}
validation_dataset_uri = "gs://cloud-samples-data/ai-platform/generative_ai/sft_validation_data.jsonl"  # @param {type:"string"}

tokenizer = get_tokenizer_for_model(BASE_MODEL)
```

We'll now load the dataset and conduct some basic statistical analysis to understand its structure and content.



```
with tf.io.gfile.GFile(training_dataset_uri) as dataset_jsonl_file:
    example_training_dataset = [
        json.loads(dataset_line) for dataset_line in dataset_jsonl_file
    ]

if validation_dataset_uri:
    with tf.io.gfile.GFile(validation_dataset_uri) as dataset_jsonl_file:
        example_validation_dataset = [
            json.loads(dataset_line) for dataset_line in dataset_jsonl_file
        ]

# Initial dataset stats
print("Num training examples:", len(example_training_dataset))
print("First example:")
for message in example_training_dataset[0]["messages"]:
    print(message)
    print(tokenizer.count_tokens(message.get("content")))

if example_validation_dataset:
    print("Num validation examples:", len(example_validation_dataset))
```

    Num training examples: 500
    First example:
    {'role': 'user', 'content': "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n#Person2#: I found it would be a good idea to get a check-up.\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n#Person2#: Ok.\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n#Person2#: Yes.\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n#Person2#: Ok, thanks doctor.\n\nProvide a summary of the article in two or three sentences:\n\n"}
    CountTokensResult(total_tokens=277)
    {'role': 'model', 'content': "Mr. Smith's getting a check-up, and Doctor Hawkins advises them to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking."}
    CountTokensResult(total_tokens=41)
    Num validation examples: 100
    

### Validate the format of the data

You can perform various error checks to validate that each tuning example in the dataset adheres to the format expected by the tuning API. Errors are categorized based on their nature for easier debugging.  
  
For how to prepare dataset for tuning gemini, please refer to this [tutorial](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-about).

1. **Presence of Message List**: Checks if a `messages` list is present in each entry. Error type: `missing_messages_list`:
2. **Message Keys Check**: Validates that each message in the messages list contains the keys `role` and `content`. Error type: `message_missing_key`.
3. **Role Validation**: Ensures the role is one of `system`, `user`, or `model`. Error type: `unrecognized_role`. Note: only the first message can have `system` as role.
5. **Content Validation**: Verifies that content has textual data and is a string. Error type: `missing_content`.
6. **Consecutive Turns**. For the chat history, it is enforced that the message must can repeat in an alternating manner. Error type: `consecutive_turns`.



```
def validate_dataset_format(dataset):
    """Validates the dataset.

    Args:
      dataset_uri: The dataset uri to be validated.
    """
    format_errors = defaultdict(list)
    if not dataset or len(dataset) == 0:
        print("Input dataset file is empty or inaccessible.")
        return

    for row_idx, example in enumerate(dataset):
        # Verify presence of messages list
        if not isinstance(example, dict):
            format_errors["missing_messages_list"].append(row_idx)
            continue
        messages = example.get("messages", None)
        try:
            validate_messages(messages, format_errors, row_idx)
        except (TypeError, AttributeError, KeyError) as e:
            print("Invalid input during validation: %s", e)
            format_errors["invalid_input"].append(row_idx)

    if format_errors:
        print("Found errors for this dataset:")
        for k, v in format_errors.items():
            print(f"{k}: {v}")
    else:
        print("No errors found for this dataset.")


def validate_messages(messages, format_errors, row_index):
    """Validates messages list format."""
    if not messages:
        format_errors["missing_messages_list"].append(row_index)
        return

    # Check if the first role is for system instruction
    if messages[0].get("role", "").lower() == "system":
        messages = messages[1:]
    else:
        messages = messages[:]

    prev_role = None

    for message in messages:
        if "role" not in message or "content" not in message:
            format_errors["message_missing_key"].append(row_index)
            return

        if message.get("role", "").lower() not in ("user", "model"):
            format_errors["unrecognized_role"].append(row_index)
            return

        content = message.get("content", None)
        if not content:
            format_errors["missing_content"].append(row_index)
            return

            role = message.get("role", "").lower()
            # messages to have alternate turns.
            if role == prev_role:
                format_errors["consecutive_turns"].append(row_index)
                return

            prev_role = role
```

Now you can check the data for any issues.


```
validate_dataset_format(example_training_dataset)
if example_validation_dataset:
    validate_dataset_format(example_validation_dataset)
```

    No errors found for this dataset.
    No errors found for this dataset.
    

### Utils for dataset analysis and token counting

This section focuses on analyzing the structure and token counts of your datasets. You will also define some utility functions to streamline subsequent steps in the notebook.

* Load and inspect sample data from the training and validation datasets.
* Calculate token counts for messages to understand the dataset's characteristics.
* Define utility functions for calculating token distributions and dataset statistics. These will help assess the suitability of your data for supervised tuning and estimate potential costs.



```
@dataclasses.dataclass
class DatasetDistribution:
    """Dataset disbribution for given a population of values.

    It optionally contains a histogram consists of bucketized data representing
    the distribution of those values. The summary statistics are the sum, min,
    max, mean, median, p5, p95.

    Attributes:
      sum: Sum of the values in the population.
      max: Max of the values in the population.
      min: Min of the values in the population.
      mean: The arithmetic mean of the values in the population.
      median: The median of the values in the population.
      p5: P5 quantile of the values in the population.
      p95: P95 quantile of the values in the population.
    """

    sum: int | None = None
    max: float | None = None
    min: float | None = None
    mean: float | None = None
    median: float | None = None
    p5: float | None = None
    p95: float | None = None


@dataclasses.dataclass
class DatasetStatistics:
    """Dataset statistics used for dataset profiling.

    Attributes:
      total_number_of_dataset_examples: Number of tuning examples in the dataset.
      total_number_of_records_for_training: Number of tuning records after
        formatting. Each model turn in the chat message will be considered as a record for tuning.
      total_number_of_billable_tokens: Number of total billable tokens in the
        dataset.
      user_input_token_length_stats: Stats for input token length.
      user_output_token_length_stats: Stats for output token length.
    """

    total_number_of_dataset_examples: int | None = None
    total_number_of_records_for_training: int | None = None
    total_number_of_billable_tokens: int | None = None
    user_input_token_length_stats: DatasetDistribution | None = None
    user_output_token_length_stats: DatasetDistribution | None = None


MAX_TOKENS_PER_EXAMPLE = 32 * 1024
ESTIMATE_PADDING_TOKEN_PER_EXAMPLE = 8
```


```
def calculate_distribution_for_population(population) -> DatasetDistribution:
    """Calculates the distribution from the population of values.

    Args:
      population: The population of values to calculate distribution for.

    Returns:
      DatasetDistribution of the given population of values.
    """
    if not population:
        raise ValueError("population is empty")

    return DatasetDistribution(
        sum=np.sum(population),
        max=np.max(population),
        min=np.min(population),
        mean=np.mean(population),
        median=np.median(population),
        p5=np.percentile(population, 5, method="nearest"),
        p95=np.percentile(population, 95, method="nearest"),
    )


def get_token_distribution_for_one_tuning_dataset_example(example):
    model_turn_token_list = []
    input_token_list = []
    input = []
    n_too_long = 0
    number_of_records_for_training = 0  # each model turn in the chat message will be considered as a record for tuning
    for message in example["messages"]:
        role = message.get("role").lower()
        text = message.get("content")

        if role.lower() == "model":
            result = tokenizer.count_tokens(input)
            input_token_list.append(result.total_tokens)
            model_turn_token_list.append(tokenizer.count_tokens(text).total_tokens)
            number_of_records_for_training += 1
            if (
                result.total_tokens + tokenizer.count_tokens(text).total_tokens
                > MAX_TOKENS_PER_EXAMPLE
            ):
                n_too_long += 1
                break

        input.append(Content(role=role, parts=[Part.from_text(text)]))

    return (
        input_token_list,
        model_turn_token_list,
        number_of_records_for_training,
        np.sum(model_turn_token_list) + np.sum(input_token_list),
        n_too_long,
    )


def get_dataset_stats_for_dataset(dataset):
    results = map(get_token_distribution_for_one_tuning_dataset_example, dataset)
    user_input_token_list = []
    model_turn_token_list = []
    number_of_records_for_training = 0
    total_number_of_billable_tokens = 0
    n_too_long_for_dataset = 0
    for (
        input_token_list_per_example,
        model_turn_token_list_per_example,
        number_of_records_for_training_per_example,
        number_of_billable_token_per_example,
        n_too_long,
    ) in results:
        user_input_token_list.extend(input_token_list_per_example)
        model_turn_token_list.extend(model_turn_token_list_per_example)
        number_of_records_for_training += number_of_records_for_training_per_example
        total_number_of_billable_tokens += number_of_billable_token_per_example
        n_too_long_for_dataset += n_too_long

    print(
        f"\n{n_too_long_for_dataset} examples may be over the {MAX_TOKENS_PER_EXAMPLE} token limit, they will be truncated during tuning."
    )

    return DatasetStatistics(
        total_number_of_dataset_examples=len(dataset),
        total_number_of_records_for_training=number_of_records_for_training,
        total_number_of_billable_tokens=total_number_of_billable_tokens
        + number_of_records_for_training * ESTIMATE_PADDING_TOKEN_PER_EXAMPLE,
        user_input_token_length_stats=calculate_distribution_for_population(
            user_input_token_list
        ),
        user_output_token_length_stats=calculate_distribution_for_population(
            model_turn_token_list
        ),
    )


def print_dataset_stats(dataset):
    dataset_stats = get_dataset_stats_for_dataset(dataset)
    print("Below you can find the dataset statistics:")
    print(
        f"Total number of examples in the dataset: {dataset_stats.total_number_of_dataset_examples}"
    )
    print(
        f"Total number of records for training: {dataset_stats.total_number_of_records_for_training}"
    )
    print(
        f"Total number of billable tokens in the dataset: {dataset_stats.total_number_of_billable_tokens}"
    )
    print(
        f"User input token length distribution: {dataset_stats.user_input_token_length_stats}"
    )
    print(
        f"User output token length distribution: {dataset_stats.user_output_token_length_stats}"
    )
    return dataset_stats
```

Next you can analyze the structure and token counts of your datasets.


```
training_dataset_stats = print_dataset_stats(example_training_dataset)

if example_validation_dataset:
    validation_dataset_stats = print_dataset_stats(example_validation_dataset)
```

    
    0 examples may be over the 32768 token limit, they will be truncated during tuning.
    Below you can find the dataset statistics:
    Total number of examples in the dataset: 500
    Total number of records for training: 500
    Total number of billable tokens in the dataset: 130300
    User input token length distribution: DatasetDistribution(sum=109172, max=712, min=70, mean=218.344, median=198.5, p5=89, p95=403)
    User output token length distribution: DatasetDistribution(sum=17128, max=124, min=12, mean=34.256, median=31.0, p5=17, p95=63)
    
    0 examples may be over the 32768 token limit, they will be truncated during tuning.
    Below you can find the dataset statistics:
    Total number of examples in the dataset: 100
    Total number of records for training: 100
    Total number of billable tokens in the dataset: 28414
    User input token length distribution: DatasetDistribution(sum=23922, max=829, min=70, mean=239.22, median=225.5, p5=92, p95=430)
    User output token length distribution: DatasetDistribution(sum=3692, max=93, min=12, mean=36.92, median=36.0, p5=17, p95=63)
    

### Cost Estimation for Supervised Fine-tuning
In this final section, you will estimate the total cost for supervised fine-tuning based on the number of tokens processed. The number of tokens used will be charged to you. Please refer to the [pricing page for the rate](https://cloud.google.com/vertex-ai/generative-ai/pricing#gemini-models).

**Important Note:** The final cost may vary slightly from this estimate due to dataset formatting and truncation logic during training.

The code calculates the total number of billable tokens by summing up the tokens from the training dataset and (if provided) the validation dataset. Then, it estimates the total cost by multiplying the total billable tokens with the number of training epochs (default is 4).

### Cost estimation

In this final section, you will estimate the total number of tokens used for supervised tuning. The number of tokens will be charged to you.

There might be a slight difference between the estimation and actual cost due to dataset formatting and truncation logic.


```
epoch_count = 4  # @param {type:"integer"}
if epoch_count is None:
    epoch_count = 4


total_number_of_billable_tokens = training_dataset_stats.total_number_of_billable_tokens


if validation_dataset_stats:
    total_number_of_billable_tokens += (
        validation_dataset_stats.total_number_of_billable_tokens
    )

print(f"Dataset has ~{total_number_of_billable_tokens} tokens that will be charged")
print(f"By default, you'll train for {epoch_count} epochs on this dataset.")
print(
    f"By default, you'll be charged for ~{epoch_count * total_number_of_billable_tokens} tokens."
)
```

    Dataset has ~158714 tokens that will be charged
    By default, you'll train for 4 epochs on this dataset.
    By default, you'll be charged for ~634856 tokens.
    




################################################## vertex_ai_prompt_optimizer_sdk.md ##################################################


```
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# Vertex prompt optimizer Notebook SDK (Preview)

<table align="left">
  <td style="text-align: center">
    <a href="https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_sdk.ipynb">
      <img width="32px" src="https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg" alt="Google Colaboratory logo"><br> Open in Colab
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fprompts%2Fprompt_optimizer%2Fvertex_ai_prompt_optimizer_sdk.ipynb">
      <img width="32px" src="https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN" alt="Google Cloud Colab Enterprise logo"><br> Open in Colab Enterprise
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_sdk.ipynb">
      <img src="https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg" alt="Vertex AI logo"><br> Open in Vertex AI Workbench
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_sdk.ipynb">
      <img width="32px" src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg" alt="GitHub logo"><br> View on GitHub
    </a>
  </td>
</table>
    

| | | |
|-|-|-|
|Author | [Ivan Nardini](https://github.com/inardini)

##  I. Overview

When developing Generative AI (Gen AI) applications, prompt engineering poses challenges due to its time-consuming and error-prone nature. Significant effort is involved when crafting and inputting prompts to achieve successful task completion. With the frequent release of foundational models, you face the added burden of migrating working prompts from one model version to another.

Vertex AI prompt optimizer aims to alleviate these challenges by providing you with an intelligent prompt optimization tool. With this tool you can both translate and optimize system instruction in the prompts and the best demonstrations (examples) for prompt templates, empowering you to shape LLM responses from any source model to a target Google model.


### Objective

This notebook demostrates how to leverage Vertex AI prompt optimizer to optimize a simple prompt for a Gemini model with respect to a question-answering task. The goal is to use Vertex AI prompt optimizer to find the new prompt template that generates the most accurate and grounded responses.

This tutorial uses the following Google Cloud ML services and resources:

- Generative AI on Vertex AI
- Vertex AI prompt optimizer
- Vertex AI Gen AI evaluation
- Vertex AI Custom job

The steps performed include:

1. Define the prompt template you want to optimize.
2. Prepare the prompt optimization dataset.
3. Set target model and evaluation metric.
4. Set optimization mode and steps.
5. Run the automatic prompt optimization job.
6. Collect the best prompt template and evaluation metric.
7. Validate the best prompt template.

### Dataset

The dataset is a question-answering dataset generated by  a simple AI cooking assistant that provides suggestions on how to prepare healthier dishes.


### Costs

This tutorial uses billable components of Google Cloud:

* Vertex AI
* Cloud Storage

Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.

## II. Before you start

### Install Vertex AI SDK for Python and other required packages



```
%pip install --upgrade --quiet 'google-cloud-aiplatform[evaluation]'
%pip install --upgrade --quiet 'plotly' 'asyncio' 'tqdm' 'tenacity' 'etils' 'importlib_resources' 'fsspec' 'gcsfs' 'nbformat>=4.2.0'
```


```
! mkdir -p ./tutorial/utils && wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vapo_lib.py -P ./tutorial/utils

```

### Restart runtime (Colab only)

To use the newly installed packages, you must restart the runtime on Google Colab.


```
import sys

if "google.colab" in sys.modules:
    import IPython

    app = IPython.Application.instance()
    app.kernel.do_shutdown(True)
```

<div class="alert alert-block alert-warning">
<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>
</div>


### Authenticate your notebook environment (Colab only)

Authenticate your environment on Google Colab.



```
# import sys

# if "google.colab" in sys.modules:
#     from google.colab import auth

#     auth.authenticate_user()
```

### Set Google Cloud project information

To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).

Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).

#### Set your project ID and project number


```
PROJECT_ID = "[your-project-id]"  # @param {type:"string"

# Set the project id
! gcloud config set project {PROJECT_ID}
```


```
PROJECT_NUMBER = !gcloud projects describe {PROJECT_ID} --format="get(projectNumber)"[0]
PROJECT_NUMBER = PROJECT_NUMBER[0]
```

#### Region

You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations).


```
REGION = "us-central1"  # @param {type: "string"}
```

#### Create a Cloud Storage bucket

Create a storage bucket to store intermediate artifacts such as datasets.


```
BUCKET_NAME = "your-bucket-name-{PROJECT_ID}-unique"  # @param {type:"string"}

BUCKET_URI = f"gs://{BUCKET_NAME}"  # @param {type:"string"}
```


```
! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}
```

#### Service Account and permissions

Vertex AI Automated Prompt Design requires a service account with the following permissions:

-   `Vertex AI User` to call Vertex LLM API
-   `Storage Object Admin` to read and write to your GCS bucket.
-   `Artifact Registry Reader` to download the pipeline template from Artifact Registry.

[Check out the documentation](https://cloud.google.com/iam/docs/manage-access-service-accounts#iam-view-access-sa-gcloud) to learn how to grant those permissions to a single service account.

> If you run following commands using Vertex AI Workbench, run directly in the terminal.



```
SERVICE_ACCOUNT = f"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com"
```


```
for role in ['aiplatform.user', 'storage.objectAdmin', 'artifactregistry.reader']:

    ! gcloud projects add-iam-policy-binding {PROJECT_ID} \
      --member=serviceAccount:{SERVICE_ACCOUNT} \
      --role=roles/{role} --condition=None
```

### Set tutorial folder and workspace

Set a local folder to collect and organize data and any tutorial artifacts.


```
from pathlib import Path as path

ROOT_PATH = path.cwd()
TUTORIAL_PATH = ROOT_PATH / "tutorial"
TUTORIAL_PATH.mkdir(parents=True, exist_ok=True)
```

Set an associated workspace to store prompt optimization results on Cloud Storage bucket.


```
from etils import epath

WORKSPACE_URI = epath.Path(BUCKET_URI) / "optimization"
INPUT_DATA_URI = epath.Path(WORKSPACE_URI) / "data"

WORKSPACE_URI.mkdir(parents=True, exist_ok=True)
INPUT_DATA_URI.mkdir(parents=True, exist_ok=True)
```

### Import libraries

Import required libraries.


```
# Tutorial
from argparse import Namespace
import json

# General
import logging
import warnings

from IPython.display import HTML, display
from google.cloud import aiplatform
import pandas as pd
from sklearn.model_selection import train_test_split
from tutorial.utils import vapo_lib
from vertexai.generative_models import GenerativeModel
```

### Libraries logging

Configure logging for libraries to display output within the notebook.


```
warnings.filterwarnings("ignore")
logging.getLogger("urllib3.connectionpool").setLevel(logging.ERROR)
```

### Define constants

Define some tutorial constants.


```
INPUT_DATA_FILE_URI = "gs://github-repo/prompts/prompt_optimizer/rag_qa_dataset.jsonl"

EXPERIMENT_NAME = "qa-prompt-eval"
INPUT_OPTIMIZATION_DATA_URI = epath.Path(WORKSPACE_URI) / "prompt_optimization_data"
INPUT_OPTIMIZATION_DATA_FILE_URI = str(
    INPUT_DATA_URI / "prompt_optimization_dataset.jsonl"
)
OUTPUT_OPTIMIZATION_DATA_URI = epath.Path(WORKSPACE_URI) / "optimization_jobs"
APD_CONTAINER_URI = (
    "us-docker.pkg.dev/vertex-ai-restricted/builtin-algorithm/apd:preview_v1_0"
)
CONFIG_FILE_URI = str(WORKSPACE_URI / "config" / "config.json")
```

### Initialize Vertex AI SDK for Python

Initialize the Vertex AI SDK for Python for your project.


```
aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)
```

## III. Automated prompt design with Vertex AI prompt optimizer

### Load the dataset

Load the cooking question-answer dataset from a Google Cloud Storage bucket. The dataset contains the following columns:

*   `user_question`: The cooking question posed by the user to the AI cooking assistant.
*   `context`: Relevant information retrieved to answer the user's question.
*   `prompt`: The content fed to the language model to generate an answer.
*   `answer`: The generated answer from the language model.
*   `reference`: The ground truth answer—the ideal response the user expects from the AI cooking assistant.


```
prompt_optimization_df = pd.read_json(INPUT_DATA_FILE_URI, lines=True)
```


```
prompt_optimization_df.head()
```

Print an example of the cooking question-answer dataset.  


```
vapo_lib.print_df_rows(prompt_optimization_df, n=1)
```

### Evaluate the system instruction in the original prompt template

Assess the original prompt's effectiveness for our AI cooking assistant's question-answering task using Vertex AI's Gen AI Evaluation service. This service offers various metrics and methods to evaluate generative models, which enables comparing the model's performance against our own expectations and criteria.

Specifically, you focus on the quality and groundedness of the answers generated in response to the prompt using a test dataset.

To learn more, see [Gen AI evaluation service overview](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview).  



```
train_prompt_optimization_df, test_prompt_optimization_df = train_test_split(
    prompt_optimization_df, test_size=0.3, random_state=8
)
```


```
evaluation_qa_results = [
    (
        "qa_eval_result_without_prompt_optimization",
        vapo_lib.evaluate_task(
            df=test_prompt_optimization_df,
            prompt_col="prompt",
            reference_col="reference",
            response_col="answer",
            experiment_name=EXPERIMENT_NAME,
            eval_metrics=["question_answering_quality", "groundedness"],
            eval_sample_n=len(test_prompt_optimization_df),
        ),
    )
]
```

Plot the evaluation metrics.


```
vapo_lib.plot_eval_metrics(evaluation_qa_results)
```

### Optimize the prompt template with Vertex AI prompt optimizer


#### Prepare the prompt template you want to optimize

A prompt consists of two key parts:

* **System Instruction Template** which is a fixed part of the prompt that control or alter the model's behavior across all queries for a given task.

* **Prompt Template** which is a dynamic part of the prompt that changes based on the task. Prompt template includes examples, context, task and more. To learn more, see [components of a prompt](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies#components-of-a-prompt) in the official documentation.

In this scenario, you use Vertex AI prompt optimizer to optimize a simple system instruction template.

And you use some examples in the remaining prompt template for evaluating different instruction templates along the optimization process.

> **Note**: Having the `target` placeholder in the prompt template is optional. It represents the prompt's ground truth response in your prompt optimization dataset that you aim to optimize for your templates. If you don't have the prompt's ground truth response, remember to set the `source_model` parameter to your prompt optimizer configuration (see below) instead of adding ground truth responses. Vertex AI prompt optimizer would run your sample prompts on the source model to generate the ground truth responses for you.



```
SYSTEM_INSTRUCTION_TEMPLATE = """
Given a question with context, provide the correct answer to the question.
"""

PROMPT_TEMPLATE = """
Some examples of correct answer to a question are:
Question: {question}
Context: {ctx}
Answer: {target}
"""
```

#### Prepare the prompt optimization dataset

To use Vertex AI prompt optimizer, you'll need a CSV or JSONL file with labeled examples.  These examples should follow a specific naming convention. For details see [Optimize prompts](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer).


> **Note**: For effective **prompt optimization**, provide a dataset of examples where your model is poor in performance when using current system instruction template. For reliable results, use 50-100 distinct samples.

> In case of **prompt migration**, consider using the source model to label examples that the target model struggles with, helping to identify areas for improvement.



```
prepared_train_prompt_optimization_df = train_prompt_optimization_df.copy()

# Prepare optimization dataset columns
prepared_train_prompt_optimization_df = prepared_train_prompt_optimization_df.rename(
    columns={"user_question": "question", "context": "ctx", "reference": "target"}
)

# Remove uneccessary columns
prepared_train_prompt_optimization_df = prepared_train_prompt_optimization_df.drop(
    columns=["prompt", "answer"]
)

# Reorder columns
prepared_train_prompt_optimization_df = prepared_train_prompt_optimization_df[
    ["question", "ctx", "target"]
]
```

Print some examples of the prompt optimization dataset.  


```
prepared_train_prompt_optimization_df.head()
```

#### Upload samples to bucket

Once you prepare your prompt optimization dataset, you can upload them on Cloud Storage bucket.


```
prepared_train_prompt_optimization_df.to_json(
    INPUT_OPTIMIZATION_DATA_FILE_URI, orient="records", lines=True
)
```

#### Configure optimization settings

Vertex AI prompt optimizer lets you control the optimization process by specifying what to optimize (instructions only, demonstrations only, or both), providing a system instruction and prompt template, and selecting the target model.  You can optionally refine the optimization with some advanced settings like its duration and the number of optimization iterations it runs, which models the Vertex AI prompt optimizer uses, and other parameters to control the structure and content of prompts.

Below are some common and recommended default configurations. For more advanced control, you can learn and explore more about all the parameters and how to best use them in the [detailed documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer).



```
PROMPT_OPTIMIZATION_JOB = "auto-prompt-design-job-" + vapo_lib.get_id()
OUTPUT_OPTIMIZATION_RUN_URI = str(
    OUTPUT_OPTIMIZATION_DATA_URI / PROMPT_OPTIMIZATION_JOB
)

args = Namespace(
    # Basic configuration
    system_instruction=SYSTEM_INSTRUCTION_TEMPLATE,  # System instructions for the target model. String.
    prompt_template=PROMPT_TEMPLATE,  # Template for prompts,  String.
    target_model="gemini-1.5-flash-001",  # Target model for optimization. String. Supported models: "gemini-1.5-flash-002", "gemini-1.5-pro-002", "gemini-1.5-flash-001", "gemini-1.5-pro-001", "gemini-1.0-pro-001", "gemini-1.0-pro-002", "gemini-1.0-ultra-001", "text-bison@001", "text-bison@002", "text-bison32k@002", "text-unicorn@001"
    optimization_mode="instruction",  # Optimization mode. String. Supported modes: "instruction", "demonstration", "instruction_and_demo"
    eval_metrics_types=[
        "question_answering_correctness",
        "groundedness",
    ],  # List of evaluation metrics. List of strings. Supported metrics: "bleu", "coherence", "exact_match", "fluidity", "fulfillment", "groundedness", "rouge_1", "rouge_2", "rouge_l", "rouge_l_sum", "safety", "question_answering_correctness", "question_answering_helpfulness", "question_answering_quality", "question_answering_relevance", "summarization_helpfulness", "summarization_quality", "summarization_verbosity", "tool_name_match", "tool_parameter_key_match", "tool_parameter_kv_match"
    eval_metrics_weights=[
        0.9,
        0.1,
    ],  # Weights for evaluation metrics. List of floats.  Length must match eval_metrics_types.  Should sum to 1.
    aggregation_type="weighted_sum",  # Aggregation type for evaluation metrics. String. Supported aggregation types: "weighted_sum", "weighted_average"
    input_data_path=INPUT_OPTIMIZATION_DATA_FILE_URI,  # Cloud Storage URI to input optimization data. String.
    output_path=OUTPUT_OPTIMIZATION_RUN_URI,  # Cloud Storage URI to save optimization results. String.
    project=PROJECT_ID,  # Google Cloud project ID. String.
    # (Optional) Advanced configuration
    num_steps=10,  # Number of iterations in instruction optimization mode. Integer between 10 and 20.
    num_template_eval_per_step=2,  # Number of system instructions generated and evaluated in instruction and instruction_and_demo mode. Integer between 1 and 4.
    num_demo_set_candidates=10,  # Number of demonstrations evaluated in instruction and instruction_and_demo mode. Integer between 10 and 30.
    demo_set_size=3,  # Number of demonstrations generated per prompt. Integer between 3 and 6.
    target_model_location="us-central1",  # Location of the target model. String. Default us-central1.
    optimizer_model="gemini-1.5-pro-001",  # Optimization model. String. Supported models: "gemini-1.5-flash-002", "gemini-1.5-pro-002", "gemini-1.5-flash-001", "gemini-1.5-pro-001", "gemini-1.0-pro-001", "gemini-1.0-pro-002", "gemini-1.0-ultra-001", "text-bison@001", "text-bison@002", "text-bison32k@002", "text-unicorn@001"
    optimizer_model_location="us-central1",  # Location of the optimization model. String. Default us-central1.
    eval_model="gemini-1.5-pro-001",  # Evaluation model. String. Supported models: "gemini-1.5-flash-002", "gemini-1.5-pro-002", "gemini-1.5-flash-001", "gemini-1.5-pro-001", "gemini-1.0-pro-001", "gemini-1.0-pro-002", "gemini-1.0-ultra-001", "text-bison@001", "text-bison@002", "text-bison32k@002", "text-unicorn@001"
    eval_model_location="us-central1",  # Location of the evaluation model. String. Default us-central1.
    source_model="",  # Google model that the system instructions and prompts were previously used with. String. Not needed if you provide target column.
    source_model_location="",  # Location of the source model. String. Default us-central1. Not needed if you provide target column.
    target_model_qps=1,  # The queries per second (QPS) sent to the target model. Integer greater or equal than 1 depending on your quota.
    optimizer_model_qps=1,  # The queries per second (QPS) sent to the optimization model. Integer greater or equal than 1 depending on your quota.
    eval_qps=1,  # The queries per second (QPS) sent to the eval model. Integer greater or equal than 1 depending on your quota.
    source_model_qps="",  # The queries per second (QPS) sent to the source model. Integer greater or equal than 1 depending on your quota.
    response_mime_type="application/json",  # MIME response type that the target model uses. String. Supported response: text/plain, application/json.
    language="English",  # Language of the system instructions. String. Supported languages: "English", "French", "German", "Hebrew", "Hindi", "Japanese", "Korean", "Portuguese", "Simplified Chinese", "Spanish", "Traditional Chinese"
    placeholder_to_content=json.loads(
        "{}"
    ),  # Placeholder to replace any parameter in the system instruction. Dict.
    data_limit=50,  # Amount of data used for validation. Integer between 5 and 100.
)
```

#### Upload Vertex AI prompt optimizer config to Cloud Storage

After define the Vertex AI prompt optimizer configuration, upload them on Cloud Storage bucket.



```
args = vars(args)

with epath.Path(CONFIG_FILE_URI).open("w") as config_file:
    json.dump(args, config_file)
config_file.close()
```

#### Run the automatic prompt optimization job

Now you are ready to run your first Vertex AI prompt optimizer job using the Vertex AI SDK for Python.


> This prompt optimization job requires ~ 40 minutes to run.

> Be sure you have provisioned enough queries per minute (QPM) quota implementing the recommended QPM for each model. If you configure the Vertex AI prompt optimizer with a QPM that is higher than the QPM than you have access to, the job might fail. [Check out](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer#before-you-begin) the documentation to know more.



```
WORKER_POOL_SPECS = [
    {
        "machine_spec": {
            "machine_type": "n1-standard-4",
        },
        "replica_count": 1,
        "container_spec": {
            "image_uri": APD_CONTAINER_URI,
            "args": ["--config=" + CONFIG_FILE_URI],
        },
    }
]

custom_job = aiplatform.CustomJob(
    display_name=PROMPT_OPTIMIZATION_JOB,
    worker_pool_specs=WORKER_POOL_SPECS,
)

custom_job.run(service_account=SERVICE_ACCOUNT, sync=False)
```

### Collect and display the optimization results

Vertex AI prompt optimizer returns both optimized templates and evaluation results for either instruction, or demostrations, or both depending on the optimization mode you define as JSONL files on Cloud Storage bucket. Those results help you understand the optimization process.

In this case, you want to collect the optimized templates and evaluation results for the system instruction.

Below you use a helper function to display those results.


```
results_ui = vapo_lib.ResultsUI(OUTPUT_OPTIMIZATION_RUN_URI)
results_df_html = """

"""

display(HTML(results_df_html))
display(results_ui.get_container())
```

### Evaluate the new prompt template with the optimized instruction.

#### Generate new responses using the optimized system instruction.

Set the optimized system instruction template you get from Vertex AI prompt optimizer job.


```
OPTIMIZED_SYSTEM_INSTRUCTION_TEMPLATE = "You are a culinary expert. Use the provided cooking tips and your culinary expertise to answer the following question in a way that is comprehensive, engaging, and easy for a home cook to understand."  # @param {type:"string"}
```

Prepare optimized prompts using the optimized system instruction template.


```
OPTIMIZED_PROMPT_TEMPLATE = (
    OPTIMIZED_SYSTEM_INSTRUCTION_TEMPLATE
    + "\nQuestion: \n{question}"
    + "\nContext: \n{context}"
    + "\nAnswer:"
)

optimized_prompts = [
    OPTIMIZED_PROMPT_TEMPLATE.format(question=q, context=c)
    for q, c in zip(
        test_prompt_optimization_df["user_question"].to_list(),
        test_prompt_optimization_df["context"].to_list(),
    )
]
```

Leverage Gemini API on Vertex AI to send parallel generation requests.


```
gemini_llm = GenerativeModel(model_name="gemini-1.5-flash-001")

gemini_predictions = [
    vapo_lib.async_generate(p, model=gemini_llm) for p in optimized_prompts
]

gemini_predictions_col = await tqdm_asyncio.gather(*gemini_predictions)
```

Prepare the test data and visualize the resulting dataset.


```
test_prompt_optimization_df["optimized_prompt_with_vapo"] = optimized_prompts
test_prompt_optimization_df["gemini_answer_with_vapo"] = gemini_predictions_col
```


```
vapo_lib.print_df_rows(test_prompt_optimization_df, n=1)
```

#### Evaluate new responses using Vertex AI Gen AI evaluation

Use the generated responses with the optimized prompt to run a new round of evaluation with Vertex AI Gen AI Evaluation.



```
evaluation_qa_results.append(
    (
        "qa_eval_result_new_model_with_prompt_optimization",
        vapo_lib.evaluate_task(
            df=test_prompt_optimization_df,
            prompt_col="optimized_prompt_with_vapo",
            reference_col="reference",
            response_col="gemini_answer_with_vapo",
            experiment_name=EXPERIMENT_NAME,
            eval_metrics=["question_answering_quality", "groundedness"],
            eval_sample_n=len(test_prompt_optimization_df),
        ),
    )
)
```

Inspect evaluation results.


```
vapo_lib.plot_eval_metrics(evaluation_qa_results)
```

## IV. Clean up


```
delete_bucket = False
delete_job = False
delete_experiment = False
delete_tutorial = False

if delete_bucket:
    ! gsutil rm -r $BUCKET_URI

if delete_job:
    custom_job.delete()

if delete_experiment:
    experiment = aiplatform.Experiment(experiment_name=EXPERIMENT_NAME)
    experiment.delete()

if delete_tutorial:
    import shutil

    shutil.rmtree(str(TUTORIAL_PATH))
```




################################################## vertex_ai_prompt_optimizer_sdk_custom_metric.md ##################################################


```
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# Vertex Prompt Optimizer Notebook SDK (Preview) - Custom metric

<table align="left">
  <td style="text-align: center">
    <a href="https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_sdk_custom_metric.ipynb">
      <img width="32px" src="https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg" alt="Google Colaboratory logo"><br> Open in Colab
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fprompts%2Fprompt_optimizer%2Fvertex_ai_prompt_optimizer_sdk_custom_metric.ipynb">
      <img width="32px" src="https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN" alt="Google Cloud Colab Enterprise logo"><br> Open in Colab Enterprise
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_sdk_custom_metric.ipynb">
      <img src="https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg" alt="Vertex AI logo"><br> Open in Vertex AI Workbench
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_sdk_custom_metric.ipynb">
      <img width="32px" src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg" alt="GitHub logo"><br> View on GitHub
    </a>
  </td>
</table>
    

| | | |
|-|-|-|
| Author | [Ivan Nardini](https://github.com/inardini) |

##  I. Overview

When developing Generative AI (Gen AI) applications, prompt engineering poses challenges due to its time-consuming and error-prone nature. Significant effort is involved when crafting and inputting prompts to achieve successful task completion. With the frequent release of foundational models, you face the added burden of migrating working prompts from one model version to another.

Vertex AI prompt optimizer alleviates these challenges by providing an intelligent prompt optimization tool. With this tool you can both translate and optimize system instructions in the prompts and best demonstrations (examples) for prompt templates, which lets you shape LLM responses from any source model to a target Google model.


### Objective

This notebook demonstrates how to leverage Vertex AI prompt optimizer to optimize a simple prompt for a Gemini model using your own metric. The goal is to use Vertex AI prompt optimizer to find a new prompt template that generates better responses based on your own optimization metric.

This tutorial uses the following Google Cloud services and resources:

- Generative AI on Vertex AI
- Vertex AI prompt optimizer
- Vertex AI Gen AI evaluation
- Vertex AI Custom job
- Cloud Run

The steps performed include:

1. Define the prompt template you want to optimize.
2. Prepare the prompt optimization dataset.
3. Define and deploy your own custom evaluation metric on Cloud function.
4. Set optimization mode and steps.
5. Run the automatic prompt optimization job.
6. Collect the best prompt template and eval metric.
7. Validate the best prompt template.

### Dataset

The dataset is a question-answering dataset generated by  a simple AI cooking assistant that provides suggestions on how to prepare healthier dishes.


### Costs

This tutorial uses billable components of Google Cloud:

* Vertex AI
* Cloud Storage

Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.

## II. Before you start

### Install Vertex AI SDK for Python and other required packages



```
%pip install --upgrade --quiet 'google-cloud-aiplatform[evaluation]'
%pip install --upgrade --quiet 'plotly' 'asyncio' 'tqdm' 'tenacity' 'etils' 'importlib_resources' 'fsspec' 'gcsfs' 'nbformat>=4.2.0'
```


```
! mkdir -p ./tutorial/utils && wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vapo_lib.py -P ./tutorial/utils
```

### Restart runtime (Colab only)

To use the newly installed packages, you must restart the runtime on Google Colab.


```
import sys

if "google.colab" in sys.modules:
    import IPython

    app = IPython.Application.instance()
    app.kernel.do_shutdown(True)
```

<div class="alert alert-block alert-warning">
<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>
</div>


### Authenticate your notebook environment (Colab only)

Authenticate your environment on Google Colab.



```
import sys

if "google.colab" in sys.modules:
    try:
        import google.auth
        import google.auth.transport.requests
        from google.colab import auth

        auth.authenticate_user()
        creds, project = google.auth.default()
        authentication = google.auth.transport.requests.Request()
        if creds.token:
            print("Authentication successful.")
        else:
            print("Authentication successful, but no token was returned.")
    except Exception as e:
        print(f"Error during Colab authentication: {e}")

! gcloud auth login
```

### Set Google Cloud project information

To get started using Vertex AI, you must have an existing Google Cloud project and [enable the following APIs](https://console.cloud.google.com/flows/enableapi?apiid=cloudresourcemanager.googleapis.com,aiplatform.googleapis.com,cloudfunctions.googleapis.com,run.googleapis.com).

Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).

#### Set your project ID and project number


```
PROJECT_ID = "[your-project-id]"  # @param {type:"string"}

# Set the project id
! gcloud config set project {PROJECT_ID}
```


```
PROJECT_NUMBER = !gcloud projects describe {PROJECT_ID} --format="get(projectNumber)"[0]
PROJECT_NUMBER = PROJECT_NUMBER[0]
```

#### Region

You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations).


```
REGION = "us-central1"  # @param {type: "string"}
```

#### Create a Cloud Storage bucket

Create a storage bucket to store intermediate artifacts such as datasets.


```
BUCKET_NAME = "your-bucket-name-{PROJECT_ID}-unique"  # @param {type:"string"}

BUCKET_URI = f"gs://{BUCKET_NAME}"  # @param {type:"string"}
```


```
! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}
```

#### Service Account and permissions

Vertex AI Prompt optimizer requires a service account with the following permissions:

-   `Vertex AI User` to call Vertex LLM API
-   `Storage Object Admin` to read and write to your GCS bucket.
-   `Artifact Registry Reader` to download the pipeline template from Artifact Registry.
-   `Cloud Run Developer` to deploy function on Cloud Run.

[Check out the documentation](https://cloud.google.com/iam/docs/manage-access-service-accounts#iam-view-access-sa-gcloud) to learn how to grant those permissions to a single service account.


> If you run following commands using Vertex AI Workbench, please directly run in the terminal.



```
SERVICE_ACCOUNT = f"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com"
```


```
for role in ['aiplatform.user', 'storage.objectAdmin', 'artifactregistry.reader', 'run.developer', 'run.invoker']:

    ! gcloud projects add-iam-policy-binding {PROJECT_ID} \
      --member=serviceAccount:{SERVICE_ACCOUNT} \
      --role=roles/{role} --condition=None
```

### Set tutorial folder and workspace

Set a local folder to collect and organize data and any tutorial artifacts.


```
from pathlib import Path as path

ROOT_PATH = path.cwd()
TUTORIAL_PATH = ROOT_PATH / "tutorial"
BUILD_PATH = TUTORIAL_PATH / "build"

TUTORIAL_PATH.mkdir(parents=True, exist_ok=True)
BUILD_PATH.mkdir(parents=True, exist_ok=True)
```

Set an associated workspace to store prompt optimization results on Cloud Storage bucket.


```
from etils import epath

WORKSPACE_URI = epath.Path(BUCKET_URI) / "optimization"
INPUT_DATA_URI = epath.Path(WORKSPACE_URI) / "data"

WORKSPACE_URI.mkdir(parents=True, exist_ok=True)
INPUT_DATA_URI.mkdir(parents=True, exist_ok=True)
```

### Import libraries


```
# Tutorial
from argparse import Namespace
import json

# General
import logging
from pprint import pprint
import warnings

from IPython.display import HTML, display
from google.cloud import aiplatform
import pandas as pd
import requests
from sklearn.model_selection import train_test_split
from tutorial.utils import vapo_lib
from vertexai.generative_models import GenerativeModel
```

### Libraries settings


```
warnings.filterwarnings("ignore")
logging.getLogger("urllib3.connectionpool").setLevel(logging.ERROR)
```

### Define constants


```
INPUT_DATA_FILE_URI = "gs://github-repo/prompts/prompt_optimizer/rag_qa_dataset.jsonl"

INPUT_OPTIMIZATION_DATA_URI = epath.Path(WORKSPACE_URI) / "prompt_optimization_data"
INPUT_OPTIMIZATION_DATA_FILE_URI = str(
    INPUT_DATA_URI / "prompt_optimization_dataset.jsonl"
)
OUTPUT_OPTIMIZATION_DATA_URI = epath.Path(WORKSPACE_URI) / "optimization_jobs"
APD_CONTAINER_URI = (
    "us-docker.pkg.dev/vertex-ai-restricted/builtin-algorithm/apd:preview_v1_0"
)
CONFIG_FILE_URI = str(WORKSPACE_URI / "config" / "config.json")
```

### Initialize Vertex AI SDK for Python

Initialize the Vertex AI SDK for Python for your project.


```
aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)
```

## III. Automated prompt design with Vertex AI prompt optimizer

### Load the dataset

Load the cooking question-answer dataset from a Google Cloud Storage bucket. The dataset contains the following columns:

*   `user_question`: The cooking question posed by the user to the AI cooking assistant.
*   `context`: Relevant information retrieved to answer the user's question.
*   `prompt`: The content fed to the language model to generate an answer.
*   `answer`: The generated answer from the language model.
*   `reference`: The ground truth answer—the ideal response the user expects from the AI cooking assistant.


```
prompt_optimization_df = pd.read_json(INPUT_DATA_FILE_URI, lines=True)
```


```
prompt_optimization_df.head()
```

Print an example of the cooking question-answer dataset.  


```
vapo_lib.print_df_rows(prompt_optimization_df, n=1)
```

### Optimize the prompt template with Vertex AI prompt optimizer with custom metric


#### Prepare the prompt template you want to optimize

A prompt consists of two key parts:

* **System Instruction Template** which is a fixed part of the prompt that control or alter the model's behavior across all queries for a given task.

* **Prompt Template** which is a dynamic part of the prompt that changes based on the task. Prompt template includes examples, context, task and more. To learn more, see [components of a prompt](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies#components-of-a-prompt) in the official documentation.

In this scenario, you use Vertex AI prompt optimizer to optimize a simple system instruction template.

And you use some examples in the remaining prompt template for evaluating different instruction templates along the optimization process.

> **Note**: Having the `target` placeholder in the prompt template is optional. It represents the prompt's ground truth response in your prompt optimization dataset that you aim to optimize for your templates. If you don't have the prompt's ground truth response, remember to set the `source_model` parameter to your prompt optimizer configuration (see below) instead of adding ground truth responses. Vertex AI prompt optimizer would run your sample prompts on the source model to generate the ground truth responses for you.


```
SYSTEM_INSTRUCTION_TEMPLATE = """
Given a question with context, provide the correct answer to the question.
"""

PROMPT_TEMPLATE = """
Some examples of correct answer to a question are:
Question: {question}
Context: {ctx}
Answer: {target}
"""
```

#### Prepare the prompt optimization dataset

To use Vertex AI prompt optimizer, you'll need a CSV or JSONL file with labeled examples.  These examples should follow a specific naming convention. For details see [Optimize prompts](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer).


> **Note**: For effective **prompt optimization**, provide a dataset of examples where your model is poor in performance when using current system instruction template. For reliable results, use 50-100 distinct samples.

> In case of **prompt migration**, consider using the source model to label examples that the target model struggles with, helping to identify areas for improvement.


```
train_prompt_optimization_df, test_prompt_optimization_df = train_test_split(
    prompt_optimization_df, test_size=0.3, random_state=8
)
```


```
prepared_train_prompt_optimization_df = train_prompt_optimization_df.copy()

# Prepare optimization dataset columns
prepared_train_prompt_optimization_df = prepared_train_prompt_optimization_df.rename(
    columns={"user_question": "question", "context": "ctx", "reference": "target"}
)

# Remove uneccessary columns
prepared_train_prompt_optimization_df = prepared_train_prompt_optimization_df.drop(
    columns=["prompt", "answer"]
)

# Reorder columns
prepared_train_prompt_optimization_df = prepared_train_prompt_optimization_df[
    ["question", "ctx", "target"]
]
```

Print some examples of the prompt optimization dataset.  


```
prepared_train_prompt_optimization_df.head()
```

#### Upload samples to bucket

Once you prepare your prompt optimization dataset, you can upload them on Cloud Storage bucket.


```
prepared_train_prompt_optimization_df.to_json(
    INPUT_OPTIMIZATION_DATA_FILE_URI, orient="records", lines=True
)
```

#### Define and deploy your own custom optimization metric on Cloud function

To optimize your prompt template using a custom optimization metric, you need to deploy a function with your own metric code on a Cloud function. To deploy a Cloud function with your own custom metric, you cover the following steps:

1.   Define requirements
2.   Write your own custom metric function code
3.   Deploy the custom code as Cloud function


##### Define requirements

Set the custom metric dependencies.


```
requirements = """
functions-framework==3.*
google-cloud-aiplatform
"""

with open(BUILD_PATH / "requirements.txt", "w") as f:
    f.write(requirements)
```

##### Write your own custom metric function

Define the module which contains your own custom metric function definition.

In this case, you have a custom evaluation metric to evaluate the user engagement and personalization. The custom evaluation metric is defined using the `evaluate_engagement_personalization_fn`.

The function leverages "gemini-1.5-pro" to act as an "auto-rater". It sends a prompt to the auto-rater, receives a score (1-5), and an explanation, then returns these as a dictionary containing two fields: the custom metric's score (as you defined it) and an explanation of how this metric helps optimize the prompt template.

You use the `main` function to deploy the `evaluate_engagement_personalization_fn` function as a Cloud Function, receiving a question, response, and a target response as input and returning the auto-rater's evaluation.  



```
custom_metric_function_code = '''
"""
This module contains the custom evaluation metric definition to optimize a prompt template with Vertex AI prompt optimizer
"""

from typing import Dict
from vertexai.generative_models import (
    GenerationConfig,
    GenerativeModel,
    HarmBlockThreshold,
    HarmCategory,
)

import json
import functions_framework

def get_autorater_response(metric_prompt: str) -> dict:
    """This function is to generate the evaluation response from the autorater."""

    metric_response_schema = {
        "type": "OBJECT",
        "properties": {
            "score": {"type": "NUMBER"},
            "explanation": {"type": "STRING"},
        },
        "required": ["score", "explanation"],
    }

    autorater = GenerativeModel(
        "gemini-1.5-pro",
        generation_config=GenerationConfig(
            response_mime_type="application/json",
            response_schema=metric_response_schema,
        ),
        safety_settings={
            HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
        },
    )

    response = autorater.generate_content(metric_prompt)

    response_json = {}
    response_json = json.loads(response.text)
    return response_json


# Define custom evaluation criteria
def evaluate_engagement_personalization_fn(question: str, response:str, target: str) -> Dict[str, str]:
    """Evaluates an AI-generated response for User Engagement and Personalization."""

    custom_metric_prompt_template = """

    # Instruction
    You are an expert evaluator. Your task is to evaluate the quality of the LLM-generated responses against a reference target response.
    You should first read the Question carefully, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.
    You will assign the response a rating following the Rating Rubric only and an step-by-step explanation for your rating.

    # Evaluation

    ## Criteria
    Relevance and Customization: The response should directly address the user's query and demonstrate an understanding of their specific needs or preferences, such as dietary restrictions, skill level, or taste preferences.
    Interactivity and Proactiveness: The response should go beyond simply answering the question by actively encouraging further interaction through follow-up questions, suggestions for additional exploration, or prompts for more information to provide a tailored experience.
    Tone and Empathy: The response should adopt an appropriate and empathetic tone that fosters a positive and supportive user experience, making the user feel heard and understood.

    ## Rating rubric
    1 - Minimal: The response lacks personalization and demonstrates minimal engagement with the user. The tone may be impersonal or generic.
    2 - Basic: The response shows some basic personalization but lacks depth or specificity. Engagement is limited, possibly with generic prompts or suggestions. The tone is generally neutral but may lack warmth or empathy.
    3 - Moderate: The response demonstrates clear personalization and attempts to engage the user with relevant follow-up questions or prompts based on their query. The tone is friendly and supportive, fostering a positive user experience.
    4 - High: The response demonstrates a high degree of personalization and actively engages the user with relevant follow-up questions or prompts. The tone is empathetic and understanding, creating a strong connection with the user.
    5 - Exceptional: The response goes above and beyond to personalize the experience, anticipating user needs, and fostering a genuine connection. The tone is warm, encouraging, and inspiring, leaving the user feeling empowered and motivated.

    ## Evaluation steps
    Step 1: Carefully read both the question and the generated response. Ensure a clear understanding of the user's intent, needs, and any specific context provided.
    Step 2: Evaluate how well the response directly addresses the user's query and demonstrates an understanding of their specific needs or preferences.
    Step 3: Determine the extent to which the response actively encourages further interaction and provides a tailored experience.
    Step 4: Evaluate Tone & Empathy: Analyze the tone of the response, ensuring it fosters a positive and supportive user experience, making the user feel heard and understood.
    Step 5: Based on the three criteria above, assign a score from 1 to 5 according to the score rubric.
    Step 5: Justify the assigned score with a clear and concise explanation, highlighting the strengths and weaknesses of the response with respect to each criterion.

    # Question : {question}
    # Generated response: {response}
    # Reference response: {target}
    """

    custom_metric_prompt = custom_metric_prompt_template.format(question=question, response=response, target=target)
    response_dict = get_autorater_response(custom_metric_prompt)

    return {
        "custom_engagement_personalization_score": response_dict["score"],
        "explanation": response_dict["explanation"],
    }

# Register an HTTP function with the Functions Framework
@functions_framework.http
def main(request):
  request_json = request.get_json(silent=True)

  if not request_json:
    raise ValueError('Cannot find request json.')

  question = request_json['question']
  response = request_json['response']
  reference = request_json['target']

  get_evaluation_result = evaluate_engagement_personalization_fn(question, response, reference)
  return json.dumps(get_evaluation_result)
'''

with open(BUILD_PATH / "main.py", "w") as f:
    f.write(custom_metric_function_code)
```

##### Deploy the custom metric as a Cloud Function

Use gcloud command line to deploy a Cloud function. To learn more, check out [Deploy a Python service to Cloud Run](https://cloud.google.com/run/docs/quickstarts/build-and-deploy/deploy-python-service) quickstart.



```
!gcloud functions deploy 'custom_engagement_personalization_metric' \
 --gen2 \
 --runtime="python310" \
 --source={str(BUILD_PATH)} \
 --entry-point=main \
 --trigger-http \
 --timeout=3600 \
 --memory=2Gb \
 --concurrency=6 \
 --min-instances=6 \
 --project {PROJECT_ID} \
 --region={REGION} \
 --quiet
```

##### Test your custom evaluation metric

After you deploy your  custom evaluation metric as Cloud function, submit a request to validate the output of the custom evaluation function.


```
custom_evaluator_function_uri = ! gcloud functions describe 'custom_engagement_personalization_metric' --gen2 --region {REGION} --format="value(url)"
custom_evaluator_function_uri = custom_evaluator_function_uri[0].strip()
```


```
headers = {
    "Authorization": f"Bearer {get_auth_token()}",
    "Content-Type": "application/json",
}

json_data = {
    "question": """
      What are some techniques for cooking red meat and pork that maximize flavor and tenderness while minimizing the formation of unhealthy compounds?
      """,
    "response": """
      * Marinating in acidic ingredients like lemon juice or vinegar to tenderize the meat \n * Cooking to an internal temperature of 145°F (63°C) for safety \n * Using high-heat cooking methods like grilling and pan-searing for browning and caramelization /n * Avoiding charring to minimize the formation of unhealthy compounds
      """,
    "target": """
      Here's how to tackle those delicious red meats and pork while keeping things healthy:
      **Prioritize Low and Slow:**
      * **Braising and Stewing:** These techniques involve gently simmering meat in liquid over low heat for an extended period. This breaks down tough collagen, resulting in incredibly tender and flavorful meat. Plus, since the cooking temperature is lower, it minimizes the formation of potentially harmful compounds associated with high-heat cooking.
      * **Sous Vide:** This method involves sealing meat in a vacuum bag and immersing it in a precisely temperature-controlled water bath.  It allows for even cooking to the exact desired doneness, resulting in incredibly juicy and tender meat.  Because the temperature is controlled and lower than traditional methods, it can be a healthier option.
      **High Heat Tips:**
      * **Marinades are Your Friend:** As you mentioned, acidic marinades tenderize meat.  They also add flavor!
      * **Temperature Control is Key:**  Use a meat thermometer to ensure you reach the safe internal temperature of 145°F (63°C) without overcooking.
      * **Don't Burn It!**  While some browning is desirable, charring creates those unhealthy compounds.  Pat meat dry before cooking to minimize steaming and promote browning.  Let the pan heat up properly before adding the meat to achieve a good sear.

      **Remember:**  Trim visible fat before cooking to reduce saturated fat content.  Let meat rest after cooking; this allows juices to redistribute, resulting in a more tender and flavorful final product.
      """,
}

response = requests.post(
    custom_evaluator_function_uri, headers=headers, json=json_data, timeout=70
).json()
pprint(response)
```

#### Configure optimization settings

Vertex AI prompt optimizer lets you control the optimization process by specifying what to optimize (instructions only, demonstrations only, or both), providing a system instruction and prompt template, and selecting the target model.  You can optionally refine the optimization with some advanced settings like its duration and the number of optimization iterations it runs, which models the Vertex AI prompt optimizer uses, and other parameters to control the structure and content of prompts. Below you have some common and recommended default configurations.

In this scenario, you set two additional parameters:

* `custom_metric_name` parameter which lets you pass your own custom metric to optimizer the prompt template.

* `custom_metric_cloud_function_name` parameter which indicates the Cloud function to call for collecting custom function evaluation metric output.

For more advanced control, you can learn and explore more about all the parameters and how to best use them in the [detailed documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer).



```
PROMPT_OPTIMIZATION_JOB = "auto-prompt-design-job-" + vapo_lib.get_id()
OUTPUT_OPTIMIZATION_RUN_URI = str(
    OUTPUT_OPTIMIZATION_DATA_URI / PROMPT_OPTIMIZATION_JOB
)

args = Namespace(
    # Basic configuration
    system_instruction=SYSTEM_INSTRUCTION_TEMPLATE,  # System instructions for the target model. String.
    prompt_template=PROMPT_TEMPLATE,  # Template for prompts,  String.
    target_model="gemini-1.5-flash-001",  # Target model for optimization. String. Supported models: "gemini-1.5-flash-002", "gemini-1.5-pro-002", "gemini-1.5-flash-001", "gemini-1.5-pro-001", "gemini-1.0-pro-001", "gemini-1.0-pro-002", "gemini-1.0-ultra-001", "text-bison@001", "text-bison@002", "text-bison32k@002", "text-unicorn@001"
    optimization_mode="instruction",  # Optimization mode. String. Supported modes: "instruction", "demonstration", "instruction_and_demo"
    custom_metric_name="custom_engagement_personalization_score",  # Metric name, as defined by the key that corresponds in the dictionary returned from Cloud function. String.
    custom_metric_cloud_function_name="custom_engagement_personalization_metric",  # Cloud Run function name you previously deployed. String.
    eval_metrics_types=[
        "question_answering_correctness",
        "custom_metric",
    ],  # List of evaluation metrics. List of strings. Supported metrics: "bleu", "coherence", "exact_match", "fluidity", "fulfillment", "groundedness", "rouge_1", "rouge_2", "rouge_l", "rouge_l_sum", "safety", "question_answering_correctness", "question_answering_helpfulness", "question_answering_quality", "question_answering_relevance", "summarization_helpfulness", "summarization_quality", "summarization_verbosity", "tool_name_match", "tool_parameter_key_match", "tool_parameter_kv_match"
    eval_metrics_weights=[
        0.8,
        0.2,
    ],  # Weights for evaluation metrics. List of floats.  Length must match eval_metrics_types.  Should sum to 1.
    aggregation_type="weighted_sum",  # Aggregation type for evaluation metrics. String. Supported aggregation types: "weighted_sum", "weighted_average"
    input_data_path=INPUT_OPTIMIZATION_DATA_FILE_URI,  # Cloud Storage URI to input optimization data. String.
    output_path=OUTPUT_OPTIMIZATION_RUN_URI,  # Cloud Storage URI to save optimization results. String.
    project=PROJECT_ID,  # Google Cloud project ID. String.
    # (Optional) Advanced configuration
    num_steps=10,  # Number of iterations in instruction optimization mode. Integer between 10 and 20.
    num_template_eval_per_step=2,  # Number of system instructions generated and evaluated in instruction and instruction_and_demo mode. Integer between 1 and 4.
    num_demo_set_candidates=10,  # Number of demonstrations evaluated in instruction and instruction_and_demo mode. Integer between 10 and 30.
    demo_set_size=3,  # Number of demonstrations generated per prompt. Integer between 3 and 6.
    target_model_location="us-central1",  # Location of the target model. String. Default us-central1.
    optimizer_model="gemini-1.5-pro-001",  # Optimization model. String. Supported models: "gemini-1.5-flash-002", "gemini-1.5-pro-002", "gemini-1.5-flash-001", "gemini-1.5-pro-001", "gemini-1.0-pro-001", "gemini-1.0-pro-002", "gemini-1.0-ultra-001", "text-bison@001", "text-bison@002", "text-bison32k@002", "text-unicorn@001"
    optimizer_model_location="us-central1",  # Location of the optimization model. String. Default us-central1.
    eval_model="gemini-1.5-pro-001",  # Evaluation model. String. Supported models: "gemini-1.5-flash-002", "gemini-1.5-pro-002", "gemini-1.5-flash-001", "gemini-1.5-pro-001", "gemini-1.0-pro-001", "gemini-1.0-pro-002", "gemini-1.0-ultra-001", "text-bison@001", "text-bison@002", "text-bison32k@002", "text-unicorn@001"
    eval_model_location="us-central1",  # Location of the evaluation model. String. Default us-central1.
    source_model="",  # Google model that the system instructions and prompts were previously used with. String. Not needed if you provide target column.
    source_model_location="",  # Location of the source model. String. Default us-central1. Not needed if you provide target column.
    target_model_qps=1,  # The queries per second (QPS) sent to the target model. Integer greater or equal than 1 depending on your quota.
    optimizer_model_qps=1,  # The queries per second (QPS) sent to the optimization model. Integer greater or equal than 1 depending on your quota.
    eval_qps=1,  # The queries per second (QPS) sent to the eval model. Integer greater or equal than 1 depending on your quota.
    source_model_qps="",  # The queries per second (QPS) sent to the source model. Integer greater or equal than 1 depending on your quota.
    response_mime_type="application/json",  # MIME response type that the target model uses. String. Supported response: text/plain, application/json.
    language="English",  # Language of the system instructions. String. Supported languages: "English", "French", "German", "Hebrew", "Hindi", "Japanese", "Korean", "Portuguese", "Simplified Chinese", "Spanish", "Traditional Chinese"
    placeholder_to_content=json.loads(
        "{}"
    ),  # Placeholder to replace any parameter in the system instruction. Dict.
    data_limit=50,  # Amount of data used for validation. Integer between 5 and 100.
)
```

#### Upload Vertex AI prompt optimizer Cloud Storage

After you define Vertex AI prompt optimizer configuration, you upload them on Cloud Storage bucket.



```
args = vars(args)

with epath.Path(CONFIG_FILE_URI).open("w") as config_file:
    json.dump(args, config_file)
config_file.close()
```

#### Run the automatic prompt optimization job

Now you are ready to run your first Vertex AI prompt optimizer job using the Vertex AI SDK for Python.

> This prompt optimization job requires ~ 40 minutes to run.

> Be sure you have provisioned enough queries per minute (QPM) quota implementing the recommended QPM for each model. If you configure the Vertex AI prompt optimizer with a QPM that is higher than the QPM than you have access to, the job might fail. [Check out](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer#before-you-begin) the documentation to know more.



```
WORKER_POOL_SPECS = [
    {
        "machine_spec": {
            "machine_type": "n1-standard-4",
        },
        "replica_count": 1,
        "container_spec": {
            "image_uri": APD_CONTAINER_URI,
            "args": ["--config=" + CONFIG_FILE_URI],
        },
    }
]

custom_job = aiplatform.CustomJob(
    display_name=PROMPT_OPTIMIZATION_JOB,
    worker_pool_specs=WORKER_POOL_SPECS,
)

custom_job.submit(service_account=SERVICE_ACCOUNT)
```

### Collect and display the optimization results

Vertex AI prompt optimizer returns both optimized templates and evaluation results for either instruction, or demostrations, or both depending on the optimization mode you define as JSONL files on Cloud Storage bucket. Those results help you understand the optimization process.

In this case, you want to collect the optimized templates and evaluation results for the system instruction.

Below you use a helper function to display those results.


```
OUTPUT_OPTIMIZATION_RUN_URI = "gs://inardini-prompt-optimization/optimization/optimization_jobs/auto-prompt-design-job-c3ct7tvc"
```


```
results_ui = vapo_lib.ResultsUI(OUTPUT_OPTIMIZATION_RUN_URI)
results_df_html = """

"""

display(HTML(results_df_html))
display(results_ui.get_container())
```

### Evaluate the quality of generated responses with the optimized instruction

Finally, you evaluate generated responses with the optimized instruction qualitatively.

If you want to know how to evaluate the new generated responses quantitatively, check out the [`vertex_ai_prompt_optimizer_sdk` notebook](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/prompts/prompt_optimizer) in the official repository.


#### Generate new responses using the optimized system instruction.

Set the optimized system instruction template you get from Vertex AI prompt optimizer job.


```
OPTIMIZED_SYSTEM_INSTRUCTION_TEMPLATE = "You are a brilliant culinary assistant, adept at extracting key information and offering insightful cooking advice. Using only the information provided in the context, answer the user's question accurately, comprehensively, and engagingly. Your response should be relevant, informative, and reflect your culinary expertise by incorporating key points from the context."
```

Prepare optimized prompts using the optimized system instruction template.


```
OPTIMIZED_PROMPT_TEMPLATE = (
    OPTIMIZED_SYSTEM_INSTRUCTION_TEMPLATE
    + "\nQuestion: \n{question}"
    + "\nContext: \n{context}"
    + "\nAnswer:"
)

optimized_prompts = [
    OPTIMIZED_PROMPT_TEMPLATE.format(question=q, context=c)
    for q, c in zip(
        test_prompt_optimization_df["user_question"].to_list(),
        test_prompt_optimization_df["context"].to_list(),
    )
]
```

Leverage Gemini API on Vertex AI to send parallel generation requests.


```
gemini_llm = GenerativeModel(model_name="gemini-1.5-flash-001")

gemini_predictions = [
    vapo_lib.async_generate(p, model=gemini_llm) for p in optimized_prompts
]

gemini_predictions_col = await tqdm_asyncio.gather(*gemini_predictions)
```

#### Evaluate new responses

Prepare the test dataset and inspect new responses.


```
test_prompt_optimization_df["optimized_prompt_with_vapo"] = optimized_prompts
test_prompt_optimization_df["gemini_answer_with_vapo"] = gemini_predictions_col
```


```
vapo_lib.print_df_rows(test_prompt_optimization_df, n=1)
```

## IV. Clean up


```
delete_bucket = False
delete_job = False
delete_run = False
delete_tutorial = False

if delete_bucket:
    ! gsutil rm -r {BUCKET_URI}

if delete_job:
    custom_job.delete()

if delete_run:
    ! gcloud functions delete 'custom_engagement_personalization_metric' --region={REGION}

if delete_tutorial:
    import shutil

    shutil.rmtree(str(TUTORIAL_PATH))
```




################################################## vertex_ai_prompt_optimizer_ui.md ##################################################


```
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# Vertex Prompt Optimizer Notebook UI (Preview)

<table align="left">
  <td style="text-align: center">
    <a href="https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_ui.ipynb">
      <img width="32px" src="https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg" alt="Google Colaboratory logo"><br> Open in Colab
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fprompts%2Fprompt_optimizer%2Fvertex_ai_prompt_optimizer_ui.ipynb">
      <img width="32px" src="https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN" alt="Google Cloud Colab Enterprise logo"><br> Open in Colab Enterprise
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_ui.ipynb">
      <img src="https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg" alt="Vertex AI logo"><br> Open in Vertex AI Workbench
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_ui.ipynb">
      <img width="32px" src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg" alt="GitHub logo"><br> View on GitHub
    </a>
  </td>
</table>

# Overview
This Notebook showcases the Vertex AI prompt optimizer, a tool that iteratively optimizes prompts to suit a target model (e.g., `gemini-1.5-pro`) using target-specific metric(s).

Key Use Cases:

* Prompt Optimization: Enhance the quality of an initial prompt by refining its structure and content to match the target model's optimal input characteristics.

* Prompt Translation: Adapt prompts optimized for one model to work effectively with a different target model.

For the detailed documentation please see [here](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer).

## Getting Started

### Authenticate your notebook environment (Colab only)

Authenticate your environment on Google Colab.



```
import sys

if "google.colab" in sys.modules:
    from google.colab import auth

    auth.authenticate_user()
```

# Step 0: Install packages and libraries


```
!wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vapo_lib.py
import vapo_lib
```

# Step 1: Create a prompt template and system instructions
Provide your system intruction and prompt template below. Refer to [here]( https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer#template-si) for instructions.

Prompts consist of two key components:

- System Instruction: System instruction is the instruction that get passed to the model before any user input in the prompt. This is the fixed part of the prompt template shared across all queries for a given task.
- Prompt template: A task is the text in the prompt that you want the model to provide a response for. Context is information that you include in the prompt that the model uses or references when generating a response. These are the dynamic parts of the prompt template that changes based on the task.

Prompt Optimizer enables the optimization or translation of the System Instruction template, while the prompt template remains essential for evaluating and selecting the best System Instruction template.


```
SYSTEM_INSTRUCTION = "Answer the following question. Let's think step by step.\n"  # @param {type:"string"}
PROMPT_TEMPLATE = "Question: {question}\n\nAnswer:{target}"  # @param {type:"string"}
```

# Step 2: Configure project settings
To optimize the prompt for your target Google model, provide a CSV or JSONL file containing labeled validation samples (input, ground truth output pairs). Refer to [here](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer#prepare-sample-prompts) for instructions.

Focus on examples that specifically demonstrate the issues you want to address.
Recommendation: Use 50-100 distinct samples for reliable results. However, the tool can still be effective with as few as 5 samples.
For prompt translation (e.g. 3P model to Google model, PaLM 2 to Gemini):

Consider using the source model to label examples that the target model struggles with, helping to identify areas for improvement.
When you select a source model, you don't need to provide labels for the input examples.
While the source model selection is limited to Google models, it still supports labeled inputs from non-Google models. If you wish to select a non-Google source model, you will need to provide labels for your input examples.



```
# @markdown **Project setup**: <br/>
PROJECT_ID = "[YOUR_PROJECT]"  # @param {type:"string"}
LOCATION = "us-central1"  # @param {type:"string"}
OUTPUT_PATH = "[OUTPUT_PATH]"  # @param {type:"string"}
INPUT_DATA_PATH = "[INPUT_DATA_PATH]"  # @param {type:"string"}
```

# Step 3: Configure optimization settings
The optimization configurations are defaulted to the values that are most commonly used, which we recommend using as the initial set-up.

The most important settings are:

* Target Model: Which model you are trying to optimize your prompts to.
* Optimization Mode: The mode in which you are trying to optimize your prompt with.
* Evaluation Metrics: The evaluation metrics in which you are trying to optimize your prompts against.
Refer [here](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer#configuration) to learn more about the different configuration settings and how to best utilize them.


```
SOURCE_MODEL = ""  # @param ["", "gemini-1.0-pro-001", "gemini-1.0-pro-002", "gemini-1.5-flash-001", "gemini-1.5-flash-002", "gemini-1.5-pro-001", "gemini-1.5-pro-002", "gemini-1.0-ultra-001", "gemini-experimental", "gemini-flash-experimental", "gemini-pro-experimental", "text-bison@001", "text-bison@002", "text-bison32k@002", "text-unicorn@001"]
TARGET_MODEL = "gemini-1.5-flash-001"  # @param ["gemini-1.0-pro-001", "gemini-1.0-pro-002", "gemini-1.5-flash-001", "gemini-1.5-flash-002", "gemini-1.5-pro-001", "gemini-1.5-pro-002", "gemini-1.0-ultra-001", "gemini-experimental", "gemini-flash-experimental", "gemini-pro-experimental"]
OPTIMIZATION_MODE = "instruction_and_demo"  # @param ["instruction", "demonstration", "instruction_and_demo"]
EVAL_METRIC = "question_answering_correctness"  # @param ["bleu", "coherence", "exact_match", "fluency", "groundedness", "text_quality", "verbosity", "rouge_1", "rouge_2", "rouge_l", "rouge_l_sum", "safety", "question_answering_correctness", "question_answering_quality", "summarization_quality", "tool_name_match", "tool_parameter_key_match", "tool_parameter_kv_match", "tool_call_valid"] {type:"string"}
```

# Step 4: Configure advanced optimization settings [Optional]
Refer [here](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer#configuration) to learn more about the different configuration settings and how to best utilize them.


```
# @markdown **Instruction Optimization Configs**: <br/>
NUM_INST_OPTIMIZATION_STEPS = 10  # @param {type:"integer"}
NUM_TEMPLATES_PER_STEP = 2  # @param {type:"integer"}

# @markdown **Demonstration Optimization Configs**: <br/>
NUM_DEMO_OPTIMIZATION_STEPS = 10  # @param {type:"integer"}
NUM_DEMO_PER_PROMPT = 3  # @param {type:"integer"}

# @markdown **Model Configs**: <br/>
TARGET_MODEL_QPS = 3.0  # @param {type:"number"}
SOURCE_MODEL_QPS = 3.0  # @param {type:"number"}
EVAL_QPS = 3.0  # @param {type:"number"}

# @markdown **Multi-metric Configs**: <br/>
# @markdown Use this section only if you need more than one metric for optimization. This will override the metric you picked above.
EVAL_METRIC_1 = "NA"  # @param ["NA", "bleu", "coherence", "exact_match", "fluency", "groundedness", "text_quality", "verbosity", "rouge_1", "rouge_2", "rouge_l", "rouge_l_sum", "safety", "question_answering_correctness", "question_answering_quality", "summarization_quality", "tool_name_match", "tool_parameter_key_match", "tool_parameter_kv_match", "tool_call_valid"] {type:"string"}
EVAL_METRIC_1_WEIGHT = 0.0  # @param {type:"number"}
EVAL_METRIC_2 = "NA"  # @param ["NA", "bleu", "coherence", "exact_match", "fluency", "groundedness", "text_quality", "verbosity", "rouge_1", "rouge_2", "rouge_l", "rouge_l_sum", "safety", "question_answering_correctness", "question_answering_quality", "summarization_quality", "tool_name_match", "tool_parameter_key_match", "tool_parameter_kv_match", "tool_call_valid"] {type:"string"}
EVAL_METRIC_2_WEIGHT = 0.0  # @param {type:"number"}
EVAL_METRIC_3 = "NA"  # @param ["NA", "bleu", "coherence", "exact_match", "fluency", "groundedness", "text_quality", "verbosity", "rouge_1", "rouge_2", "rouge_l", "rouge_l_sum", "safety", "question_answering_correctness", "question_answering_quality", "summarization_quality", "tool_name_match", "tool_parameter_key_match", "tool_parameter_kv_match", "tool_call_valid"] {type:"string"}
EVAL_METRIC_3_WEIGHT = 0.0  # @param {type:"number"}
METRIC_AGGREGATION_TYPE = "weighted_sum"  # @param ["weighted_sum", "weighted_average"]

# @markdown **Misc Configs**: <br/>
PLACEHOLDER_TO_VALUE = "{}"  # @param
RESPONSE_MIME_TYPE = (
    "application/json"  # @param ["text/plain", "application/json", "text/x.enum"]
)
RESPONSE_SCHEMA = ""
TARGET_LANGUAGE = "English"  # @param ["English", "French", "German", "Hebrew", "Hindi", "Japanese", "Korean", "Portuguese", "Simplified Chinese", "Spanish", "Traditional Chinese"]
TOOLS = ""  # @param
TOOL_CONFIG = ""  # @param
```

# Step 5: Run Prompt Optimizer
A progress bar will appear to let you know how long the job takes.


```
import datetime
import json
import time

timestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
display_name = f"pt_{timestamp}"

label_enforced = vapo_lib.is_run_target_required(
    [
        EVAL_METRIC,
        EVAL_METRIC_1,
        EVAL_METRIC_2,
        EVAL_METRIC_3,
    ],
    SOURCE_MODEL,
)
input_data_path = f"{INPUT_DATA_PATH}"
vapo_lib.validate_prompt_and_data(
    "\n".join([SYSTEM_INSTRUCTION, PROMPT_TEMPLATE]),
    input_data_path,
    PLACEHOLDER_TO_VALUE,
    label_enforced,
)

output_path = f"{OUTPUT_PATH}/{display_name}"

params = {
    "project": PROJECT_ID,
    "num_steps": NUM_INST_OPTIMIZATION_STEPS,
    "system_instruction": SYSTEM_INSTRUCTION,
    "prompt_template": PROMPT_TEMPLATE,
    "target_model": TARGET_MODEL,
    "target_model_qps": TARGET_MODEL_QPS,
    "target_model_location": LOCATION,
    "source_model": SOURCE_MODEL,
    "source_model_qps": SOURCE_MODEL_QPS,
    "source_model_location": LOCATION,
    "eval_qps": EVAL_QPS,
    "eval_model_location": LOCATION,
    "optimization_mode": OPTIMIZATION_MODE,
    "num_demo_set_candidates": NUM_DEMO_OPTIMIZATION_STEPS,
    "demo_set_size": NUM_DEMO_PER_PROMPT,
    "aggregation_type": METRIC_AGGREGATION_TYPE,
    "data_limit": 50,
    "num_template_eval_per_step": NUM_TEMPLATES_PER_STEP,
    "input_data_path": input_data_path,
    "output_path": output_path,
    "response_mime_type": RESPONSE_MIME_TYPE,
    "response_schema": RESPONSE_SCHEMA,
    "language": TARGET_LANGUAGE,
    "placeholder_to_content": json.loads(PLACEHOLDER_TO_VALUE),
    "tools": TOOLS,
    "tool_config": TOOL_CONFIG,
}

if EVAL_METRIC_1 == "NA":
    params["eval_metrics_types"] = [EVAL_METRIC]
    params["eval_metrics_weights"] = [1.0]
else:
    metrics = []
    weights = []
    for metric in [EVAL_METRIC_1, EVAL_METRIC_2, EVAL_METRIC_3]:
        if metric == "NA":
            break
        metrics.append(metric)
        weights.append(EVAL_METRIC_1_WEIGHT)
    params["eval_metrics_types"] = metrics
    params["eval_metrics_weights"] = weights

job = vapo_lib.run_apd(params, OUTPUT_PATH, display_name)
print(f"Job ID: {job.name}")

progress_form = vapo_lib.ProgressForm(params)
while progress_form.monitor_progress(job):
    time.sleep(5)
```

# Step 6: Inspect the results
For a clearer look at the specific responses generated by each prompt template during the optimization process, use the cell below.
This will allow you to inspect all the predictions made by all the
generated templates during one or multiple vertex prompt optimizer runs.



```
from IPython.display import HTML, display

RESULT_PATH = "[OUTPUT_PATH]"  # @param {type:"string"}

results_ui = vapo_lib.ResultsUI(RESULT_PATH)

results_df_html = """
<style>
  .scrollable {
    width: 100%;
    height: 80px;
    overflow-y: auto;
    overflow-x: hidden;  /* Hide horizontal scrollbar */
  }
  tr:nth-child(odd) {
    background: var(--colab-highlighted-surface-color);
  }
  tr:nth-child(even) {
    background-color: var(--colab-primary-surface-color);
  }
  th {
    background-color: var(--colab-highlighted-surface-color);
  }
</style>
"""

display(HTML(results_df_html))
display(results_ui.get_container())
```




################################################## vertex_ai_text_generation_inference_gemma.md ##################################################


```
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# Hugging Face DLCs: Serving Gemma with Text Generation Inference (TGI) on Vertex AI

<table align="left">
  <td style="text-align: center">
    <a href="https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/open-models/serving/vertex_ai_text_generation_inference_gemma.ipynb">
      <img width="32px" src="https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg" alt="Google Colaboratory logo"><br> Open in Colab
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fopen-models%2Fserving%2Fvertex_ai_text_generation_inference_gemma.ipynb">
      <img width="32px" src="https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN" alt="Google Cloud Colab Enterprise logo"><br> Open in Colab Enterprise
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/open-models/serving/vertex_ai_text_generation_inference_gemma.ipynb">
      <img src="https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg" alt="Vertex AI logo"><br> Open in Vertex AI Workbench
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/serving/vertex_ai_text_generation_inference_gemma.ipynb">
      <img width="32px" src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg" alt="GitHub logo"><br> View on GitHub
    </a>
  </td>
</table>

![Hugging Face x Google Cloud](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/google-cloud/thumbnail.png)

| | |
|-|-|
| Author(s) | [Alvaro Bartolome](https://github.com/alvarobartt), [Philipp Schmid](https://github.com/philschmid), [Simon Pagezy](https://github.com/pagezyhf), and [Jeff Boudier](https://github.com/jeffboudier) |

## Overview

> [**Gemma**](https://ai.google.dev/gemma) is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models, developed by Google DeepMind and other teams across Google.

> [**Text Generation Inference (TGI)**](https://github.com/huggingface/text-generation-inference) is a toolkit developed by Hugging Face for deploying and serving LLMs, with high performance text generation.

> [**Hugging Face DLCs**](https://github.com/huggingface/Google-Cloud-Containers) are pre-built and optimized Deep Learning Containers (DLCs) maintained by Hugging Face and Google Cloud teams to simplify environment configuration for your ML workloads.

> [**Google Vertex AI**](https://cloud.google.com/vertex-ai) is a Machine Learning (ML) platform that lets you train and deploy ML models and AI applications, and customize large language models (LLMs) for use in your AI-powered applications.

This notebook showcases how to deploy Google Gemma from the Hugging Face Hub on Vertex AI using the Hugging Face Deep Learning Container (DLC) for Text Generation Inference (TGI).

By the end of this notebook, you will learn how to:

- Register any LLM from the Hugging Face Hub on Vertex AI
- Deploy an LLM on Vertex AI
- Send online predictions on Vertex AI

## Get started

### Install Vertex AI SDK and other required packages

To run this example, you will only need the [`google-cloud-aiplatform`](https://github.com/googleapis/python-aiplatform) Python SDK and the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) Python package.


```
%pip install --upgrade --user --quiet google-cloud-aiplatform huggingface_hub
```

### Restart runtime (Colab only)

To use the newly installed packages in this Jupyter environment, if you are on Colab you must restart the runtime. You can do this by running the cell below, which restarts the current kernel. The restart might take a minute or longer. After it's restarted, continue to the next step.


```
# Automatically restart kernel after installs so that your environment can access the new packages
# import IPython

# app = IPython.Application.instance()
# app.kernel.do_shutdown(True)
```

<div class="alert alert-block alert-warning">
<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>
</div>


### Authenticate your Google Cloud account

Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below.

**1. Vertex AI Workbench**

* Do nothing as you are already authenticated.

**2. Local JupyterLab instance, uncomment and run:**


```
# !gcloud auth login
```

**3. Colab, uncomment and run:**


```
# from google.colab import auth
# auth.authenticate_user()
```

### Authenticate your Hugging Face account

As [`google/gemma-7b-it`](https://huggingface.co/google/gemma-7b-it) is a gated model, you need to have a Hugging Face Hub account, and accept the Google's usage license for Gemma. Once that's done, you need to generate a new user access token with read-only access so that the weights can be downloaded from the Hub in the Hugging Face DLC for TGI.

> Note that the user access token can only be generated via [the Hugging Face Hub UI](https://huggingface.co/settings/tokens/new), where you can either select read-only access to your account, or follow the recommendations and generate a fine-grained token with read-only access to [`google/gemma-7b-it`](https://huggingface.co/google/gemma-7b-it).

Then you can install the `huggingface_hub` that comes with a CLI that will be used for the authentication with the token generated in advance. So that then the token can be safely retrieved via `huggingface_hub.get_token`.


```
from huggingface_hub import interpreter_login

interpreter_login()
```

Read more about [Hugging Face Security](https://huggingface.co/docs/hub/en/security), specifically about [Hugging Face User Access Tokens](https://huggingface.co/docs/hub/en/security-tokens).

### Set Google Cloud project information and initialize Vertex AI SDK

To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com), if not enabled already.

Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).


```
import os

from google.cloud import aiplatform

PROJECT_ID = "[your-project-id]"  # @param {type:"string", isTemplate: true}
if PROJECT_ID == "[your-project-id]":
    PROJECT_ID = str(os.environ.get("GOOGLE_CLOUD_PROJECT"))

LOCATION = os.environ.get("GOOGLE_CLOUD_REGION", "us-central1")

aiplatform.init(project=PROJECT_ID, location=LOCATION)
```

## Requirements

You will need to have the following IAM roles set:

- Artifact Registry Reader (roles/artifactregistry.reader)
- Vertex AI User (roles/aiplatform.user)

For more information about granting roles, see [Manage access](https://cloud.google.com/iam/docs/granting-changing-revoking-access).

---

You will also need to enable the following APIs (if not enabled already):

- Vertex AI API (aiplatform.googleapis.com)
- Artifact Registry API (artifactregistry.googleapis.com)

For more information about API enablement, see [Enabling APIs](https://cloud.google.com/apis/docs/getting-started#enabling_apis).

---

To access Gemma on Hugging Face, you’re required to review and agree to Google’s usage license on the Hugging Face Hub for any of the models from the [Gemma release collection](https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b), and the access request will be processed inmediately.

## Register Google Gemma on Vertex AI

To serve Gemma with Text Generation Inference (TGI) on Vertex AI, you start importing the model on Vertex AI Model Registry.

The Vertex AI Model Registry is a central repository where you can manage the lifecycle of your ML models. From the Model Registry, you have an overview of your models so you can better organize, track, and train new versions. When you have a model version you would like to deploy, you can assign it to an endpoint directly from the registry, or using aliases, deploy models to an endpoint. The models on the Vertex AI Model Registry just contain the model configuration and not the weights per se, as it's not a storage.

Before going into the code to upload or import a model on Vertex AI, let's quickly review the arguments provided to the `aiplatform.Model.upload` method:

* **`display_name`** is the name that will be shown in the Vertex AI Model Registry.

* **`serving_container_image_uri`** is the location of the Hugging Face DLC for TGI that will be used for serving the model.

* **`serving_container_environment_variables`** are the environment variables that will be used during the container runtime, so these are aligned with the environment variables defined by `text-generation-inference`, which are analog to the [`text-generation-launcher` arguments](https://huggingface.co/docs/text-generation-inference/en/basic_tutorials/launcher). Additionally, the Hugging Face DLCs for TGI also capture the `AIP_` environment variables from Vertex AI as in [Vertex AI Documentation - Custom container requirements for prediction](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements).

    * `MODEL_ID` is the identifier of the model in the Hugging Face Hub. To explore all the supported models you can check [the Hugging Face Hub](https://huggingface.co/models?sort=trending&other=text-generation-inference).
    * `NUM_SHARD` is the number of shards to use if you don't want to use all GPUs on a given machine e.g. if you have two GPUs but you just want to use one for TGI then `NUM_SHARD=1`, otherwise it matches the `CUDA_VISIBLE_DEVICES`.
    * `MAX_INPUT_TOKENS` is the maximum allowed input length (expressed in number of tokens), the larger it is, the larger the prompt can be, but also more memory will be consumed.
    * `MAX_TOTAL_TOKENS` is the most important value to set as it defines the "memory budget" of running clients requests, the larger this value, the larger amount each request will be in your RAM and the less effective batching can be.
    * `MAX_BATCH_PREFILL_TOKENS` limits the number of tokens for the prefill operation, as it takes the most memory and is compute bound, it is interesting to limit the number of requests that can be sent.
    * `HUGGING_FACE_HUB_TOKEN` is the Hugging Face Hub token, required as [`google/gemma-7b-it`](https://huggingface.co/google/gemma-7b-it) is a gated model.

* (optional) **`serving_container_ports`** is the port where the Vertex AI endpoint |will be exposed, by default 8080.

For more information on the supported `aiplatform.Model.upload` arguments, check [its Python reference](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_upload).


```
from huggingface_hub import get_token

model = aiplatform.Model.upload(
    display_name="google--gemma-7b-it",
    serving_container_image_uri="us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310",
    serving_container_environment_variables={
        "MODEL_ID": "google/gemma-7b-it",
        "NUM_SHARD": "1",
        "MAX_INPUT_TOKENS": "512",
        "MAX_TOTAL_TOKENS": "1024",
        "MAX_BATCH_PREFILL_TOKENS": "1512",
        "HUGGING_FACE_HUB_TOKEN": get_token(),
    },
    serving_container_ports=[8080],
)
model.wait()
```

## Deploy Google Gemma on Vertex AI

After the model is registered on Vertex AI, you can deploy the model to an endpoint.

You need to first deploy a model to an endpoint before that model can be used to serve online predictions. Deploying a model associates physical resources with the model so it can serve online predictions with low latency.

Before going into the code to deploy a model to an endpoint, let's quickly review the arguments provided to the `aiplatform.Model.deploy` method:

- **`endpoint`** is the endpoint to deploy the model to, which is optional, and by default will be set to the model display name with the `_endpoint` suffix.
- **`machine_type`**, **`accelerator_type`** and **`accelerator_count`** are arguments that define which instance to use, and additionally, the accelerator to use and the number of accelerators, respectively. The `machine_type` and the `accelerator_type` are tied together, so you will need to select an instance that supports the accelerator that you are using and vice-versa. More information about the different instances at [Compute Engine Documentation - GPU machine types](https://cloud.google.com/compute/docs/gpus), and about the `accelerator_type` naming at [Vertex AI Documentation - MachineSpec](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec).

For more information on the supported `aiplatform.Model.deploy` arguments, you can check [its Python reference](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_deploy).


```
deployed_model = model.deploy(
    endpoint=aiplatform.Endpoint.create(display_name="google--gemma-7b-it-endpoint"),
    machine_type="g2-standard-4",
    accelerator_type="NVIDIA_L4",
    accelerator_count=1,
)
```

> Note that the model deployment on Vertex AI can take around 15 to 25 minutes; most of the time being the allocation / reservation of the resources, setting up the network and security, and such.

## Online predictions on Vertex AI

Once the model is deployed on Vertex AI, you can run the online predictions using the `aiplatform.Endpoint.predict` method, which will send the requests to the running endpoint in the `/predict` route specified within the container following Vertex AI I/O payload formatting.

As you are serving a `text-generation` model fine-tuned for instruction-following, you will need to make sure that the chat template, if any, is applied correctly to the input conversation; meaning that `transformers` need to be installed so as to instantiate the `tokenizer` for [`google/gemma-7b-it`](https://huggingface.co/google/gemma-7b-it) and run the `apply_chat_template` method over the input conversation before sending the input within the payload to the Vertex AI endpoint.

> Note that the Messages API will be supported on Vertex AI on upcoming TGI releases, starting on 2.3, meaning that at the time of writing this post, the prompts need to be formatted before sending the request if you want to achieve nice results (assuming you are using an intruction-following model and not a base model).


```
%pip install --upgrade --user --quiet transformers jinja2
```

After the installation is complete, the following snippet will apply the chat template to the conversation:


```
from huggingface_hub import get_token
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google/gemma-7b-it", token=get_token())

messages = [
    {"role": "user", "content": "What's Deep Learning?"},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)
# <bos><start_of_turn>user\nWhat's Deep Learning?<end_of_turn>\n<start_of_turn>model\n
```

Which is what you will be sending within the payload to the deployed Vertex AI Endpoint, as well as [the generation parameters](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation).

### Via Python

#### Within the same session

To run the online prediction via the Vertex AI SDK, you can simply use the `predict` method.


```
output = deployed_model.predict(
    instances=[
        {
            "inputs": "<bos><start_of_turn>user\nWhat's Deep Learning?<end_of_turn>\n<start_of_turn>model\n",
            "parameters": {
                "max_new_tokens": 20,
                "do_sample": True,
                "top_p": 0.95,
                "temperature": 1.0,
            },
        },
    ]
)
output
```

#### From a different session

To run the online prediction from a different session, you can run the following snippet.


```
import os

from google.cloud import aiplatform

PROJECT_ID = "[your-project-id]"  # @param {type:"string", isTemplate: true}
if PROJECT_ID == "[your-project-id]":
    PROJECT_ID = str(os.environ.get("GOOGLE_CLOUD_PROJECT"))

LOCATION = os.environ.get("GOOGLE_CLOUD_REGION", "us-central1")

aiplatform.init(project=PROJECT_ID, location=LOCATION)

endpoint_display_name = (
    "google--gemma-7b-it-endpoint"  # TODO: change to your endpoint display name
)

# Iterates over all the Vertex AI Endpoints within the current project and keeps the first match (if any), otherwise set to None
ENDPOINT_ID = next(
    (
        endpoint.name
        for endpoint in aiplatform.Endpoint.list()
        if endpoint.display_name == endpoint_display_name
    ),
    None,
)
assert ENDPOINT_ID, (
    "`ENDPOINT_ID` is not set, please make sure that the `endpoint_display_name` is correct at "
    f"https://console.cloud.google.com/vertex-ai/online-prediction/endpoints?project={os.getenv('PROJECT_ID')}"
)

endpoint = aiplatform.Endpoint(
    f"projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}"
)
output = endpoint.predict(
    instances=[
        {
            "inputs": "<bos><start_of_turn>user\nWhat's Deep Learning?<end_of_turn>\n<start_of_turn>model\n",
            "parameters": {
                "max_new_tokens": 20,
                "do_sample": True,
                "top_p": 0.95,
                "temperature": 0.7,
            },
        },
    ],
)
output
```

### Via gcloud

You can also send the requests using the `gcloud` CLI via the `gcloud ai endpoints` command.

> Note that, before proceeding, you should either replace the values or set the following environment variables in advance from the Python variables set in the example, as follows:
>
> ```python
> import os
> os.environ["PROJECT_ID"] = PROJECT_ID
> os.environ["LOCATION"] = LOCATION
> os.environ["ENDPOINT_NAME"] = "google--gemma-7b-it-endpoint"
> ```


```bash
%%bash
ENDPOINT_ID=$(gcloud ai endpoints list \
  --project=$PROJECT_ID \
  --region=$LOCATION \
  --filter="display_name=$ENDPOINT_NAME" \
  --format="value(name)" \
  | cut -d'/' -f6)

echo '{
  "instances": [
    {
      "inputs": "<bos><start_of_turn>user\nWhat'\''s Deep Learning?<end_of_turn>\n<start_of_turn>model\n",
      "parameters": {
        "max_new_tokens": 20,
        "do_sample": true,
        "top_p": 0.95,
        "temperature": 1.0
      }
    }
  ]
}' | gcloud ai endpoints predict $ENDPOINT_ID \
  --project=$PROJECT_ID \
  --region=$LOCATION \
  --json-request="-"
```

### Via cURL

Alternatively, you can also send the requests via `cURL`.

> Note that, before proceeding, you should either replace the values or set the following environment variables in advance from the Python variables set in the example, as follows:
>
> ```python
> import os
> os.environ["PROJECT_ID"] = PROJECT_ID
> os.environ["LOCATION"] = LOCATION
> os.environ["ENDPOINT_NAME"] = "google--gemma-7b-it-endpoint"
> ```


```bash
%%bash
ENDPOINT_ID=$(gcloud ai endpoints list \
  --project=$PROJECT_ID \
  --region=$LOCATION \
  --filter="display_name=$ENDPOINT_NAME" \
  --format="value(name)" \
  | cut -d'/' -f6)

curl -X POST \
    -H "Authorization: Bearer $(gcloud auth print-access-token)" \
    -H "Content-Type: application/json" \
    https://$LOCATION-aiplatform.googleapis.com/v1/projects/$PROJECT_ID/locations/$LOCATION/endpoints/$ENDPOINT_ID:predict \
    -d '{
        "instances": [
            {
                "inputs": "<bos><start_of_turn>user\nWhat'\''s Deep Learning?<end_of_turn>\n<start_of_turn>model\n",
                "parameters": {
                    "max_new_tokens": 20,
                    "do_sample": true,
                    "top_p": 0.95,
                    "temperature": 1.0
                }
            }
        ]
    }'
```

## Cleaning up

Finally, you can already release the resources that you've created as follows, to avoid unnecessary costs:

- `deployed_model.undeploy_all` to undeploy the model from all the endpoints.
- `deployed_model.delete` to delete the endpoint/s where the model was deployed gracefully, after the `undeploy_all` method.
- `model.delete` to delete the model from the registry.


```
deployed_model.undeploy_all()
deployed_model.delete()
model.delete()
```

Alternatively, you can also remove those from the Google Cloud Console following the steps:

- Go to Vertex AI in Google Cloud
- Go to Deploy and use -> Online prediction
- Click on the endpoint and then on the deployed model/s to "Undeploy model from endpoint"
- Then go back to the endpoint list and remove the endpoint
- Finally, go to Deploy and use -> Model Registry, and remove the model

## References

- [GitHub Repository - Hugging Face DLCs for Google Cloud](https://github.com/huggingface/Google-Cloud-Containers): contains all the containers developed by the collaboration of both Hugging Face and Google Cloud teams; as well as a lot of examples on both training and inference, covering both CPU and GPU, as well support for most of the models within the Hugging Face Hub.
- [Google Cloud Documentation - Hugging Face DLCs](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face): contains a table with the latest released Hugging Face DLCs on Google Cloud.
- [Google Artifact Registry - Hugging Face DLCs](https://console.cloud.google.com/artifacts/docker/deeplearning-platform-release/us/gcr.io): contains all the DLCs released by Google Cloud that can be used.
- [Hugging Face Documentation - Google Cloud](https://huggingface.co/docs/google-cloud): contains the official Hugging Face documentation for the Google Cloud DLCs.




################################################## vertex_ai_trl_fine_tuning_gemma.md ##################################################


```
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# Hugging Face DLCs: Fine-tuning Gemma with Transformer Reinforcement Learning (TRL) on Vertex AI

<table align="left">
  <td style="text-align: center">
    <a href="https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/open-models/fine-tuning/vertex_ai_trl_fine_tuning_gemma.ipynb">
      <img width="32px" src="https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg" alt="Google Colaboratory logo"><br> Open in Colab
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fopen-models%2Ffine-tuning%2Fvertex_ai_trl_fine_tuning_gemma.ipynb">
      <img width="32px" src="https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN" alt="Google Cloud Colab Enterprise logo"><br> Open in Colab Enterprise
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/open-models/fine-tuning/vertex_ai_trl_fine_tuning_gemma.ipynb">
      <img src="https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg" alt="Vertex AI logo"><br> Open in Vertex AI Workbench
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/fine-tuning/vertex_ai_trl_fine_tuning_gemma.ipynb">
      <img width="32px" src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg" alt="GitHub logo"><br> View on GitHub
    </a>
  </td>
</table>

![Hugging Face x Google Cloud](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/google-cloud/thumbnail.png)

| | |
|-|-|
| Author(s) | [Alvaro Bartolome](https://github.com/alvarobartt), [Philipp Schmid](https://github.com/philschmid), [Simon Pagezy](https://github.com/pagezyhf), and [Jeff Boudier](https://github.com/jeffboudier) |

## Overview

> [**Gemma**](https://ai.google.dev/gemma) is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models, developed by Google DeepMind and other teams across Google.

> [**Transformer Reinforcement Learning (TRL)**](https://github.com/huggingface/trl) is a framework developed by Hugging Face to fine-tune and align both transformer language and diffusion models using methods such as Supervised Fine-Tuning (SFT), Reward Modeling (RM), Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), and others.

> [**Hugging Face DLCs**](https://github.com/huggingface/Google-Cloud-Containers) are pre-built and optimized Deep Learning Containers (DLCs) maintained by Hugging Face and Google Cloud teams to simplify environment configuration for your ML workloads.

> [**Google Vertex AI**](https://cloud.google.com/vertex-ai) is a Machine Learning (ML) platform that lets you train and deploy ML models and AI applications, and customize large language models (LLMs) for use in your AI-powered applications.

This notebook showcases how to fine-tune Google Gemma with Supervised Fine-Tuning (SFT) and Low-Rank Adaptation (LoRA) in a single GPU via a custom job on Vertex AI using the Hugging Face PyTorch Deep Learning Container (DLC) for Training.

By the end of this notebook, you will learn:

- About Hugging Face's TRL and LLM fine-tuning
- How to create and run a custom container job on Vertex AI

## Get started

### Install Vertex AI SDK and other required packages

To run this example, you will only need the [`google-cloud-aiplatform`](https://github.com/googleapis/python-aiplatform) Python SDK and the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) Python package.


```
%pip install --upgrade --user --quiet google-cloud-aiplatform huggingface_hub
```

### Restart runtime (Colab only)

To use the newly installed packages in this Jupyter environment, if you are on Colab you must restart the runtime. You can do this by running the cell below, which restarts the current kernel. The restart might take a minute or longer. After it's restarted, continue to the next step.


```
# Automatically restart kernel after installs so that your environment can access the new packages
# import IPython

# app = IPython.Application.instance()
# app.kernel.do_shutdown(True)
```

<div class="alert alert-block alert-warning">
<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>
</div>


### Authenticate your Google Cloud account

Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below.

**1. Vertex AI Workbench**

* Do nothing as you are already authenticated.

**2. Local JupyterLab instance, uncomment and run:**


```
# !gcloud auth login
```

**3. Colab, uncomment and run:**


```
# from google.colab import auth
# auth.authenticate_user()
```

### Authenticate your Hugging Face account

As [`google/gemma-2b`](https://huggingface.co/google/gemma-2b) is a gated model, you need to have a Hugging Face Hub account, and accept the Google's usage license for Gemma. Once that's done, you need to generate a new user access token with read-only access so that the weights can be downloaded from the Hub in the Hugging Face DLC for TGI.

> Note that the user access token can only be generated via [the Hugging Face Hub UI](https://huggingface.co/settings/tokens/new), where you can either select read-only access to your account, or follow the recommendations and generate a fine-grained token with read-only access to [`google/gemma-2b`](https://huggingface.co/google/gemma-2b).

Then you can install the `huggingface_hub` that comes with a CLI that will be used for the authentication with the token generated in advance. So that then the token can be safely retrieved via `huggingface_hub.get_token`.


```
from huggingface_hub import interpreter_login

interpreter_login()
```

Read more about [Hugging Face Security](https://huggingface.co/docs/hub/en/security), specifically about [Hugging Face User Access Tokens](https://huggingface.co/docs/hub/en/security-tokens).

### Set Google Cloud environment variables

You will need to set the following environment variables that are required to run this example.


```
import os

PROJECT_ID = "[your-project-id]"  # @param {type:"string", isTemplate: true}
if PROJECT_ID == "[your-project-id]":
    PROJECT_ID = str(os.environ.get("GOOGLE_CLOUD_PROJECT"))
os.environ["PROJECT_ID"] = PROJECT_ID

os.environ["LOCATION"] = os.environ.get("GOOGLE_CLOUD_REGION", "us-central1")
```

### (Optional) Create a bucket in Google Cloud Storage (GCS)

As the custom job on Vertex AI will need to dump the fine-tuned artifacts somewhere on Google Cloud, you will need to have a bucket available on Google Cloud Storage (GCS), so that you can specify it as the bucket to be used within the custom job, so that anything written within the container in that path is automatically uploaded to GCS.


```
BUCKET_URI = "[your-bucket-uri]"  # @param {type:"string", isTemplate: true}
if BUCKET_URI == "[your-bucket-uri]":
    raise ValueError(
        "A valid BUCKET_URI (e.g. `gs://path/to/bucket`) needs to be specified"
    )
os.environ["BUCKET_URI"] = BUCKET_URI
```

> Uncomment the `gcloud storage buckets create` command below if you need to create a bucket on GCS.


```
# !gcloud storage buckets create $BUCKET_URI --project $PROJECT_ID --location=$LOCATION --default-storage-class=STANDARD --uniform-bucket-level-access
```

### Initialize Vertex AI SDK

To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com), if not enabled already.

Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).


```
aiplatform.init(
    project=os.getenv("PROJECT_ID"),
    location=os.getenv("LOCATION"),
    staging_bucket=os.getenv("BUCKET_URI"),
)
```

## Requirements

You will need to have the following IAM roles set:

- Artifact Registry Reader (roles/artifactregistry.reader)
- Vertex AI User (roles/aiplatform.user)
- Storage Object Creator (roles/storage.objectCreator)

For more information about granting roles, see [Manage access](https://cloud.google.com/iam/docs/granting-changing-revoking-access).

---

You will also need to enable the following APIs (if not enabled already):

- Vertex AI API (aiplatform.googleapis.com)
- Cloud Storage API (storage-api.googleapis.com)
- Artifact Registry API (artifactregistry.googleapis.com)

For more information about API enablement, see [Enabling APIs](https://cloud.google.com/apis/docs/getting-started#enabling_apis).

---

You will also need to have a Google Cloud Storage (GCS) bucket where the custom job can read and write artifacts.

---

To access Gemma on Hugging Face, you're required to review and agree to Google's usage license on the Hugging Face Hub for any of the models from the [Gemma release collection](https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b), and the access request will be processed immediately.

## Create a `CustomContainerTrainingJob` on Vertex AI

You need to define a `CustomContainerTrainingJob` that runs on top of the Hugging Face PyTorch DLC for Training.

The Hugging Face PyTorch DCL for Training comes with most of the Hugging Face Python libraries installed, so as to provide a seamless environment to use the Hugging Face stack on Google Cloud. In this example, as already mentioned, [`trl`](https://github.com/huggingface/trl) will be used to run the Supervised Fine-Tuning (SFT) on top of [`transformers`](https://github.com/huggingface/transformers) for the modelling, [`peft`](https://github.com/huggingface/peft) for the LoRA support and [`bitsandbytes`](https://github.com/huggingface/bitsandbytes) for the quantization support.

Since the Hugging Face PyTorch DLC for Training does not have any pre-defined `CMD` or `ENTRYPOINT`, you will need to set it to `trl sft`; which is the TRL command to run the SFT fine-tuning via the recently released [TRL CLI](https://huggingface.co/docs/trl/en/clis).


```
job = aiplatform.CustomContainerTrainingJob(
    display_name="gemma-2b-sft-lora",
    container_uri="us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-pytorch-training-cu121.2-3.transformers.4-42.ubuntu2204.py310",
    command="trl sft",
)
```

## Prepare `CustomContainerTrainingJob` on Vertex AI

Before submitting the `CustomContainerTrainingJob` you need to define the following:

### Model to fine-tune

In this case you will be fine-tuning a base model, [`google/gemma-2b`](https://huggingface.co/google/gemma-2b), meaning that the model by itself will just generate text and has not been fined-tuned in advance i.e. no prompt templates, etc.

### Model requirements

Fine-tuning a model can be expensive, you can estimate that to full fine-tune an LLM in half precision you would need around four times the disk size of the LLM in GPU VRAM.

For a 2B LLM that takes around 5GiB on disk, it could translate into 20GiB for the half precision fine-tuning; but that could be reduced further by using a quantized optimizer, using LoRA or QLoRA, reducing the batch size, etc. In this case, you will be using an L4 NVIDIA GPU that comes with 24GiB of VRAM so both SFT or SFT + LoRA will work out of the box as those fit within the 24GiB VRAM limit.

Read more about it in [Eleuther AI - Transformer Math 101](https://blog.eleuther.ai/transformer-math/).

### Dataset to fine-tune on

SFT expects either a text-only, a conversational, or a prompt-completion dataset; in this case, a text-only dataset [`timdettmers/openassistant-guanaco`](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) will be used.

This dataset is a subset of the [Open Assistant dataset](https://huggingface.co/datasets/OpenAssistant/oasst1); and this subset only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.

## Submit `CustomContainerTrainingJob` on Vertex AI

As the `CustomContainerTrainingJob` defines the command `trl sft` the arguments to be provided are listed either in the Python reference at [trl.SFTConfig](https://huggingface.co/docs/trl/en/sft_trainer#trl.SFTConfig) or via the `trl sft --help` command.

Read more about the [TRL CLI](https://huggingface.co/docs/trl/en/clis).


```
args = [
    # MODEL
    "--model_name_or_path=google/gemma-2b",
    "--torch_dtype=bfloat16",
    "--attn_implementation=flash_attention_2",
    # DATASET
    "--dataset_name=timdettmers/openassistant-guanaco",
    "--dataset_text_field=text",
    # PEFT
    "--use_peft",
    "--lora_r=16",
    "--lora_alpha=32",
    "--lora_dropout=0.1",
    "--lora_target_modules=all-linear",
    # TRAINER
    "--bf16",
    "--max_seq_length=1024",
    "--per_device_train_batch_size=8",
    "--gradient_accumulation_steps=4",
    "--gradient_checkpointing",
    "--learning_rate=0.0002",
    "--lr_scheduler_type=cosine",
    "--optim=adamw_bnb_8bit",
    "--num_train_epochs=1",
    "--logging_steps=10",
    "--do_eval",
    "--eval_steps=100",
    "--report_to=none",
    f"--output_dir={os.getenv('BUCKET_URI').replace('gs://', '/gcs/')}/gemma-2b-sft-lora",
    "--overwrite_output_dir",
    "--seed=42",
    "--log_level=debug",
]
```

> It's important to note that since GCS FUSE is used to mount the bucket as a directory within the running container job, the mounted path follows the formatting `/gcs/<BUCKET_NAME>`; meaning that anything the `SFTTrainer` writes there will be automatically uploaded to the GCS Bucket.
>
> More information in the [Vertex AI Documentation - Prepare training code](https://cloud.google.com/vertex-ai/docs/training/code-requirements).

Then you need to call the `submit` method on the `aiplatform.CustomContainerTrainingJob`, which is a non-blocking method that will schedule the job without blocking the execution.

The arguments provided to the `submit` method are listed below:

* **`args`** defines the list of arguments to be provided to the `trl sft` command, provided as `trl sft --arg_1=value ...`.

* **`replica_count`** defines the number of replicas to run the job in, for training normally this value will be set to one.

* **`machine_type`**, **`accelerator_type`** and **`accelerator_count`** define the machine i.e. Compute Engine instance, the accelerator (if any), and the number of accelerators (ranging from 1 to 8); respectively. The `machine_type` and the `accelerator_type` are tied together, so you will need to select an instance that supports the accelerator that you are using and vice-versa. More information about the different instances at [Compute Engine Documentation - GPU machine types](https://cloud.google.com/compute/docs/gpus), and about the `accelerator_type` naming at [Vertex AI Documentation - MachineSpec](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec).

* **`base_output_dir`** defines the base directory that will be mounted within the running container from the GCS Bucket, conditioned by the `staging_bucket` argument provided to the `aiplatform.init` initially.

* (optional) **`environment_variables`** defines the environment variables to define within the running container. As you are fine-tuning a gated model i.e. [`google/gemma-2b`](https://huggingface.co/google/gemma-2b), you need to set the `HF_TOKEN` environment variable. Additionally, some other environment variables are defined to set the cache path (`HF_HOME`) and to ensure that the logging messages are streamed to Google Cloud Logs Explorer properly (`TRL_USE_RICH`, `ACCELERATE_LOG_LEVEL`, `TRANSFORMERS_LOG_LEVEL`, and `TQDM_POSITION`).

* (optional) **`timeout`** and **`create_request_timeout`** define the timeouts in seconds before interrupting the job execution or the job creation request (time to allocate required resources and start the execution), respectively.


```
from huggingface_hub import get_token

job.submit(
    args=args,
    replica_count=1,
    machine_type="g2-standard-12",
    accelerator_type="NVIDIA_L4",
    accelerator_count=1,
    base_output_dir=f"{os.getenv('BUCKET_URI')}/gemma-2b-sft-lora",
    environment_variables={
        "HF_HOME": "/root/.cache/huggingface",
        "HF_TOKEN": get_token(),
        "TRL_USE_RICH": "0",
        "ACCELERATE_LOG_LEVEL": "INFO",
        "TRANSFORMERS_LOG_LEVEL": "INFO",
        "TQDM_POSITION": "-1",
    },
    timeout=60 * 60 * 3,  # 3 hours (10800s)
    create_request_timeout=60 * 10,  # 10 minutes (600s)
)
```

> The `CustomContainerTrainingJob` will run asynchronously, meaning that the execution won't be blocked and the job will automatically allocate and deallocate the required resources. So once triggered, you can go to Vertex AI in the Google Cloud Console to monitor the job closely.

## What's next?

Once the fine-tuning is done, you can already deploy it!

To deploy it, you can either [merge the LoRA adapters into the fine-tuned Google Gemma model](https://huggingface.co/docs/peft/en/developer_guides/model_merging) in advance, or just [deploy the base model and the adapter separately](https://huggingface.co/blog/multi-lora-serving).

As deployment and serving is out of the scope of this example, you can refer to the following examples in [`GoogleCloudPlatform/generative-ai`](https://github.com/GoogleCloudPlatform/generative-ai):

- [Hugging Face DLCs: Serving Gemma with Text Generation Inference (TGI) on Vertex AI](./vertex_ai_text_generation_inference_gemma.ipynb)

Or to the examples available within the [`huggingface/Google-Cloud-Containers`](https://github.com/huggingface/Google-Cloud-Containers) repository:

- [Deploy Gemma 7B with TGI on Vertex AI](https://github.com/huggingface/Google-Cloud-Containers/blob/main/examples/vertex-ai/notebooks/deploy-gemma-on-vertex-ai/vertex-notebook.ipynb)
- [Deploy Gemma 7B from GCS with TGI on Vertex AI](https://github.com/huggingface/Google-Cloud-Containers/blob/main/examples/vertex-ai/notebooks/deploy-gemma-from-gcs-on-vertex-ai/vertex-notebook.ipynb)

## References

- [GitHub Repository - Hugging Face DLCs for Google Cloud](https://github.com/huggingface/Google-Cloud-Containers): contains all the containers developed by the collaboration of both Hugging Face and Google Cloud teams; as well as a lot of examples on both training and inference, covering both CPU and GPU, as well support for most of the models within the Hugging Face Hub.
- [Google Cloud Documentation - Hugging Face DLCs](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face): contains a table with the latest released Hugging Face DLCs on Google Cloud.
- [Google Artifact Registry - Hugging Face DLCs](https://console.cloud.google.com/artifacts/docker/deeplearning-platform-release/us/gcr.io): contains all the DLCs released by Google Cloud that can be used.
- [Hugging Face Documentation - Google Cloud](https://huggingface.co/docs/google-cloud): contains the official Hugging Face documentation for the Google Cloud DLCs.




################################################## vespa.md ##################################################


# Vespa

>[Vespa](https://vespa.ai/) is a fully featured search engine and vector database. It supports vector search (ANN), lexical search, and search in structured data, all in the same query.

This notebook shows how to use `Vespa.ai` as a LangChain retriever.

In order to create a retriever, we use [pyvespa](https://pyvespa.readthedocs.io/en/latest/index.html) to
create a connection a `Vespa` service.


```python
%pip install --upgrade --quiet  pyvespa
```


```python
from vespa.application import Vespa

vespa_app = Vespa(url="https://doc-search.vespa.oath.cloud")
```

This creates a connection to a `Vespa` service, here the Vespa documentation search service.
Using `pyvespa` package, you can also connect to a
[Vespa Cloud instance](https://pyvespa.readthedocs.io/en/latest/deploy-vespa-cloud.html)
or a local
[Docker instance](https://pyvespa.readthedocs.io/en/latest/deploy-docker.html).


After connecting to the service, you can set up the retriever:


```python
from langchain_community.retrievers import VespaRetriever

vespa_query_body = {
    "yql": "select content from paragraph where userQuery()",
    "hits": 5,
    "ranking": "documentation",
    "locale": "en-us",
}
vespa_content_field = "content"
retriever = VespaRetriever(vespa_app, vespa_query_body, vespa_content_field)
```

This sets up a LangChain retriever that fetches documents from the Vespa application.
Here, up to 5 results are retrieved from the `content` field in the `paragraph` document type,
using `doumentation` as the ranking method. The `userQuery()` is replaced with the actual query
passed from LangChain.

Please refer to the [pyvespa documentation](https://pyvespa.readthedocs.io/en/latest/getting-started-pyvespa.html#Query)
for more information.

Now you can return the results and continue using the results in LangChain.


```python
retriever.invoke("what is vespa?")
```




################################################## Video.md ##################################################


##### Copyright 2024 Google LLC.


```
# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# Gemini API: Prompting with Video

<table align="left">
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Video.ipynb"><img src="../images/colab_logo_32px.png" />Run in Google Colab</a>
  </td>
</table>

This notebook provides a quick example of how to prompt the Gemini API using a video file. In this case, you'll use a short clip of [Big Buck Bunny](https://peach.blender.org/about/).

### Install dependencies


```
!pip install -U "google-generativeai>=0.7.2"
```


```
import google.generativeai as genai
```

### Authentication Overview

**Important:** The File API uses API keys for authentication and access. Uploaded files are associated with the API key's cloud project. Unlike other Gemini APIs that use API keys, your API key also grants access data you've uploaded to the File API, so take extra care in keeping your API key secure. For best practices on securing API keys, refer to Google's [documentation](https://support.google.com/googleapi/answer/6310037).

#### Set up your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../quickstarts/Authentication.ipynb) for an example.


```
from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
genai.configure(api_key=GOOGLE_API_KEY)
```

## Upload a video to the Files API

The Gemini API accepts video file formats directly. The File API accepts files under 2GB in size and can store up to 20GB of files per project. Files last for 2 days and cannot be downloaded from the API. For the example, you will use the short film "Big Buck Bunny".

> "Big Buck Bunny" is (c) copyright 2008, Blender Foundation / www.bigbuckbunny.org and [licensed](https://peach.blender.org/about/) under the [Creative Commons Attribution 3.0](http://creativecommons.org/licenses/by/3.0/) License.

Note: You can also [upload your own files](https://github.com/google-gemini/cookbook/blob/main/examples/Upload_files_to_Colab.ipynb) to use.


```
!wget https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4
```

    --2024-05-31 18:02:17--  https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4
    Resolving download.blender.org (download.blender.org)... 172.67.14.163, 104.22.64.163, 104.22.65.163, ...
    Connecting to download.blender.org (download.blender.org)|172.67.14.163|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 64657027 (62M) [video/mp4]
    Saving to: ‘BigBuckBunny_320x180.mp4’
    
    BigBuckBunny_320x18 100%[===================>]  61.66M  46.9MB/s    in 1.3s    
    
    2024-05-31 18:02:19 (46.9 MB/s) - ‘BigBuckBunny_320x180.mp4’ saved [64657027/64657027]
    
    


```
video_file_name = "BigBuckBunny_320x180.mp4"

print(f"Uploading file...")
video_file = genai.upload_file(path=video_file_name)
print(f"Completed upload: {video_file.uri}")
```

    Uploading file...
    Completed upload: https://generativelanguage.googleapis.com/v1beta/files/u9hwqc7yo4u9
    

## Get File

After uploading the file, you can verify the API has successfully received the files by calling `files.get`.

`files.get` lets you see the file uploaded to the File API that are associated with the Cloud project your API key belongs to. Only the `name` (and by extension, the `uri`) are unique.


```
import time

while video_file.state.name == "PROCESSING":
    print('Waiting for video to be processed.')
    time.sleep(10)
    video_file = genai.get_file(video_file.name)

if video_file.state.name == "FAILED":
  raise ValueError(video_file.state.name)
print(f'Video processing complete: ' + video_file.uri)
```

    Waiting for video to be processed.
    Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/u9hwqc7yo4u9
    

## Generate Content

After the video has been uploaded, you can make `GenerateContent` requests that reference the File API URI.


```
# Create the prompt.
prompt = "Describe this video."

# Set the model to Gemini 1.5 Flash.
model = genai.GenerativeModel(model_name="models/gemini-1.5-flash")

# Make the LLM request.
print("Making LLM inference request...")
response = model.generate_content([prompt, video_file],
                                  request_options={"timeout": 600})
print(response.text)
```

    Making LLM inference request...
    The video is a animated short film called "Big Buck Bunny". It starts with a view of a beautiful, sunny forest, the camera moves down a small stream, past green grass and small flowers, to the edge of the forest, where a fat, purple bird is sitting on a branch. 
    
    The bird looks up at the sky and flaps its wings, then the text "The Peach Open Movie Project Presents" appears on the screen. 
    
    The next scene shows a large, gray rabbit coming out of a hole under a tree. He stretches and yawns, then he walks into a clearing and looks around. 
    
    Suddenly, a pink butterfly appears and lands on the rabbit's nose. The rabbit is delighted by the butterfly and smiles, then he walks toward the center of the clearing. 
    
    The video then cuts to three animals hiding behind a tree. The first animal is a small, brown squirrel with big blue eyes. The second is a red fox with a big, toothy smile. The third is a gray chinchilla, holding a brown nut. 
    
    The rabbit notices them and walks towards them, but the squirrel runs up the tree and tries to hide. The fox and the chinchilla stay hidden behind the tree. 
    
    The rabbit is surprised to see them, but then he smiles and starts to walk away. He picks up an apple from the ground and starts to eat it, then the squirrel jumps down from the tree and runs towards the rabbit. 
    
    The rabbit throws the apple at the squirrel, and the squirrel catches it and flies off with it. The fox and the chinchilla come out from behind the tree and laugh. 
    
    The rabbit looks up at the sky and smiles, then he walks back to his burrow. 
    
    The video ends with a black screen, showing the names of the people who made the film. 
    
    

## Delete File

Files are automatically deleted after 2 days or you can manually delete them using `files.delete()`.


```
genai.delete_file(video_file.name)
print(f'Deleted file {video_file.uri}')
```

    Deleted file https://generativelanguage.googleapis.com/v1beta/files/u9hwqc7yo4u9
    

## Next Steps
### Useful API references:

The File API lets you upload a variety of multimodal MIME types, including images, audio, and video formats. The File API handles inputs that can be used to generate content with [`model.generateContent`](https://ai.google.dev/api/rest/v1/models/generateContent) or [`model.streamGenerateContent`](https://ai.google.dev/api/rest/v1/models/streamGenerateContent).

* Learn more about the [File API](../quickstarts/File_API.ipynb) with the quickstart.

* Learn more about prompting with [media files](https://ai.google.dev/tutorials/prompting_with_media) in the docs, including the supported formats and maximum length.

### Related examples

Check those examples videos with the Gemini API to give you more ideas what you can do with them:
* Analyze videos to [classify](../examples/Analyze_a_Video_Classification.ipynb) or [summarize](../examples/Analyze_a_Video_Summarization.ipynb) them
* Have the Gemini API recognize an [historical moment](../examples/Analyze_a_Video_Historic_Event_Recognition.ipynb) and tell you more about it

### Continue your discovery of the Gemini API

If you're not already familiar with it, learn how [tokens are counted](../quickstarts/Counting_Tokens.ipynb). Then check how to use the File API to use [Audio files](../quickstarts/Audio.ipynb) with the Gemini API.





################################################## video_analysis.md ##################################################


```
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# Video Analysis with Gemini

<table align="left">
  <td style="text-align: center">
    <a href="https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/video-analysis/video_analysis.ipynb">
      <img src="https://cloud.google.com/ml-engine/images/colab-logo-32px.png" alt="Google Colaboratory logo"><br> Run in Colab
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fvideo-analysis%2Fvideo_analysis.ipynb">
      <img width="32px" src="https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN" alt="Google Cloud Colab Enterprise logo"><br> Run in Colab Enterprise
    </a>
  </td>       
  <td style="text-align: center">
    <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/video-analysis/video_analysis.ipynb">
      <img src="https://cloud.google.com/ml-engine/images/github-logo-32px.png" alt="GitHub logo"><br> View on GitHub
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/video-analysis/video_analysis.ipynb">
      <img src="https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32" alt="Vertex AI logo"><br> Open in Vertex AI Workbench
    </a>
  </td>
</table>


| | |
|-|-|
|Author(s) | [Holt Skinner](https://github.com/holtskinner) |

## Getting Started


### Install Vertex AI SDK for Python


```
%pip install --upgrade --user -q google-cloud-aiplatform
```

### Restart current runtime

To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel.


```
# Restart kernel after installs so that your environment can access the new packages
import IPython

app = IPython.Application.instance()
app.kernel.do_shutdown(True)
```

<div class="alert alert-block alert-warning">
<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>
</div>


### Authenticate your notebook environment (Colab only)

If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).



```
import sys

# Additional authentication is required for Google Colab
if "google.colab" in sys.modules:
    # Authenticate user to Google Cloud
    from google.colab import auth

    auth.authenticate_user()
```

### Set Google Cloud project information and initialize Vertex AI SDK

To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).

Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).


```
# Define project information
PROJECT_ID = "YOUR_PROJECT_ID"  # @param {type:"string"}
LOCATION = "us-central1"  # @param {type:"string"}

# Initialize Vertex AI
import vertexai

vertexai.init(project=PROJECT_ID, location=LOCATION)
```

### Import libraries



```
from IPython.display import Markdown, Video, display
from vertexai.preview.generative_models import (
    GenerationConfig,
    GenerativeModel,
    HarmBlockThreshold,
    HarmCategory,
    Part,
)
```

### Load the Gemini 1.5 Pro model

Gemini 1.5 Pro (`gemini-1.5-pro`) is a multimodal model that supports multimodal prompts. You can include text, image(s), PDFs, audio, and video in your prompt requests and get text or code responses.


```
model = GenerativeModel("gemini-1.5-pro")

generation_config = GenerationConfig(temperature=1, top_p=0.95, max_output_tokens=8192)

safety_settings = {
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
}
```

### Helper functions


```
def get_url_from_gcs(gcs_uri: str) -> str:
    # converts GCS uri to url for display.
    return gcs_uri.replace("gs://", "https://storage.googleapis.com/").replace(
        " ", "%20"
    )
```

## Sentiment Analysis from Video and Audio

For this example, we will be analyzing a video of the first televised US presidential debate between John F. Kennedy and Richard Nixon. This debate is largely believed to have been critical in securing JFK's victory due to his calm and personable demeanor for the broadcast.

The video is an hour long, which is roughly equivalent to 1 Million Tokens for Gemini 1.5 Pro.


```
video_analysis_prompt = """You are an expert in politics and history. Provide a detailed analysis of the video including each speakers facial expressions and mood at key points."""
```


```
# Load file directly from Google Cloud Storage
video_uri = "gs://github-repo/video/KennedyNixon1960PresidentialDebate.mp4"

# Load contents
contents = [
    Part.from_uri(
        uri=video_uri,
        mime_type="video/mp4",
    ),
    video_analysis_prompt,
]

# Display the Video
display(Video(get_url_from_gcs(video_uri)))
```

Note: due to the length of the video, this will take a few minutes to complete.


```
# Send to Gemini
response = model.generate_content(contents, generation_config=generation_config)

# Display results
display(Markdown(response.text))
```




################################################## video_captioning.md ##################################################


# Video Captioning
This notebook shows how to use VideoCaptioningChain, which is implemented using Langchain's ImageCaptionLoader and AssemblyAI to produce .srt files.

This system autogenerates both subtitles and closed captions from a video URL.

## Installing Dependencies


```python
# !pip install ffmpeg-python
# !pip install assemblyai
# !pip install opencv-python
# !pip install torch
# !pip install pillow
# !pip install transformers
# !pip install langchain
```

## Imports


```python
import getpass

from langchain.chains.video_captioning import VideoCaptioningChain
from langchain.chat_models.openai import ChatOpenAI
```

## Setting up API Keys


```python
OPENAI_API_KEY = getpass.getpass("OpenAI API Key:")

ASSEMBLYAI_API_KEY = getpass.getpass("AssemblyAI API Key:")
```

**Required parameters:**

* llm: The language model this chain will use to get suggestions on how to refine the closed-captions
* assemblyai_key: The API key for AssemblyAI, used to generate the subtitles

**Optional Parameters:**

* verbose (Default: True): Sets verbose mode for downstream chain calls
* use_logging (Default: True): Log the chain's processes in run manager
* frame_skip (Default: None): Choose how many video frames to skip during processing. Increasing it results in faster execution, but less accurate results. If None, frame skip is calculated manually based on the framerate Set this to 0 to sample all frames
* image_delta_threshold (Default: 3000000): Set the sensitivity for what the image processor considers a change in scenery in the video, used to delimit closed captions. Higher = less sensitive
* closed_caption_char_limit (Default: 20): Sets the character limit on closed captions
* closed_caption_similarity_threshold (Default: 80): Sets the percentage value to how similar two closed caption models should be in order to be clustered into one longer closed caption
* use_unclustered_video_models (Default: False): If true, closed captions that could not be clustered will be included. May result in spontaneous behaviour from closed captions such as very short lasting captions or fast-changing captions. Enabling this is experimental and not recommended

## Example run


```python
# https://ia804703.us.archive.org/27/items/uh-oh-here-we-go-again/Uh-Oh%2C%20Here%20we%20go%20again.mp4
# https://ia601200.us.archive.org/9/items/f58703d4-61e6-4f8f-8c08-b42c7e16f7cb/f58703d4-61e6-4f8f-8c08-b42c7e16f7cb.mp4

chain = VideoCaptioningChain(
    llm=ChatOpenAI(model="gpt-4", max_tokens=4000, openai_api_key=OPENAI_API_KEY),
    assemblyai_key=ASSEMBLYAI_API_KEY,
)

srt_content = chain.run(
    video_file_path="https://ia601200.us.archive.org/9/items/f58703d4-61e6-4f8f-8c08-b42c7e16f7cb/f58703d4-61e6-4f8f-8c08-b42c7e16f7cb.mp4"
)

print(srt_content)
```

## Writing output to .srt file


```python
with open("output.srt", "w") as file:
    file.write(srt_content)
```




################################################## Video_REST.md ##################################################


##### Copyright 2024 Google LLC.


```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# Gemini API: Video prompting with REST

<table align="left">
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Video_REST.ipynb"><img src="../../images/colab_logo_32px.png" />Run in Google Colab</a>
  </td>
</table>


This notebook provides quick code examples that show you how  to prompt the Gemini API using a video file with `curl`. In this case, you'll use a short clip of [Big Buck Bunny](https://peach.blender.org/about/).

You can run this in Google Colab, or you can copy/paste the `curl` commands into your terminal.

## Set up the environment

To run this notebook, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you are running in a different environment, you can store your key in an environment variable. See [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) to learn more.

### Authentication Overview

**Important:** The File API uses API keys for authentication and access. Uploaded files are associated with the API key's cloud project. Unlike other Gemini APIs that use API keys, your API key also grants access data you've uploaded to the File API, so take extra care in keeping your API key secure. For best practices on securing API keys, refer to the [API console support center](https://support.google.com/googleapi/answer/6310037).


```
import os
from google.colab import userdata

os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')
```

Install `jq` to help with processing of JSON API responses.


```
!apt install -q jq
```

## Use a video file with the Gemini API

The Gemini API accepts video file formats through the File API. The File API accepts files under 2GB in size and can store up to 20GB of files per project. Files last for 2 days and cannot be downloaded from the API. For this example, you will use the short film "Big Buck Bunny".

> "Big Buck Bunny" is (C) Copyright 2008, Blender Foundation / www.bigbuckbunny.org and [licensed](https://peach.blender.org/about/) under the [Creative Commons Attribution 3.0](http://creativecommons.org/licenses/by/3.0/) License.

Note: In Colab, you can also [upload your own files](https://github.com/google-gemini/cookbook/blob/main/examples/Upload_files_to_Colab.ipynb) to use.


```
!wget https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4
```

With the video file now available locally, generate some metadata that you will use in subsequent steps.


```bash
%%bash

VIDEO_PATH="./BigBuckBunny_320x180.mp4"
DISPLAY_NAME="Big Buck Bunny"

# Auto-detect the metadata needed when you upload the video.
MIME_TYPE=$(file -b --mime-type "${VIDEO_PATH}")
NUM_BYTES=$(wc -c < "${VIDEO_PATH}")

echo $VIDEO_PATH $MIME_TYPE $NUM_BYTES

# Colab doesn't allow sharing shell variables between cells, so save them.
cat >./vars.sh <<-EOF
  export BASE_URL="https://generativelanguage.googleapis.com"
  export DISPLAY_NAME="${DISPLAY_NAME}"
  export VIDEO_PATH=${VIDEO_PATH}
  export MIME_TYPE=${MIME_TYPE}
  export NUM_BYTES=${NUM_BYTES}
EOF
```

    ./BigBuckBunny_320x180.mp4 video/mp4 64657027
    

### Start the upload task

Media uploads in the File API are resumable, so the first step is to define an upload task. This initial request gives you a reference you can use for subsequent upload operations, and allows you to query the status of the upload before sending data, in case of network issues during the data transfer.

The API returns the upload URL in the `x-goog-upload-url` header, so take note of that in the response headers - you will send the payload data to this URL.

No payload data (video bytes) are sent in this initial request.


```bash
%%bash
. vars.sh

# Create the "new upload" request by providing the relevant metadata.
curl "${BASE_URL}/upload/v1beta/files?key=${GOOGLE_API_KEY}" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2>/dev/null

# Print the status.
head -1 upload-header.tmp
```

    HTTP/2 200 



### Upload video data

Now that you have created the upload task, you can upload the file data by sending bytes to the returned upload URL.


```bash
%%bash
. vars.sh

# Extract the upload URL to use from the response headers.
upload_url=$(grep -i "x-goog-upload-url: " upload-header.tmp | cut -d" " -f2 | tr -d "\r")
# The header contains our API key, so don't leave it lying around.
rm upload-header.tmp

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${VIDEO_PATH}" >file_info.json 2>/dev/null

# Show the output. You will use it in a later step.
cat file_info.json
```

    {
      "file": {
        "name": "files/4if4o2bqvugf",
        "displayName": "Big Buck Bunny",
        "mimeType": "video/mp4",
        "sizeBytes": "64657027",
        "createTime": "2024-08-26T08:24:56.068012Z",
        "updateTime": "2024-08-26T08:24:56.068012Z",
        "expirationTime": "2024-08-28T08:24:56.049455995Z",
        "sha256Hash": "Zjc4ZjM5NjAzZTY3NzQ5MDdmMmZhYWZhYmYyNmE2NjdmNGE2ZmMzMTc2OWVjMzA0YThhOGY3YzYyZDI4MDUwOA==",
        "uri": "https://generativelanguage.googleapis.com/v1beta/files/4if4o2bqvugf",
        "state": "PROCESSING"
      }
    }
    

### Get file info

After uploading the file, you can verify the API has successfully received the files by querying the [`files.get` endpoint](https://ai.google.dev/api/files#method:-files.get).

`files.get` lets you see the file uploaded to the File API that are associated with the Cloud project your API key belongs to. Only the `name` (and by extension, the `uri`) are unique.


```bash
%%bash
. vars.sh

file_uri=$(jq -r ".file.uri" file_info.json)

curl "${file_uri}?key=${GOOGLE_API_KEY}" 2>/dev/null
```

    {
      "name": "files/4if4o2bqvugf",
      "displayName": "Big Buck Bunny",
      "mimeType": "video/mp4",
      "sizeBytes": "64657027",
      "createTime": "2024-08-26T08:24:56.068012Z",
      "updateTime": "2024-08-26T08:25:03.977029Z",
      "expirationTime": "2024-08-28T08:24:56.049455995Z",
      "sha256Hash": "Zjc4ZjM5NjAzZTY3NzQ5MDdmMmZhYWZhYmYyNmE2NjdmNGE2ZmMzMTc2OWVjMzA0YThhOGY3YzYyZDI4MDUwOA==",
      "uri": "https://generativelanguage.googleapis.com/v1beta/files/4if4o2bqvugf",
      "state": "ACTIVE",
      "videoMetadata": {
        "videoDuration": "596s",
        "videoThumbnailBytes": "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"
      }
    }
    

### Wait for processing

Once the file is uploaded, the file service will perform some pre-processing to prepare the video for use in the LLM. For simple media types this is typically a negligible amount of time, but for video content you may need a short wait.

You can use the `state` field to query if the video is ready for use. If you use it in a prompt prematurely you will see an error like `The File ... is not in an ACTIVE state and usage is not allowed`.


```bash
%%bash

state=$(jq -r ".file.state" file_info.json)
file_uri=$(jq -r ".file.uri" file_info.json)

while [[ "${state}" == "PROCESSING" ]];
do
  echo "Processing video..."
  sleep 5
  # Get the file of interest to check state
  curl "${file_uri}?key=${GOOGLE_API_KEY}" >file_info.json 2>/dev/null
  state=$(jq -r ".state" file_info.json)
done

echo "Video is now ${state}."
```

    Processing video...
    Processing video...
    Processing video...
    Video is now ACTIVE.
    

### Prompt with the video

Now that the video is uploaded and processed, you can reference it in a prompt.

When assembling your [`contents`](https://ai.google.dev/api/generate-content#request-body), the video can be referenced using a `file_data` part, like this:

```json
{
  "file_data": {
    "mime_type": "video/mp4",
    "file_uri": "https://uri/from/previous/steps"
  }
}
```

Try it yourself with this request.


```bash
%%bash
. vars.sh

file_uri=$(jq ".uri" file_info.json)

model="gemini-1.5-flash"

curl "${BASE_URL}/v1beta/models/${model}:generateContent?key=${GOOGLE_API_KEY}" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "Please describe this file."},
          {"file_data": {
            "mime_type": "'${MIME_TYPE}'",
            "file_uri": '${file_uri}'}}]
        }]
       }' 2>/dev/null >response.json

jq -C .candidates[].content response.json
```

    [1;39m{
      [0m[34;1m"parts"[0m[1;39m: [0m[1;39m[
        [1;39m{
          [0m[34;1m"text"[0m[1;39m: [0m[0;32m"This is a 3D animated short film about a big bunny that gets tired of his usual diet of grass. He wants to try something different like fruit and learns the value of what he has. The film's title is Big Buck Bunny. "[0m[1;39m
        [1;39m}[0m[1;39m
      [1;39m][0m[1;39m,
      [0m[34;1m"role"[0m[1;39m: [0m[0;32m"model"[0m[1;39m
    [1;39m}[0m
    

## Further reading

The File API lets you upload a variety of multimodal MIME types, including images, audio, and video formats. The File API handles inputs that can be used to generate content with the [content generation endpoint](https://ai.google.dev/api/generate-content).

* Read the [`File API`](https://ai.google.dev/api/files) reference.

* Learn more about prompting with [media files](https://ai.google.dev/tutorials/prompting_with_media) in the docs, including the supported formats and maximum length.




################################################## video_summary.md ##################################################


## This demo app shows:
* How to use LangChain's YoutubeLoader to retrieve the caption in a YouTube video
* How to ask Llama 3 to summarize the content (per the Llama's input size limit) of the video in a naive way using LangChain's stuff method
* How to bypass the limit of Llama 3's max input token size by using a more sophisticated way using LangChain's map_reduce and refine methods - see [here](https://python.langchain.com/docs/use_cases/summarization) for more info

We start by installing the necessary packages:
- [youtube-transcript-api](https://pypi.org/project/youtube-transcript-api/) API to get transcript/subtitles of a YouTube video
- [langchain](https://python.langchain.com/docs/get_started/introduction) provides necessary RAG tools for this demo
- [tiktoken](https://github.com/openai/tiktoken) BytePair Encoding tokenizer
- [pytube](https://pytube.io/en/latest/) Utility for downloading YouTube videos

**Note** This example uses OctoAI to host the Llama 3 model. If you have not set up/or used OctoAI before, we suggest you take a look at the [HelloLlamaCloud](HelloLlamaCloud.ipynb) example for information on how to set up OctoAI before continuing with this example.
If you do not want to use OctoAI, you will need to make some changes to this notebook as you go along.


```python
!pip install langchain==0.1.19 youtube-transcript-api tiktoken pytube
```

Let's first load a long (2:47:16) YouTube video (Lex Fridman with Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI) transcript using the YoutubeLoader.


```python
from langchain.document_loaders import YoutubeLoader

loader = YoutubeLoader.from_youtube_url(
    "https://www.youtube.com/watch?v=5t1vTLU7s40", add_video_info=True
)
```


```python
# load the youtube video caption into Documents
docs = loader.load()
```


```python
# check the docs length and content
len(docs[0].page_content), docs[0].page_content[:300]
```

You should see 142689 returned for the doc character length, which is about 30k words or 40k tokens, beyond the 8k context length limit of Llama 3. You'll see how to summarize a text longer than the limit.

**Note**: We are using OctoAI in this example to host our Llama 3 model so you will need to get a OctoAI token.

To get the OctoAI token:

- You will need to first sign in with OctoAI with your github account
- Then create a free API token [here](https://octo.ai/docs/getting-started/how-to-create-an-octoai-access-token) that you can use for a while (a month or $10 in OctoAI credits, whichever one runs out first)

After the free trial ends, you will need to enter billing info to continue to use Llama2 hosted on OctoAI.


```python
# enter your OctoAI API token, or you can use local Llama. See README for more info
from getpass import getpass
import os

OCTOAI_API_TOKEN = getpass()
os.environ["OCTOAI_API_TOKEN"] = OCTOAI_API_TOKEN
```

Next we call the Llama 3 model from OctoAI. In this example we will use the Llama 3 8b instruct model. You can find more on Llama models on the [OctoAI text generation solution page](https://octoai.cloud/text).

At the time of writing this notebook the following Llama models are available on OctoAI:
* meta-llama-3-8b-instruct
* meta-llama-3-70b-instruct
* codellama-7b-instruct
* codellama-13b-instruct
* codellama-34b-instruct
* llama-2-13b-chat
* llama-2-70b-chat
* llamaguard-7b


```python
from langchain.llms.octoai_endpoint import OctoAIEndpoint

llama3_8b = "meta-llama-3-8b-instruct"
llm = OctoAIEndpoint(
    model=llama3_8b,
    max_tokens=500,
    temperature=0.01
)
```

Once everything is set up, we prompt Llama 3 to summarize the first 4000 characters of the transcript for us.


```python
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

prompt_template = "Give me a summary of the text below: {text}?"
prompt = PromptTemplate(
    input_variables=["text"], template=prompt_template
)
chain = prompt | llm

# be careful of the input text length sent to LLM
text = docs[0].page_content[:10000]
summary = chain.invoke(text)

# Note: The context length of 8k tokens in Llama 3 is roughly 6000-7000 words or 32k characters
print(summary)
```

If you try the whole content which has over 142k characters, about 40k tokens, which exceeds the 8k limit, you'll get an empty result (OctoAI used to return an error "BadRequestError: The token count (32704) of your prompt (32204) + your setting of `max_tokens` (500) cannot exceed this model's context length (8192).").


```python
# this will generate an empty result because the input exceeds Llama 3's context length limit
text = docs[0].page_content
summary = llm.invoke(f"Give me a summary of the text below: {text}.")
print(summary)
```

To fix this, you can use LangChain's load_summarize_chain method (detail [here](https://python.langchain.com/docs/use_cases/summarization)).

First you'll create splits or sub-documents of the original content, then use the LangChain's `load_summarize_chain` with the `refine` or `map_reduce type`.

Because this may involve many calls to Llama 3, it'd be great to set up a quick free LangChain API key [here](https://smith.langchain.com/settings), run the following cell to set up necessary environment variables, and check the logs on [LangSmith](https://docs.smith.langchain.com/) during and after the run.


```python
import os
os.environ["LANGCHAIN_API_KEY"] = "your_langchain_api_key"
os.environ["LANGCHAIN_API_KEY"] = "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"
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "Video Summary with Llama 3"
```


```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# we need to split the long input text
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=1000, chunk_overlap=0
)
split_docs = text_splitter.split_documents(docs)
```


```python
# check the splitted docs lengths
len(split_docs), len(docs), len(split_docs[0].page_content), len(docs[0].page_content)
```

The `refine` type implements the following steps under the hood:

1. Call Llama 3 on the first sub-document to generate a concise summary;
2. Loop over each subsequent sub-document, pass the previous summary with the current sub-document to generate a refined new summary;
3. Return the final summary generated on the final sub-document as the final answer - the summary of the whole content.

An example prompt template for each call in step 2, which gets used under the hood by LangChain, is:

```
Your job is to produce a final summary.
We have provided an existing summary up to a certain point:
<previous_summary>
Refine the existing summary (only if needed) with some more content below:
<new_content>
```

**Note**: The following call will make 33 calls to Llama 3 and genereate the final summary in about 10 minutes.


```python
from langchain.chains.summarize import load_summarize_chain

chain = load_summarize_chain(llm, chain_type="refine")
print(chain.run(split_docs))
```

You can also set `chain_type` to `map_reduce` to generate the summary of the entire content using the standard map and reduce method, which works behind the scene by first mapping each split document to a sub-summary via a call to LLM, then combines all those sub-summaries into a single final summary by yet another call to LLM.

**Note**: The following call takes about 3 minutes and all the calls to Llama 3.


```python
chain = load_summarize_chain(llm, chain_type="map_reduce")
print(chain.run(split_docs))
```




################################################## video_thumbnail_generation.md ##################################################


```
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# Video Thumbnail Generation using Gemini 1.5 Pro (API & Python SDK)

<table align="left">
  <td style="text-align: center">
    <a href="https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/vision/use-cases/video-thumbnail-generation/video_thumbnail_generation.ipynb">
      <img src="https://cloud.google.com/ml-engine/images/colab-logo-32px.png" alt="Google Colaboratory logo"><br> Run in Colab
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fvision%2Fuse-cases%2Fvideo-thumbnail-generation%2Fvideo_thumbnail_generation.ipynb">
      <img width="32px" src="https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN" alt="Google Cloud Colab Enterprise logo"><br> Run in Colab Enterprise
    </a>
  </td>    
  <td style="text-align: center">
    <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/use-cases/video-thumbnail-generation/video_thumbnail_generation.ipynb">
      <img src="https://cloud.google.com/ml-engine/images/github-logo-32px.png" alt="GitHub logo"><br> View on GitHub
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/vision/use-cases/video-thumbnail-generation/video_thumbnail_generation.ipynb">
      <img src="https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32" alt="Vertex AI logo"><br> Open in Vertex AI Workbench
    </a>
  </td>
</table>

| | |
|-|-|
|Author(s) | [Kartik Chaudhary](https://github.com/kartikgill)|

## Objectives

In this tutorial, you will learn how to extract meaningful thumbnail images from a video using Gemini 1.5 Pro (`gemini-1.5-pro`) model.

You will complete the following tasks:

- Install the Vertex AI SDK for Python
- Use the Gemini API in Vertex AI to interact with the Gemini 1.5 Pro model
    - Extract thumbnails for a Video along with captions using Gemini 1.5 Pro
    - Use **[moviepy](https://zulko.github.io/moviepy/)** python library for frame extraction for a given timestamp
    - Using a better prompt to improve results

### Costs

This tutorial uses billable components of Google Cloud:

- Vertex AI

Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.


## Getting Started


### Install libraries for Python

- **[Vertex AI SDK](https://cloud.google.com/vertex-ai/docs/python-sdk/use-vertex-ai-python-sdk)**: to call the Gemini API in Vertex AI.
- **[moviepy](https://zulko.github.io/moviepy/)**: A module for video editing.


```
%pip install --upgrade --user google-cloud-aiplatform
%pip install --upgrade --user moviepy
```

### Restart current runtime

To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel.


```
# Restart kernel after installs so that your environment can access the new packages
import IPython

app = IPython.Application.instance()
app.kernel.do_shutdown(True)
```

<div class="alert alert-block alert-warning">
<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>
</div>


### Authenticate your notebook environment (Colab only)

If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).



```
import sys

# Additional authentication is required for Google Colab
if "google.colab" in sys.modules:
    # Authenticate user to Google Cloud
    from google.colab import auth

    auth.authenticate_user()
```

### Set Google Cloud project information and initialize Vertex AI SDK

To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).

Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).


```
PROJECT_ID = "your-project-id"  # @param {type:"string"}
LOCATION = "us-central1"  # @param {type:"string"}

# Initialize Vertex AI
import vertexai

vertexai.init(project=PROJECT_ID, location=LOCATION)
```

### Import libraries



```
import json

import matplotlib.pyplot as plt
import moviepy
from moviepy.editor import VideoFileClip
from vertexai.generative_models import GenerationConfig, GenerativeModel, Part
```

## Using the Gemini 1.5 Pro model

The Gemini 1.5 Pro (`gemini-1.5-pro`) model is a foundation model that performs well at a variety of multimodal tasks such as visual understanding, classification, summarization, and creating content from image, audio and video. It's adept at processing visual and text inputs such as photographs, documents, infographics, and screenshots.


### Load the Gemini 1.5 Pro model



```
model = GenerativeModel("gemini-1.5-pro")
```

### Sample Video path from Google Cloud Storage

#### [Click here to watch/download this video](https://cloud.google.com/vertex-ai/generative-ai/docs/prompt-gallery/samples/video_video_q_and_a_89?hl=en)


```
video_uri = "gs://sample-videofile/sample_video_google_trips.webm"
```

### Creating a local copy of the video for easy frame extraction


```
!gsutil cp {video_uri} sample_video.webm
```

### Creating a MoviePy Clip Object (Helps in extracting frame at a given timestamp)


```
clip = VideoFileClip("sample_video.webm")
```

### Define a function to Call Gemini API


```
def call_gemini(
    prompt: str,
    gcs_video_path: str,
    model: vertexai.generative_models.GenerativeModel,
) -> str:
    """Call Gemini 1.5 Pro API with video and prompt."""
    # define fixed schema for Gemini outputs
    response_schema = {
        "type": "array",
        "items": {
            "type": "object",
            "properties": {
                "timestamp": {
                    "type": "string",
                },
                "caption": {
                    "type": "string",
                },
            },
            "required": ["timestamp", "caption"],
        },
    }
    # model configurations
    generation_config = GenerationConfig(
        temperature=1,
        top_p=0.8,
        max_output_tokens=8192,
        response_schema=response_schema,
        response_mime_type="application/json",
    )
    # creating video input for API call
    video_input = Part.from_uri(
        mime_type="video/webm",
        uri=gcs_video_path,
    )
    # calling Gemini API
    responses = model.generate_content(
        [video_input, prompt],
        generation_config=generation_config,
        stream=False,
    )
    return responses.text
```

### Defining a function to parse output and display results


```
def display_results(
    response_text: str,
    clip: moviepy.video.io.VideoFileClip.VideoFileClip,
) -> None:
    """Parse json output, extract thumbnail frames and display."""
    # loading json output object
    json_response = json.loads(response_text)

    # Image plotting settings
    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 9))

    # extract frame for each timestamp and plot the images
    counter = 0
    for item in json_response:
        timestamp = item["timestamp"]
        caption = item["caption"]
        frame = clip.get_frame(timestamp)
        row, col = counter // 2, counter % 2
        ax[row, col].imshow(frame)
        ax[row, col].set_title(caption, fontdict={"fontsize": 9})
        counter += 1

    fig.show()
```

# Case 1: Using a Simple Prompt

### Writing a basic prompt for thumbnail generation


```
basic_prompt = (
    """Generate 4 thumbnail images from the given video file with short captions."""
)
```

### calling Gemini API with our prompt and video


```
response_text = call_gemini(
    prompt=basic_prompt,
    gcs_video_path=video_uri,
    model=model,
)
```

### showing JSON output from Gemini


```
print(json.loads(response_text))
```

### displaying thumbnail results with captions


```
display_results(response_text, clip)
```

# Case 2: Using an Advanced Prompt

### Writing an advanced prompt for better thumbnail generation


```
advanced_prompt = """You are an expert in video content creation and content marketing.
You have the ability to find best thumbnails from a video and provide meaningful and short and catchy captions for them.
Your task is to find the best 4 thumbnails from a given video along with short, and meaningful captions that is good for marketing.
Consider the following rules while generating thubmnails:

- Thumbnail should have clear focus on the key objects and people, less focus on background
- Thumbnail image should be high quality and bright, avoid blurry images
- Thumbnail image and caption together tell a story
- Thumbnail caption is good for marketing
"""
```

### calling Gemini API with advanced prompt


```
response_text_advanced = call_gemini(
    prompt=advanced_prompt,
    gcs_video_path=video_uri,
    model=model,
)
```

### showing JSON output string


```
print(json.loads(response_text_advanced))
```

### displaying final thumbnails with captions


```
display_results(response_text_advanced, clip)
```

### Observations

#### Better prompting shows the following effects on results
- Results have improved in quality
- Captions are more meaningful
- Thumbnail images and captions tell a better story

## Conclusion

- We just saw that Gemini 1.5 Pro has multimodal capabilities, and can be used for video understanding.
- Results can be improved by better prompting with proper guidelines and expectations.




################################################## vikingdb.md ##################################################


# viking DB

>[viking DB](https://www.volcengine.com/docs/6459/1163946) is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.

This notebook shows how to use functionality related to the VikingDB vector database.

You'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration

To run, you should have a [viking DB instance up and running](https://www.volcengine.com/docs/6459/1165058).





```python
!pip install --upgrade volcengine
```

We want to use VikingDBEmbeddings so we have to get the VikingDB API Key.


```python
import getpass
import os

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")
```


```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores.vikingdb import VikingDB, VikingDBConfig
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
```


```python
loader = TextLoader("./test.txt")
documents = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=10, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

embeddings = OpenAIEmbeddings()
```


```python
db = VikingDB.from_documents(
    docs,
    embeddings,
    connection_args=VikingDBConfig(
        host="host", region="region", ak="ak", sk="sk", scheme="http"
    ),
    drop_old=True,
)
```


```python
query = "What did the president say about Ketanji Brown Jackson"
docs = db.similarity_search(query)
```


```python
docs[0].page_content
```

### Compartmentalize the data with viking DB Collections

You can store different unrelated documents in different collections within same viking DB instance to maintain the context

Here's how you can create a new collection


```python
db = VikingDB.from_documents(
    docs,
    embeddings,
    connection_args=VikingDBConfig(
        host="host", region="region", ak="ak", sk="sk", scheme="http"
    ),
    collection_name="collection_1",
    drop_old=True,
)
```

And here is how you retrieve that stored collection


```python
db = VikingDB.from_documents(
    embeddings,
    connection_args=VikingDBConfig(
        host="host", region="region", ak="ak", sk="sk", scheme="http"
    ),
    collection_name="collection_1",
)
```

After retrieval you can go on querying it as usual.




################################################## Vision_Fine_tuning_on_GPT4o_for_Visual_Question_Answering.md ##################################################


# Vision Fine-tuning on GPT-4o for Visual Question Answering

We're excited to announce the launch of [Vision Fine-Tuning on GPT-4o](https://openai.com/index/introducing-vision-to-the-fine-tuning-api/), a cutting-edge multimodal fine-tuning capability that empowers developers to fine-tune GPT-4o using both **images** and **text**. With this new feature, you can customize models to have stronger image understanding capabilities, unlocking possibilities across various industries and applications.

From **advanced visual search** to **improved object detection** for autonomous vehicles or smart cities, vision fine-tuning enables you to craft solutions tailored to your specific needs. By combining text and image inputs, this product is uniquely positioned for tasks like **visual question answering**, where detailed, context-aware answers are derived from analyzing images. In general, this seems to be most effective when the model is presented with questions and images that resemble the training data as we are able to teach the model how to search and identify relevant parts of the image to answer the question correctly. Similarly to fine-tuning on text inputs, vision fine-tuning is not as useful for teaching the model new information.

In this guide, we’ll walk you through the steps to fine-tune GPT-4o with multimodal inputs. Specifically, we’ll demonstrate how to train a model for answering questions related to **images of books**, but the potential applications span countless domains—from **web design** and **education** to **healthcare** and **research**.

Whether you're looking to build smarter defect detection models for manufacturing, enhance complex document processing and diagram understanding, or develop applications with better visual comprehension for a variety of other use cases, this guide will show you just how fast and easy it is to get started.

For more information, check out the full [Documentation](https://platform.openai.com/docs/guides/fine-tuning/vision).


```python
from openai import OpenAI, ChatCompletion
import json
import os

client = OpenAI()
```

### Load Dataset

We will work with a dataset of question-answer pairs on images of books from the [OCR-VQA dataset](https://ocr-vqa.github.io/), accessible through HuggingFace. This dataset contains 207,572 images of books with associated question-answer pairs inquiring about title, author, edition, year and genre of the book. In total, the dataset contains ~1M QA pairs. For the purposes of this guide, we will only use a small subset of the dataset to train, validate and test our model.

We believe that this dataset will be well suited for fine-tuning on multimodal inputs as it requires the model to not only accurately identify relevant bounding boxes to extract key information, but also reason about the content of the image to answer the question correctly.


```python
from datasets import load_dataset

# load dataset
ds = load_dataset("howard-hou/OCR-VQA")
```

We'll begin by sampling 150 training examples, 50 validation examples and 100 test examples. We will also explode the `questions` and `answers` columns to create a single QA pair for each row. Additionally, since our images are stored as byte strings, we'll convert them to images for processing.


```python
import pandas as pd
from io import BytesIO
from PIL import Image

# sample 150 training examples, 50 validation examples and 100 test examples
ds_train = ds['train'].shuffle(seed=42).select(range(150))
ds_val = ds['validation'].shuffle(seed=42).select(range(50))
ds_test = ds['test'].shuffle(seed=42).select(range(100))

# convert to pandas dataframe
ds_train = ds_train.to_pandas()
ds_val = ds_val.to_pandas()
ds_test = ds_test.to_pandas()

# convert byte strings to images
ds_train['image'] = ds_train['image'].apply(lambda x: Image.open(BytesIO(x['bytes'])))
ds_val['image'] = ds_val['image'].apply(lambda x: Image.open(BytesIO(x['bytes'])))
ds_test['image'] = ds_test['image'].apply(lambda x: Image.open(BytesIO(x['bytes'])))

# explode the 'questions' and 'answers' columns
ds_train = ds_train.explode(['questions', 'answers'])
ds_val = ds_val.explode(['questions', 'answers'])
ds_test = ds_test.explode(['questions', 'answers'])

# rename columns
ds_train = ds_train.rename(columns={'questions': 'question', 'answers': 'answer'})
ds_val = ds_val.rename(columns={'questions': 'question', 'answers': 'answer'})
ds_test = ds_test.rename(columns={'questions': 'question', 'answers': 'answer'})

# create unique ids for each example
ds_train = ds_train.reset_index(drop=True)
ds_val = ds_val.reset_index(drop=True)
ds_test = ds_test.reset_index(drop=True)

# select columns
ds_train = ds_train[['question', 'answer', 'image']]
ds_val = ds_val[['question', 'answer', 'image']]
ds_test = ds_test[['question', 'answer', 'image']]

```

Let's inspect a random sample from the training set.

In this example, the question prompts the model to determine the title of the book. In this case, the answer is quite ambiguous as there is the main title "Patty's Patterns - Advanced Series Vol. 1 & 2" as well as the subtitle "100 Full-Page Patterns Value Bundle" which are found in different parts of the image. Also, the name of the author here is not an individual, but a group called "Penny Farthing Graphics" which could be mistaken as part of the title.

This type of task is typical in visual question answering, where the model must interpret complex images and provide accurate, context-specific responses. By training on these kinds of questions, we can enhance the model's ability to perform detailed image analysis across a variety of domains.


```python
from IPython.display import display

# display a random training example
print('QUESTION:', ds_train.iloc[198]['question'])
display(ds_train.iloc[198]['image'])
print('ANSWER:', ds_train.iloc[198]['answer'])

```

    QUESTION: What is the title of this book?
    


    
![png](output_9_1.png)
    


    ANSWER: Patty's Patterns - Advanced Series Vol. 1 & 2: 100 Full-Page Patterns Value Bundle
    

### Data Preparation

To ensure successful fine-tuning of our model, it’s crucial to properly structure the training data. Correctly formatting the data helps avoid validation errors during training and ensures the model can effectively learn from both text and image inputs. The good news is, this process is quite straightforward.

Each example in the training dataset should be a conversation in the same format as the **Chat Completions API**. Specifically, this means structuring the data as a series of **messages**, where each message includes a `role` (such as "user" or "assistant") and the `content` of the message.

Since we are working with both text and images for vision fine-tuning, we’ll construct these messages to include both content types. For each training sample, the question about the image is presented as a user message, and the corresponding answer is provided as an assistant message.

Images can be included in one of two ways:
* As **HTTP URLs**, referencing the location of the image.
* As **data URLs** containing the image encoded in **base64**.

Here’s an example of how the message format should look:


```python
{
    "messages": 
    [
        {
            "role": "system",
            "content": "Use the image to answer the question."
        },
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "What is the title of this book?"},
                {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,<encoded_image>"}}
            ]
        }
    ]
}
```

Let's start by defining the **system instructions** for our model. These instructions provide the model with important context, guiding how it should behave when processing the training data. Clear and concise system instructions are particularly useful to make sure the model reasons well on both text and images.


```python
SYSTEM_PROMPT = """
Generate an answer to the question based on the image of the book provided.
Questions will include both open-ended questions and binary "yes/no" questions.
The questions will inquire about the title, author, edition, year and genre of the book in the image.

You will read the question and examine the corresponding image to provide an accurate answer.

# Steps

1. **Read the Question:** Carefully analyze the question to understand what information is being asked.
2. **Examine the Image:**
   - **Identify Relevant Bounding Boxes (if applicable):** For questions requiring specific details like the title or author, focus on the relevant areas or bounding boxes within the image to extract the necessary text. There may be multiple relevant bounding boxes in the image, so be sure to consider all relevant areas.
   - **Analyze the Whole Image:** For questions that need general reasoning (e.g., "Is this book related to Children's Books?"), consider the entire image, including title, graphics, colors, and overall design elements.
3. **Formulate a Reasoned Answer:**
   - For binary questions (yes/no), use evidence from the image to support your answer.
   - For open-ended questions, provide the exact text from the image or a concise phrase that best describes the requested information.

# Output Format

- Provide your answer in a concise and clear manner. Always return the final conclusion only, no additional text or reasoning.
- If the question is binary, answer with "Yes" or "No."
- For open-ended questions requesting specific details (e.g., title, author), return the exact text from the image.
- For questions about general attributes like "genre," return a single word or phrase that best describes it.

# Notes

- Always prioritize accuracy and clarity in your responses.
- If multiple authors are listed, return the first author listed.
- If the information is not present in the image, try to reason about the question using the information you can gather from the image e.g. if the author is not listed, use the title and genre to find the author.
- Ensure reasoning steps logically lead to the conclusions before stating your final answer.

# Examples
You will be provided with examples of questions and corresponding images of book covers, along with the reasoning and conclusion for each example. Use these examples to guide your reasoning process."""
```

To ensure our images are properly formatted for vision fine-tuning, they must be in **base64 format** and either **RGB or RGBA**. This ensures the model can accurately process the images during training. Below is a function that handles the encoding of images, while also converting them to the correct format if necessary.

This function allows us to control the quality of the image encoding, which can be useful if we want to reduce the size of the file. 100 is the highest quality, and 1 is the lowest. The maximum file size for a fine-tuning job is 1GB, but we are unlikely to see improvements with a very large amount of training data. Nevertheless, we can use the `quality` parameter to reduce the size of the file if needed to accomodate file size limits.


```python
import base64

def encode_image(image, quality=100):
    if image.mode != 'RGB':
        image = image.convert('RGB')  # Convert to RGB
    buffered = BytesIO()
    image.save(buffered, format="JPEG", quality=quality) 
    return base64.b64encode(buffered.getvalue()).decode("utf-8")
```

We will also include **Few-Shot examples** from the training set as user and assistant messages to help guide the model's reasoning process.



```python
FEW_SHOT_EXAMPLES = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "**Example 1:**\n\n**Question:** Who wrote this book?"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encode_image(ds_train.iloc[286]['image'], quality=50)}"}}
        ]
    },
    {
        "role": "assistant",
        "content": [
            {"type": "text", "text": "**Reasoning:** The cover clearly displays two authors' names, 'Evelyn M. Thomson' and 'Orlen N. Johnson,' at the bottom of the cover, with Evelyn M. Thomson listed first. Typically, the first-listed author is considered the primary author or main contributor.\n\n**Conclusion:** Evelyn Thomson"}
        ]
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "**Example 2:**\n\n**Question:** What is the title of this book?"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encode_image(ds_train.iloc[22]['image'], quality=50)}"}}
        ]
    },
    {
        "role": "assistant",
        "content": [
            {"type": "text", "text": "**Answer:**\n\n**Reasoning:** The cover prominently displays the title across the top and center of the image. The full title reads, 'Computer Systems: An Integrated Approach to Architecture and Operating Systems,' with each component of the title clearly separated and formatted to stand out.\n\n**Conclusion:** Computer Systems: An Integrated Approach to Architecture and Operating Systems"}
        ]
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "**Example 3:**\n\n**Question:** Is this book related to Children's Books?"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encode_image(ds_train.iloc[492]['image'], quality=50)}"}}
        ]
    },
    {
        "role": "assistant",
        "content": [
            {"type": "text", "text": "**Answer:**\n\n**Reasoning:** The cover illustration features a whimsical mermaid holding a red shoe, with gentle, child-friendly artwork that suggests it is targeted toward a young audience. Additionally, the style and imagery are typical of children's literature.\n\n**Conclusion:** Yes"}
        ]
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "**Example 4:**\n\n**Question:** Is this book related to History?"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encode_image(ds_train.iloc[68]['image'], quality=50)}"}}
        ]
    },
    {
        "role": "assistant",
        "content": [
            {"type": "text", "text": "**Answer:**\n\n**Reasoning:** The title 'Oliver Wendell Holmes, Jr.: Civil War Soldier, Supreme Court Justice' clearly indicates that this book focuses on the life of Oliver Wendell Holmes, Jr., providing a biographical account rather than a general historical analysis. Although it references historical elements (Civil War, Supreme Court), the primary focus is on the individual rather than historical events as a whole.\n\n**Conclusion:** No"}
        ]
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "**Example 5:**\n\n**Question:** What is the genre of this book?"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encode_image(ds_train.iloc[42]['image'], quality=50)}"}}
        ]
    },
    {
        "role": "assistant",
        "content": [
            {"type": "text", "text": "**Answer:**\n\n**Reasoning:** The cover prominently features an image of a train station and the title 'Railway Depots, Stations & Terminals,' which directly suggests a focus on railway infrastructure. This points to the book being related to topics within Engineering & Transportation.\n\n**Conclusion:** Engineering & Transportation"}
        ]
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "**Example 6:**\n\n**Question:** What type of book is this?"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encode_image(ds_train.iloc[334]['image'], quality=50)}"}}
        ]
    },
    {
        "role": "assistant",
        "content": [
            {"type": "text", "text": "**Answer:**\n\n**Reasoning:** The title 'Principles and Practice of Modern Chromatographic Methods' suggests a focus on chromatography, a scientific technique used in chemistry and biology. This aligns with the academic and technical nature typical of books in the 'Science & Math' category.\n\n**Conclusion:** Science & Math"}
        ]
    }
]
```

Now that we have our system instructions, few-shot examples, and the image encoding function in place, the next step is to iterate through the training set and construct the messages required for fine-tuning. As a reminder, each training example must be formatted as a conversation and must include both the image (in base64 format) and the corresponding question and answer.

To fine-tune GPT-4o, we recommend providing at least **10 examples**, but you’ll typically see noticeable improvements with **50 to 100** training examples. In this case, we'll go all-in and fine-tune the model using our larger training sample of **150 images, and 721 QA pairs**.


```python
from tqdm import tqdm

# constructing the training set
json_data = []

for idx, example in tqdm(ds_train.iterrows()):
    system_message = {
        "role": "system",
        "content": [{"type": "text", "text": SYSTEM_PROMPT}]
    }
    
    user_message = {
        "role": "user",
        "content": [
            {"type": "text", "text": f"Question [{idx}]: {example['question']}"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encode_image(example['image'], quality=50)}"}}
        ]
    }
    
    assistant_message = {
        "role": "assistant",
        "content": [{"type": "text", "text": example["answer"]}]
    }

    all_messages = [system_message] + FEW_SHOT_EXAMPLES + [user_message, assistant_message]
    
    json_data.append({"messages": all_messages})
```

    721it [00:01, 518.61it/s]
    

We save our final training set in a `.jsonl` file where each line in the file represents a single example in the training dataset.


```python
# save the JSON data to a file
with open("ocr-vqa-train.jsonl", "w") as f:
    for message in json_data:
        json.dump(message, f)
        f.write("\n")
```

Just like the training set, we need to structure our validation and test sets in the same message format. However, for the test set, there's a key difference: since the test set is used for evaluation, we do not include the assistant's message (i.e., the answer). This ensures the model generates its own answers, which we can later compare to the ground truth for performance evaluation.


```python
# constructing the validation set
json_data = []

for idx, example in tqdm(ds_val.iterrows()):
    system_message = {
        "role": "system",
        "content": [{"type": "text", "text": SYSTEM_PROMPT}]
    }
    
    user_message = {
        "role": "user",
        "content": [
            {"type": "text", "text": f"Question [{idx}]: {example['question']}"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encode_image(example['image'], quality=50)}"}}
        ]
    }

    assistant_message = {
        "role": "assistant",
        "content": [{"type": "text", "text": example["answer"]}]
    }

    all_messages = [system_message] + FEW_SHOT_EXAMPLES + [user_message, assistant_message]
    
    json_data.append({"messages": all_messages})

# save the JSON data to a file
with open("ocr-vqa-validation.jsonl", "w") as f:
    for message in json_data:
        json.dump(message, f)
        f.write("\n")
```

    239it [00:00, 474.76it/s]
    


```python
# constructing the test set
json_data = []

for idx, example in tqdm(ds_test.iterrows()):
    system_message = {
        "role": "system",
        "content": [{"type": "text", "text": SYSTEM_PROMPT}]
    }
    
    user_message = {
        "role": "user",
        "content": [
            {"type": "text", "text": f"Question [{idx}]: {example['question']}"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encode_image(example['image'], quality=50)}"}}
        ]
    }

    all_messages = [system_message] + FEW_SHOT_EXAMPLES + [user_message]
    
    json_data.append({"messages": all_messages})

# save the JSON data to a file
with open("ocr-vqa-test.jsonl", "w") as f:
    for message in json_data:
        json.dump(message, f)
        f.write("\n")
```

    485it [00:00, 490.79it/s]
    

### Fine-tuning

Now that we have prepared our training and validation datasets in the right format, we can upload them using the [Files API](https://platform.openai.com/docs/api-reference/files/create) for fine-tuning.


```python
# upload training file
train_file = client.files.create(
  file=open("ocr-vqa-train.jsonl", "rb"),
  purpose="fine-tune"
)

# upload validation file
val_file = client.files.create(
  file=open("ocr-vqa-validation.jsonl", "rb"),
  purpose="fine-tune"
)
```

Once the files are uploaded, we're ready to proceed to the next step: starting the fine-tuning job.

To create a fine-tuning job, we use the fine-tuning API. This may take some time to complete, but you can track the progress of the fine-tuning job in the [Platform UI](https://platform.openai.com/finetune/).


```python
# create fine tuning job
file_train = train_file.id
file_val = val_file.id

client.fine_tuning.jobs.create(
  training_file=file_train,
  # note: validation file is optional
  validation_file=file_val,
  model="gpt-4o-2024-08-06"
)
```




    FineTuningJob(id='ftjob-I1GKWTvusx0900L4ggohrGCP', created_at=1730479789, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-l89177bnhkme4a44292n5r3j', result_files=[], seed=662273734, status='validating_files', trained_tokens=None, training_file='file-UzGnMr4kYPgcFeuq121UBifQ', validation_file='file-LoWiW0fCIa3eirRZExRU3pKB', estimated_finish=None, integrations=[], user_provided_suffix=None, method=None)



### Evaluation

Once the fine-tuning job is complete, it’s time to evaluate the performance of our model by running inference on the test set. This step involves using the fine-tuned model to generate responses to the questions in the test set and comparing its predictions to the ground truth answers for evaluation. We will also run inference on the test set using the non-fine-tuned GPT-4o model for comparison.


```python
from concurrent.futures import ThreadPoolExecutor, as_completed
import re

# load the test data from JSONL file
test_data = []
with open("ocr-vqa-test.jsonl", "r") as f:
    for line in f:
        test_data.append(json.loads(line))

def process_example(example, model):
    response = client.chat.completions.create(
        model=model,
        messages=example["messages"],
        store=True,
        metadata={'dataset': 'ocr-vqa-test'}
    )
    predicted_answer = response.choices[0].message.content.strip()
    
    # regex to get the question ID
    match = re.search(r'\[(\d+)\]', example["messages"][-1]["content"][0]["text"])
    if match:
        example_id = int(match.group(1))
    else:
        example_id = -1
    
    actual_answer = ds_test.iloc[example_id]['answer']

    return {
        "example_id": example_id,
        "predicted_answer": predicted_answer,
        "actual_answer": actual_answer
    }

# run the prompts through the finetuned model and store the results
model = "ft:gpt-4o-2024-08-06:openai::AOY1M8VG"
results = []
with ThreadPoolExecutor() as executor:
    futures = {executor.submit(process_example, example, model): example for example in test_data}
    for future in tqdm(as_completed(futures), total=len(futures)):
        results.append(future.result())

# save the results to a file
with open("ocr-vqa-ft-results.jsonl", "w") as f:
    for result in results:
        json.dump(result, f)
        f.write("\n")

# run the prompts through the non-fine-tuned model and store the results
model = "gpt-4o"
results = []
with ThreadPoolExecutor() as executor:
    futures = {executor.submit(process_example, example, model): example for example in test_data}
    for future in tqdm(as_completed(futures), total=len(futures)):
        results.append(future.result())

# save the results to a file
with open("ocr-vqa-4o-results.jsonl", "w") as f:
    for result in results:
        json.dump(result, f)
        f.write("\n")
```

    100%|██████████| 485/485 [02:03<00:00,  3.93it/s]
    100%|██████████| 485/485 [01:35<00:00,  5.09it/s]
    

Now that we’ve run inference using our fine-tuned model, let’s inspect a few specific examples to understand how well the model performed compared to the actual answers.


```python
# Q: What is the title of this book?
{"example_id": 6, "predicted_answer": "A Wrinkle in Time", "actual_answer": "A Wrinkle in Time (Time Quintet)"}
# Q: Who wrote this book?
{"example_id": 10, "predicted_answer": "DK Travel", "actual_answer": "DK Publishing"}
# Q: What is the title of this book?
{"example_id": 11, "predicted_answer": "DK Eyewitness Travel Guide: Peru", "actual_answer": "DK Eyewitness Travel Guide: Peru"}
# Q: What type of book is this?
{"example_id": 12, "predicted_answer": "Travel", "actual_answer": "Travel"}
# Q: Who wrote this book?
{"example_id": 437, "predicted_answer": "Cookshack, Inc.", "actual_answer": "Cookshack"}
# Q: What type of book is this?
{"example_id": 482, "predicted_answer": "Christian Books & Bibles", "actual_answer": "Religion & Spirituality"}
```

As we can see, the fine-tuned model does a great job at answering the questions, with many responses being exactly correct. 

However, there are also cases where the model’s **predicted answers** are close to the **ground truth**, while not matching exactly, particularly in open-ended questions where phrasing or details may differ. To assess the quality of these predictions, we will use GPT-4o to evaluate the similarity between the predicted responses and the ground truth labels from the dataset.

In order to evaluate our model responses, we will use GPT-4o to determine the similarity between the ground truth and our predicted responses. We will rank our predicted answers based on the following criteria:
* **Very Similar**: The predicted answer exactly matches the ground truth and there is no important information omitted, although there may be some minor ommissions or discrepancies in punctuation.

* **Mostly Similar**: The predicted answer closely aligns with the ground truth, perhaps with some missing words or phrases.

* **Somewhat Similar**: Although the predicted answer has noticeable differences to the ground truth, the core content is accurate and semantically similar, perhaps with some missing information.

* **Incorrect**: The predicted answer is completely incorrect, irrelevant, or contains critical errors or omissions from the ground truth.



```python
from pydantic import BaseModel, Field

# define output schema
class Result(BaseModel):
    example_id: int = Field(description="The unique ID of the question")
    rating: str = Field(description="The assigned similarity rating. One of [Very Similar | Mostly Similar | Somewhat Similar | Incorrect]")
    type: str = Field(description="The type of question. Open if the question is binary yes/no, otherwise Closed. One of [Open | Closed]")

EVAL_PROMPT = """
Evaluate the closeness between the predicted answer and the ground truth for each provided result.
Rank the predicted answer based on the following criteria:

1. **Very Similar**: The predicted answer exactly matches the ground truth and there is no important information omitted, although there may be some minor ommissions or discrepancies in punctuation.
2. **Mostly Similar**: The predicted answer closely aligns with the ground truth, perhaps with some missing words or phrases.
3. **Somewhat Similar**: Although the predicted answer has noticeable differences to the ground truth, the core content is accurate and semantically similar, perhaps with some missing information.
4. **Incorrect**: The predicted answer is completely incorrect, irrelevant, or contains critical errors or omissions from the ground truth.

Ensure to consider both open-ended and yes/no questions.

# Steps
1. **Analyze the Answers**: Read the predicted answer, and ground truth carefully.
2. **Evaluate Similarity**:
    - Check if the predicted answer contains the same core information and correctness as the ground truth.
    - Determine if there are any important omissions or errors.
3. **Assign a Rating**: Based on your evaluation, assign the appropriate rating: Very Similar, Mostly Similar, Somewhat Similar, or Incorrect.

# Output Format
```json
[
    {
        "example_id": [example_id],
        "rating": "[Very Similar | Mostly Similar | Somewhat Similar | Incorrect]",
        "type": "[Open | Closed]
    }
]
```

# Examples

**Input:**
```json
{"example_id": 6, "predicted_answer": "A Wrinkle in Time", "actual_answer": "A Wrinkle in Time (Time Quintet)"}
```
**Reasoning:**
The predicted answer "A Wrinkle in Time" is a very close match to the ground truth "A Wrinkle in Time (Time Quintet)" with a missing tagline or subtitle.
**Output:**
```json
{ "example_id": 6, "rating": "Mostly Similar", "type": "Open" }
```

**Input:**
```json
{"example_id": 437, "predicted_answer": "Cookshack, Inc.", "actual_answer": "Cookshack"}
```
**Reasoning:**
The predicted answer "Cookshack, Inc." is exactly the same as the ground truth "Cookshack", with only a difference in punctuation.
**Output:**
```json
{ "example_id": 437, "rating": "Very Similar", "type": "Open" }
```

**Input:**
```json
{"example_id": 482, "predicted_answer": "Christian Books & Bibles", "actual_answer": "Religion & Spirituality"}
```
**Reasoning:**
The predicted answer "Christian Books & Bibles" is semantically similar to the ground truth "Religion & Spirituality", however there is a key difference in the predicted answer.
**Output:**
```json
{ "example_id": 482, "rating": "Somewhat Similar", "type": "Open" }
```

**Input:**
```json
{ "example_id": 417, "predicted_answer": "yes", "actual_answer": "no" }
```
**Reasoning:**
The predicted answer "yes" is completely incorrect compared to the actual answer "no."
**Output:**
```json
{ "example_id": 417, "rating": "Incorrect", "type": "Closed" }
```
"""

def process_result(result):
    messages = [
        {
            "role": "system",
            "content": EVAL_PROMPT
        },
        {
            "role": "user",
            "content": str(result)
        }
    ]

    response = client.beta.chat.completions.parse(
        model='gpt-4o',
        messages=messages,
        temperature=0,
        response_format=Result
    )

    return json.loads(response.choices[0].message.content)

# fine-tuned model results with scores
results = []
with open("ocr-vqa-ft-results.jsonl", "r") as f:
    for line in f:
        results.append(json.loads(line))

results_w_scores = []
with ThreadPoolExecutor() as executor:
    futures = {executor.submit(process_result, result): result for result in results}
    for future in tqdm(as_completed(futures), total=len(futures)):
        results_w_scores.append(future.result())

# Save the results to a file
with open("ocr-vqa-ft-similarity.jsonl", "w") as f:
    for score in results_w_scores:
        json.dump(score, f)
        f.write("\n")

# non-fine-tuned model results with scores
results = []
with open("ocr-vqa-4o-results.jsonl", "r") as f:
    for line in f:
        results.append(json.loads(line))

results_w_scores_4o = []
with ThreadPoolExecutor() as executor:
    futures = {executor.submit(process_result, result): result for result in results}
    for future in tqdm(as_completed(futures), total=len(futures)):
        results_w_scores_4o.append(future.result())

# Save the results to a file
with open("ocr-vqa-4o-similarity.jsonl", "w") as f:
    for score in results_w_scores_4o:
        json.dump(score, f)
        f.write("\n")
```

    100%|██████████| 485/485 [00:18<00:00, 25.58it/s]
    100%|██████████| 485/485 [00:17<00:00, 27.09it/s]
    

To fully understand the impact of fine-tuning, we also evaluated the same set of test questions using the **non-fine-tuned GPT-4o** model.

Let's start by comparing the performance of the fine-tuned model vs the non-fine-tuned model for **Closed** form (Yes/No) questions.

Note that with the fine-tuned model, we can check for exact matches between the predicted and actual answers because the model has learned to produce consistent answers that follow the response format specified in the system prompt. However, for the non-fine-tuned model, we need to account for variations in phrasing and wording in the predicted answers. Below is an example of a non-fine-tuned model output. As we can see, the final answer is correct but the response format is inconsistent and outputs reasoning in the response.


```python
# example of non-fine-tuned model output
{"example_id": 14, "predicted_answer": "**Answer:**\n\nNo. \n\n**Reasoning:** The cover shows \"Eyewitness Travel\" and \"Peru,\" indicating it is a travel guide focused on the country, rather than a pharmaceutical book.", "actual_answer": "No"}
```


```python
# read in results
results_ft = []
with open("ocr-vqa-ft-results.jsonl", "r") as f:
    for line in f:
        results_ft.append(json.loads(line))

results_4o = []
with open("ocr-vqa-4o-results.jsonl", "r") as f:
    for line in f:
        results_4o.append(json.loads(line))

# filter results for yes/no questions
results_ft_closed = [result for result in results_ft if result['actual_answer'] in ['Yes', 'No']]
results_4o_closed = [result for result in results_4o if result['actual_answer'] in ['Yes', 'No']]

# check for correct predictions
correct_ft_closed = [result for result in results_ft_closed if result['predicted_answer'] == result['actual_answer']]
correct_4o_closed = [
    result for result in results_4o_closed 
    if result['predicted_answer'].lower() == result['actual_answer'].lower() 
    or result['actual_answer'].lower() in result['predicted_answer'].lower()
]
print(f"Fine-tuned model accuracy: {round(100*len(correct_ft_closed) / len(results_ft_closed), 2)}%")
print(f"Non-fine-tuned model accuracy: {round(100*len(correct_4o_closed) / len(results_4o_closed), 2)}%")
```

    Fine-tuned model accuracy: 90.53%
    Non-fine-tuned model accuracy: 87.89%
    

With a generous allowance for variations in phrasing and wording for the non-fine-tuned model including ignoring case and allowing for partial matches, the fine-tuned model still outperforms the non-fine-tuned model by a margin of **2.64%** on this set of questions.

Now, let's compare the performance of the fine-tuned model vs the non-fine-tuned model over all the open-ended questions. First, we'll check for exact matches between the predicted and actual answers, again allowing for general variations in phrasing and wording for the non-fine-tuned model, but maintaining a strict standard for the fine-tuned model.



```python
# filter results for open-ended questions
results_ft_open = [result for result in results_ft if result['actual_answer'] not in ['Yes', 'No']]
results_4o_open = [result for result in results_4o if result['actual_answer'] not in ['Yes', 'No']]

# check for correct predictions
correct_ft_open = [result for result in results_ft_open if result['predicted_answer'] == result['actual_answer']]
correct_4o_open = [
    result for result in results_4o_open 
    if result['predicted_answer'].lower() == result['actual_answer'].lower() 
    or result['actual_answer'].lower() in result['predicted_answer'].lower()
]
print(f"Fine-tuned model accuracy: {round(100*len(correct_ft_open) / len(results_ft_open), 2)}%")
print(f"Non-fine-tuned model accuracy: {round(100*len(correct_4o_open) / len(results_4o_open), 2)}%")
```

    Fine-tuned model accuracy: 64.07%
    Non-fine-tuned model accuracy: 46.1%
    

The improvement in accuracy here is much more pronounced, with the fine-tuned model outperforming the non-fine-tuned model by a substantial margin of **17.97%**, even with very generous allowances for variations in phrasing and wording for the non-fine-tuned model!

If we were to afford the same leniency to the fine-tuned model, we would see an additional 4.1% increase in accuracy, bringing the total margin of improvement to **22.07%**.

To dig a little deeper, we can also look at the accuracy by question type.


```python
import matplotlib.pyplot as plt

# seperate by question type
def get_question_type(question):
    if question in ["What is the title of this book?"]:
        return "Title"
    elif question in ["What is the genre of this book?", "What type of book is this?"]:
        return "Genre"
    elif question in ["Who wrote this book?", "Who is the author of this book?"]:
        return "Author"
    else:
        return "Other"

# get index numbers for each question type
question_type_indexes = {
    "Title": [],
    "Genre": [],
    "Author": [],
    "Other": []
}

for idx, row in ds_test.iterrows():
    question_type = get_question_type(row['question'])
    question_type_indexes[question_type].append(idx)

# plot accuracy by question type]
accuracy_by_type_ft = {}
accuracy_by_type_4o = {}

for question_type, indexes in question_type_indexes.items():
    correct_predictions_ft = [
        result for result in results_ft if result['example_id'] in indexes and (
            result['predicted_answer'].lower() == result['actual_answer'].lower() or
            result['actual_answer'].lower() in result['predicted_answer'].lower()
        )
    ]
    correct_predictions_4o = [
        result for result in results_4o if result['example_id'] in indexes and (
            result['predicted_answer'].lower() == result['actual_answer'].lower() or
            result['actual_answer'].lower() in result['predicted_answer'].lower()
        )
    ]
    accuracy_ft = len(correct_predictions_ft) / len(indexes) if indexes else 0
    accuracy_4o = len(correct_predictions_4o) / len(indexes) if indexes else 0
    accuracy_by_type_ft[question_type] = accuracy_ft * 100 
    accuracy_by_type_4o[question_type] = accuracy_4o * 100

# prepare data for plotting
question_types = list(accuracy_by_type_ft.keys())
accuracies_ft = list(accuracy_by_type_ft.values())
accuracies_4o = list(accuracy_by_type_4o.values())

# plot grouped bar chart
bar_width = 0.35
index = range(len(question_types))

plt.figure(figsize=(10, 6))
bar1 = plt.bar(index, accuracies_ft, bar_width, label='Fine-tuned GPT-4o', color='skyblue')
bar2 = plt.bar([i + bar_width for i in index], accuracies_4o, bar_width, label='Non-fine-tuned GPT-4o', color='lightcoral')

plt.xlabel('Question Type')
plt.ylabel('Accuracy (%)')
plt.title('Accuracy by Question Type')
plt.ylim(0, 100)
plt.xticks([i + bar_width / 2 for i in index], question_types, rotation=45)
plt.legend()

plt.show()
```


    
![png](output_44_0.png)
    


It appears that the largest performance gains for the fine-tuned model are for questions in the **Genre** category e.g. "What type of book is this?" or "What is the genre of this book?". This might be indicative of the benefits of fine-tuning in general in that we teach the model to classify genres based on the categories present in the training data. However, it also highlights the model's strong visual undserstanding capabilties, since we are able to identify the genre based on the visual content of the book cover alone.

Additionally, we see significant lift in the **Title** category, which suggests that fine-tuning has boosted the model's OCR capbilities and its ability to understand the layout and structure of the book cover to extract the relevant information.

Finally, let's compare the distribution of similarity ratings between the fine-tuned model and the non-fine-tuned model to allow for variations in phrasing and wording.


```python
from collections import Counter

# extract ratings
ratings_ft = [result['rating'] for result in results_w_scores if result['type'] == 'Open']
ratings_4o = [result['rating'] for result in results_w_scores_4o if result['type'] == 'Open']

# count occurrences of each rating
rating_counts_ft = Counter(ratings_ft)
rating_counts_4o = Counter(ratings_4o)

# define the order of ratings
rating_order = ["Very Similar", "Mostly Similar", "Somewhat Similar", "Incorrect"]

# create bar chart
bar_width = 0.35
index = range(len(rating_order))

fig, ax = plt.subplots()
bar1 = ax.bar(index, [rating_counts_ft.get(rating, 0) for rating in rating_order], bar_width, label='FT GPT-4o')
bar2 = ax.bar([i + bar_width for i in index], [rating_counts_4o.get(rating, 0) for rating in rating_order], bar_width, label='GPT-4o')

ax.set_xlabel('Ratings')
ax.set_ylabel('Count')
ax.set_title('Ratings Distribution')
ax.set_xticks([i + bar_width / 2 for i in index])
ax.set_xticklabels(rating_order)
ax.legend()

plt.show()
```


    
![png](output_46_0.png)
    


The results provide a clear picture of the benefits gained through fine-tuning, without any other modifications.
Comparing the distribution of ratings between the **fine-tuned GPT-4o** model and **GPT-4o without fine-tuning**, we see that the fine-tuned model gets many more responses exactly correct, with a comparable amount of incorrect responses.
### Key Takeaways

* **Improved Precision**: Fine-tuning helped the model produce more precise answers that matched the ground truth, especially in highly domain-specific tasks like OCR on book covers.
* **Better Generalization**: While the non-fine-tuned GPT-4o was able to get at least somewhat to the ground truth for many questions, it was less consistent. The fine-tuned model exhibited better generalization across a variety of test questions, thanks to the exposure to multimodal data during training.

While the results from vision fine-tuning are promising, there are still opportunities for improvement. Much like fine-tuning on text, the effectiveness of vision fine-tuning depends heavily on the **quality, diversity, and representativeness** of the training data. In particular, models benefit from focusing on cases where errors occur most frequently, allowing for targeted improvements.

Upon reviewing the incorrect results, many of the "Incorrect" responses from the fine-tuned model are in fact due to inconsistencies in the labels from the dataset. For example, some ground truth answers provide only the first and last name of the author, whereas the image actually shows the middle initial as well. Similarly, some ground truth labels for the title include subheadings and taglines, whereas others do not. 

Another common theme was miscategorization of genres. Although the model was almost always able to produce a semantically similar genre to the ground truth, the answer sometimes deviated. This is likely due to the lack of presence of these genres in the training data. Providing the model with more diverse training examples to cover these genres, or clearer instructions for dealing with edge cases can help to guide the model’s understanding.

### Next Steps:
* **Expand the Training Dataset**: Adding more varied examples that cover the model’s weaker areas, such as identifying genres, could significantly enhance performance.

* **Expert-Informed Prompts**: Incorporating domain-specific instructions into the training prompts may further refine the model’s ability to accurately interpret and respond in complex cases.

Although there is still some progress to be made on this particular task, the initial results are highly encouraging. With minimal setup and effort, we’ve already observed a substantial uplift in overall accuracy with vision fine-tuning, indicating that this approach holds great potential. Vision fine-tuning opens up possibilities for improvement across a wide range of visual question answering tasks, as well as other tasks that rely on strong visual understanding.




################################################## visualization.md ##################################################


# How to visualize your graph

This guide walks through how to visualize the graphs you create. This works with ANY [Graph](https://langchain-ai.github.io/langgraph/reference/graphs/).

## Setup

First, let's install the required packages


```python
%%capture --no-stderr
%pip install -U langgraph
```

## Set up Graph

You can visualize any arbitrary [Graph](https://langchain-ai.github.io/langgraph/reference/graphs/), including [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph). Let's have some fun by drawing fractals :).


```python
import random
from typing import Annotated, Literal

from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages


class State(TypedDict):
    messages: Annotated[list, add_messages]


class MyNode:
    def __init__(self, name: str):
        self.name = name

    def __call__(self, state: State):
        return {"messages": [("assistant", f"Called node {self.name}")]}


def route(state) -> Literal["entry_node", "__end__"]:
    if len(state["messages"]) > 10:
        return "__end__"
    return "entry_node"


def add_fractal_nodes(builder, current_node, level, max_level):
    if level > max_level:
        return

    # Number of nodes to create at this level
    num_nodes = random.randint(1, 3)  # Adjust randomness as needed
    for i in range(num_nodes):
        nm = ["A", "B", "C"][i]
        node_name = f"node_{current_node}_{nm}"
        builder.add_node(node_name, MyNode(node_name))
        builder.add_edge(current_node, node_name)

        # Recursively add more nodes
        r = random.random()
        if r > 0.2 and level + 1 < max_level:
            add_fractal_nodes(builder, node_name, level + 1, max_level)
        elif r > 0.05:
            builder.add_conditional_edges(node_name, route, node_name)
        else:
            # End
            builder.add_edge(node_name, "__end__")


def build_fractal_graph(max_level: int):
    builder = StateGraph(State)
    entry_point = "entry_node"
    builder.add_node(entry_point, MyNode(entry_point))
    builder.add_edge(START, entry_point)

    add_fractal_nodes(builder, entry_point, 1, max_level)

    # Optional: set a finish point if required
    builder.add_edge(entry_point, END)  # or any specific node

    return builder.compile()


app = build_fractal_graph(3)
```

## Mermaid

We can also convert a graph class into Mermaid syntax.


```python
print(app.get_graph().draw_mermaid())
```

    %%{init: {'flowchart': {'curve': 'linear'}}}%%
    graph TD;
    	__start__([<p>__start__</p>]):::first
    	entry_node(entry_node)
    	node_entry_node_A(node_entry_node_A)
    	node_entry_node_B(node_entry_node_B)
    	node_node_entry_node_B_A(node_node_entry_node_B_A)
    	node_node_entry_node_B_B(node_node_entry_node_B_B)
    	node_node_entry_node_B_C(node_node_entry_node_B_C)
    	__end__([<p>__end__</p>]):::last
    	__start__ --> entry_node;
    	entry_node --> __end__;
    	entry_node --> node_entry_node_A;
    	entry_node --> node_entry_node_B;
    	node_entry_node_B --> node_node_entry_node_B_A;
    	node_entry_node_B --> node_node_entry_node_B_B;
    	node_entry_node_B --> node_node_entry_node_B_C;
    	node_entry_node_A -.-> entry_node;
    	node_entry_node_A -.-> __end__;
    	node_node_entry_node_B_A -.-> entry_node;
    	node_node_entry_node_B_A -.-> __end__;
    	node_node_entry_node_B_B -.-> entry_node;
    	node_node_entry_node_B_B -.-> __end__;
    	node_node_entry_node_B_C -.-> entry_node;
    	node_node_entry_node_B_C -.-> __end__;
    	classDef default fill:#f2f0ff,line-height:1.2
    	classDef first fill-opacity:0
    	classDef last fill:#bfb6fc
    
    

## PNG

If preferred, we could render the Graph into a  `.png`. Here we could use three options:

- Using Mermaid.ink API (does not require additional packages)
- Using Mermaid + Pyppeteer (requires `pip install pyppeteer`)
- Using graphviz (which requires `pip install graphviz`)


### Using Mermaid.Ink

By default, `draw_mermaid_png()` uses Mermaid.Ink's API to generate the diagram.


```python
from IPython.display import Image, display
from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles

display(
    Image(
        app.get_graph().draw_mermaid_png(
            draw_method=MermaidDrawMethod.API,
        )
    )
)
```


    
![jpeg](output_7_0.jpg)
    


### Using Mermaid + Pyppeteer


```python
%%capture --no-stderr
%pip install --quiet pyppeteer
%pip install --quiet nest_asyncio
```


```python
import nest_asyncio

nest_asyncio.apply()  # Required for Jupyter Notebook to run async functions

display(
    Image(
        app.get_graph().draw_mermaid_png(
            curve_style=CurveStyle.LINEAR,
            node_colors=NodeStyles(first="#ffdfba", last="#baffc9", default="#fad7de"),
            wrap_label_n_words=9,
            output_file_path=None,
            draw_method=MermaidDrawMethod.PYPPETEER,
            background_color="white",
            padding=10,
        )
    )
)
```


    
![png](output_10_0.png)
    


### Using Graphviz


```python
%%capture --no-stderr
%pip install pygraphviz
```


```python
try:
    display(Image(app.get_graph().draw_png()))
except ImportError:
    print(
        "You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt"
    )
```


    
![png](output_13_0.png)
    





################################################## Visualizing_embeddings_in_2D.md ##################################################


## Visualizing the embeddings in 2D

We will use t-SNE to reduce the dimensionality of the embeddings from 1536 to 2. Once the embeddings are reduced to two dimensions, we can plot them in a 2D scatter plot. The dataset is created in the [Get_embeddings_from_dataset Notebook](Get_embeddings_from_dataset.ipynb).

### 1. Reduce dimensionality

We reduce the dimensionality to 2 dimensions using t-SNE decomposition.


```python
import pandas as pd
from sklearn.manifold import TSNE
import numpy as np
from ast import literal_eval

# Load the embeddings
datafile_path = "data/fine_food_reviews_with_embeddings_1k.csv"
df = pd.read_csv(datafile_path)

# Convert to a list of lists of floats
matrix = np.array(df.embedding.apply(literal_eval).to_list())

# Create a t-SNE model and transform the data
tsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)
vis_dims = tsne.fit_transform(matrix)
vis_dims.shape
```




    (1000, 2)



### 2. Plotting the embeddings

We colour each review by its star rating, ranging from red to green.

We can observe a decent data separation even in the reduced 2 dimensions.


```python
import matplotlib.pyplot as plt
import matplotlib
import numpy as np

colors = ["red", "darkorange", "gold", "turquoise", "darkgreen"]
x = [x for x,y in vis_dims]
y = [y for x,y in vis_dims]
color_indices = df.Score.values - 1

colormap = matplotlib.colors.ListedColormap(colors)
plt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)
for score in [0,1,2,3,4]:
    avg_x = np.array(x)[df.Score-1==score].mean()
    avg_y = np.array(y)[df.Score-1==score].mean()
    color = colors[score]
    plt.scatter(avg_x, avg_y, marker='x', color=color, s=100)

plt.title("Amazon ratings visualized in language using t-SNE")
```




    Text(0.5, 1.0, 'Amazon ratings visualized in language using t-SNE')




    
![png](output_5_1.png)
    





################################################## Visualizing_embeddings_in_3D.md ##################################################


# Visualizing embeddings in 3D

The example uses [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to reduce the dimensionality fo the embeddings from 1536 to 3. Then we can visualize the data points in a 3D plot. The small dataset `dbpedia_samples.jsonl` is curated by randomly sampling 200 samples from [DBpedia validation dataset](https://www.kaggle.com/danofer/dbpedia-classes?select=DBPEDIA_val.csv).

### 1. Load the dataset and query embeddings


```python
import pandas as pd
samples = pd.read_json("data/dbpedia_samples.jsonl", lines=True)
categories = sorted(samples["category"].unique())
print("Categories of DBpedia samples:", samples["category"].value_counts())
samples.head()

```

    Categories of DBpedia samples: Artist                    21
    Film                      19
    Plant                     19
    OfficeHolder              18
    Company                   17
    NaturalPlace              16
    Athlete                   16
    Village                   12
    WrittenWork               11
    Building                  11
    Album                     11
    Animal                    11
    EducationalInstitution    10
    MeanOfTransportation       8
    Name: category, dtype: int64
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Morada Limited is a textile company based in ...</td>
      <td>Company</td>
    </tr>
    <tr>
      <th>1</th>
      <td>The Armenian Mirror-Spectator is a newspaper ...</td>
      <td>WrittenWork</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Mt. Kinka (金華山 Kinka-zan) also known as Kinka...</td>
      <td>NaturalPlace</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Planning the Play of a Bridge Hand is a book ...</td>
      <td>WrittenWork</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Wang Yuanping (born 8 December 1976) is a ret...</td>
      <td>Athlete</td>
    </tr>
  </tbody>
</table>
</div>




```python
from utils.embeddings_utils import get_embeddings
# NOTE: The following code will send a query of batch size 200 to /embeddings
matrix = get_embeddings(samples["text"].to_list(), model="text-embedding-3-small")

```

### 2. Reduce the embedding dimensionality


```python
from sklearn.decomposition import PCA
pca = PCA(n_components=3)
vis_dims = pca.fit_transform(matrix)
samples["embed_vis"] = vis_dims.tolist()

```

### 3. Plot the embeddings of lower dimensionality


```python
%matplotlib widget
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure(figsize=(10, 5))
ax = fig.add_subplot(projection='3d')
cmap = plt.get_cmap("tab20")

# Plot each sample category individually such that we can set label name.
for i, cat in enumerate(categories):
    sub_matrix = np.array(samples[samples["category"] == cat]["embed_vis"].to_list())
    x=sub_matrix[:, 0]
    y=sub_matrix[:, 1]
    z=sub_matrix[:, 2]
    colors = [cmap(i/len(categories))] * len(sub_matrix)
    ax.scatter(x, y, zs=z, zdir='z', c=colors, label=cat)

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')
ax.legend(bbox_to_anchor=(1.1, 1))

```




    <matplotlib.legend.Legend at 0x1622180a0>





<div style="display: inline-block;">
    <div class="jupyter-widgets widget-label" style="text-align: center;">
        Figure
    </div>
    <img src='data:image/png;base64,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' width=1000.0/>
</div>






################################################## Visualizing_embeddings_in_Kangas.md ##################################################


## Visualizing the embeddings in Kangas

In this Jupyter Notebook, we construct a Kangas DataGrid containing the data and projections of the embeddings into 2 dimensions.

## What is Kangas?

[Kangas](https://github.com/comet-ml/kangas/) as an open source, mixed-media, dataframe-like tool for data scientists. It was developed by [Comet](https://comet.com/), a company designed to help reduce the friction of moving models into production. 

### 1. Setup

To get started, we pip install kangas, and import it.


```python
%pip install kangas --quiet
```


```python
import kangas as kg
```

### 2. Constructing a Kangas DataGrid

We create a Kangas Datagrid with the original data and the embeddings. The data is composed of a rows of reviews, and the embeddings are composed of 1536 floating-point values. In this example, we get the data directly from github, in case you aren't running this notebook inside OpenAI's repo.

We use Kangas to read the CSV file into a DataGrid for further processing.


```python
data = kg.read_csv("https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/data/fine_food_reviews_with_embeddings_1k.csv")
```

    Loading CSV file 'fine_food_reviews_with_embeddings_1k.csv'...
    

    1001it [00:00, 2412.90it/s]
    100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2899.16it/s]
    

We can review the fields of the CSV file:


```python
data.info()
```

    DataGrid (in memory)
        Name   : fine_food_reviews_with_embeddings_1k
        Rows   : 1,000
        Columns: 9
    #   Column                Non-Null Count DataGrid Type       
    --- -------------------- --------------- --------------------
    1   Column 1                       1,000 INTEGER             
    2   ProductId                      1,000 TEXT                
    3   UserId                         1,000 TEXT                
    4   Score                          1,000 INTEGER             
    5   Summary                        1,000 TEXT                
    6   Text                           1,000 TEXT                
    7   combined                       1,000 TEXT                
    8   n_tokens                       1,000 INTEGER             
    9   embedding                      1,000 TEXT                
    

And get a glimpse of the first and last rows:


```python
data
```




<table><th colspan='1' >          row-id </th> <th colspan='1' >        Column 1 </th> <th colspan='1' >       ProductId </th> <th colspan='1' >          UserId </th> <th colspan='1' >           Score </th> <th colspan='1' >         Summary </th> <th colspan='1' >            Text </th> <th colspan='1' >        combined </th> <th colspan='1' >        n_tokens </th> <th colspan='1' >       embedding </th> <tr>
<td colspan='1' >               1 </td> <td colspan='1' >               0 </td> <td colspan='1' >      B003XPF9BO </td> <td colspan='1' >  A3R7JR3FMEBXQB </td> <td colspan='1' >               5 </td> <td colspan='1' > where does one  </td> <td colspan='1' > Wanted to save  </td> <td colspan='1' > Title: where do </td> <td colspan='1' >              52 </td> <td colspan='1' > [0.007018072064 </td> <tr>
<td colspan='1' >               2 </td> <td colspan='1' >             297 </td> <td colspan='1' >      B003VXHGPK </td> <td colspan='1' >  A21VWSCGW7UUAR </td> <td colspan='1' >               4 </td> <td colspan='1' > Good, but not W </td> <td colspan='1' > Honestly, I hav </td> <td colspan='1' > Title: Good, bu </td> <td colspan='1' >             178 </td> <td colspan='1' > [-0.00314055196 </td> <tr>
<td colspan='1' >               3 </td> <td colspan='1' >             296 </td> <td colspan='1' >      B008JKTTUA </td> <td colspan='1' >  A34XBAIFT02B60 </td> <td colspan='1' >               1 </td> <td colspan='1' > Should advertis </td> <td colspan='1' > First, these sh </td> <td colspan='1' > Title: Should a </td> <td colspan='1' >              78 </td> <td colspan='1' > [-0.01757248118 </td> <tr>
<td colspan='1' >               4 </td> <td colspan='1' >             295 </td> <td colspan='1' >      B000LKTTTW </td> <td colspan='1' >  A14MQ40CCU8B13 </td> <td colspan='1' >               5 </td> <td colspan='1' > Best tomato sou </td> <td colspan='1' > I have a hard t </td> <td colspan='1' > Title: Best tom </td> <td colspan='1' >             111 </td> <td colspan='1' > [-0.00139322795 </td> <tr>
<td colspan='1' >               5 </td> <td colspan='1' >             294 </td> <td colspan='1' >      B001D09KAM </td> <td colspan='1' >  A34XBAIFT02B60 </td> <td colspan='1' >               1 </td> <td colspan='1' > Should advertis </td> <td colspan='1' > First, these sh </td> <td colspan='1' > Title: Should a </td> <td colspan='1' >              78 </td> <td colspan='1' > [-0.01757248118 </td> <tr>
<tr><td colspan='10' style='text-align: left;'>...</td></tr><td colspan='1' >             996 </td> <td colspan='1' >             623 </td> <td colspan='1' >      B0000CFXYA </td> <td colspan='1' >  A3GS4GWPIBV0NT </td> <td colspan='1' >               1 </td> <td colspan='1' > Strange inflamm </td> <td colspan='1' > Truthfully wasn </td> <td colspan='1' > Title: Strange  </td> <td colspan='1' >             110 </td> <td colspan='1' > [0.000110913533 </td> <tr>
<td colspan='1' >             997 </td> <td colspan='1' >             624 </td> <td colspan='1' >      B0001BH5YM </td> <td colspan='1' >   A1BZ3HMAKK0NC </td> <td colspan='1' >               5 </td> <td colspan='1' > My favorite and </td> <td colspan='1' > You've just got </td> <td colspan='1' > Title: My favor </td> <td colspan='1' >              80 </td> <td colspan='1' > [-0.02086931467 </td> <tr>
<td colspan='1' >             998 </td> <td colspan='1' >             625 </td> <td colspan='1' >      B0009ET7TC </td> <td colspan='1' >  A2FSDQY5AI6TNX </td> <td colspan='1' >               5 </td> <td colspan='1' > My furbabies LO </td> <td colspan='1' > Shake the conta </td> <td colspan='1' > Title: My furba </td> <td colspan='1' >              47 </td> <td colspan='1' > [-0.00974910240 </td> <tr>
<td colspan='1' >             999 </td> <td colspan='1' >             619 </td> <td colspan='1' >      B007PA32L2 </td> <td colspan='1' >  A15FF2P7RPKH6G </td> <td colspan='1' >               5 </td> <td colspan='1' > got this for th </td> <td colspan='1' > all i have hear </td> <td colspan='1' > Title: got this </td> <td colspan='1' >              50 </td> <td colspan='1' > [-0.00521062919 </td> <tr>
<td colspan='1' >            1000 </td> <td colspan='1' >             999 </td> <td colspan='1' >      B001EQ5GEO </td> <td colspan='1' >  A3VYU0VO6DYV6I </td> <td colspan='1' >               5 </td> <td colspan='1' > I love Maui Cof </td> <td colspan='1' > My first experi </td> <td colspan='1' > Title: I love M </td> <td colspan='1' >             118 </td> <td colspan='1' > [-0.00605782261 </td> <tr>
<tr>
<td colspan='10' style="text-align: left;"> [1000 rows x 9 columns] </td> <tr>
<tr><td colspan='10' style='text-align: left;'></td></tr><tr><td colspan='10' style='text-align: left;'>*  Use DataGrid.save() to save to disk</td></tr><tr><td colspan='10' style='text-align: left;'>** Use DataGrid.show() to start user interface</td></tr></table>



Now, we create a new DataGrid, converting the numbers into an Embedding:


```python
import ast # to convert string of a list of numbers into a list of numbers

dg = kg.DataGrid(
    name="openai_embeddings",
    columns=data.get_columns(),
    converters={"Score": str},
)
for row in data:
    embedding = ast.literal_eval(row[8])
    row[8] = kg.Embedding(
        embedding, 
        name=str(row[3]), 
        text="%s - %.10s" % (row[3], row[4]),
        projection="umap",
    )
    dg.append(row)
```

The new DataGrid now has an Embedding column with proper datatype.


```python
dg.info()
```

    DataGrid (in memory)
        Name   : openai_embeddings
        Rows   : 1,000
        Columns: 9
    #   Column                Non-Null Count DataGrid Type       
    --- -------------------- --------------- --------------------
    1   Column 1                       1,000 INTEGER             
    2   ProductId                      1,000 TEXT                
    3   UserId                         1,000 TEXT                
    4   Score                          1,000 TEXT                
    5   Summary                        1,000 TEXT                
    6   Text                           1,000 TEXT                
    7   combined                       1,000 TEXT                
    8   n_tokens                       1,000 INTEGER             
    9   embedding                      1,000 EMBEDDING-ASSET     
    

We simply save the datagrid, and we're done.


```python
dg.save()
```

### 3. Render 2D Projections

To render the data directly in the notebook, simply show it. Note that each row contains an embedding projection. 

Scroll to far right to see embeddings projection per row.

The color of the point in projection space represents the Score.


```python
dg.show()
```



<iframe
    width="100%"
    height="750px"
    src="http://127.0.1.1:4000/?datagrid=openai_embeddings.datagrid&timestamp=1685559502.7515423"
    frameborder="0"
    allowfullscreen

></iframe>



Group by "Score" to see rows of each group.


```python
dg.show(group="Score", sort="Score", rows=5, select="Score,embedding")
```



<iframe
    width="100%"
    height="750px"
    src="http://127.0.1.1:4000/?datagrid=openai_embeddings.datagrid&timestamp=1685559502.7515423&group=Score&sort=Score&rows=5&select=Score%2Cembedding"
    frameborder="0"
    allowfullscreen

></iframe>



An example of this datagrid is hosted here: https://kangas.comet.com/?datagrid=/data/openai_embeddings.datagrid




################################################## Visualizing_embeddings_in_wandb.md ##################################################


## Visualizing embeddings in W&B

We will upload the data to [Weights & Biases](http://wandb.ai) and use an [Embedding Projector](https://docs.wandb.ai/ref/app/features/panels/weave/embedding-projector) to visualize the embeddings using common dimension reduction algorithms like PCA, UMAP, and t-SNE. The dataset is created in the [Get_embeddings_from_dataset Notebook](Get_embeddings_from_dataset.ipynb).

## What is Weights & Biases?

[Weights & Biases](http://wandb.ai) is a machine learning platform used by OpenAI and other ML teams to build better models faster. They use it to quickly track experiments, evaluate model performance, reproduce models, visualize results, and share findings with colleagues.

### 1. Log the data to W&B

We create a [W&B Table](https://docs.wandb.ai/guides/data-vis/log-tables) with the original data and the embeddings. Each review is a new row and the 1536 embedding floats are given their own column named `emb_{i}`.


```python
import pandas as pd
from sklearn.manifold import TSNE
import numpy as np
from ast import literal_eval

# Load the embeddings
datafile_path = "data/fine_food_reviews_with_embeddings_1k.csv"
df = pd.read_csv(datafile_path)

# Convert to a list of lists of floats
matrix = np.array(df.embedding.apply(literal_eval).to_list())
```


```python
import wandb

original_cols = df.columns[1:-1].tolist()
embedding_cols = ['emb_'+str(idx) for idx in range(len(matrix[0]))]
table_cols = original_cols + embedding_cols

with wandb.init(project='openai_embeddings'):
    table = wandb.Table(columns=table_cols)
    for i, row in enumerate(df.to_dict(orient="records")):
        original_data = [row[col_name] for col_name in original_cols]
        embedding_data = matrix[i].tolist()
        table.add_data(*(original_data + embedding_data))
    wandb.log({'openai_embedding_table': table})
```

### 2. Render as 2D Projection

After navigating to the W&B run link, we click the ⚙️ icon in the top right of the Table and change "Render As:" to "Combined 2D Projection". 

Example: http://wandb.me/openai_embeddings




################################################## Visualizing_embeddings_with_Atlas.md ##################################################


## Visualizing Open AI Embeddings in Atlas

In this example, we will upload food review embeddings to [Atlas](https://atlas.nomic.ai) to visualize the embeddings.

## What is Atlas?

[Atlas](https://atlas.nomic.ai) is a machine learning tool used to visualize massive datasets of embeddings in your web browser. Upload millions of embeddings to Atlas and interact with them in your web browser or jupyter notebook.

### 1. Login to Atlas.



```python
!pip install nomic
```

    


```python
import pandas as pd
import numpy as np
from ast import literal_eval

# Load the embeddings
datafile_path = "data/fine_food_reviews_with_embeddings_1k.csv"
df = pd.read_csv(datafile_path)

# Convert to a list of lists of floats
embeddings = np.array(df.embedding.apply(literal_eval).to_list())
df = df.drop('embedding', axis=1)
df = df.rename(columns={'Unnamed: 0': 'id'})

```


```python
import nomic
from nomic import atlas
nomic.login('7xDPkYXSYDc1_ErdTPIcoAR9RNd8YDlkS3nVNXcVoIMZ6') #demo account

data = df.to_dict('records')
project = atlas.map_embeddings(embeddings=embeddings, data=data,
                               id_field='id',
                               colorable_fields=['Score'])
map = project.maps[0]
```

    

### 2. Interact with your embeddings in Jupyter


```python
map
```





    <h3>Project: meek-laborer</h3>
    <script>
    destroy = function() {
        document.getElementById("iframe463f4614-7689-47e4-b55b-1da0cc679559").remove()
    }
</script>

<h4>Projection ID: 463f4614-7689-47e4-b55b-1da0cc679559</h4>
<div class="actions">
    <div id="hide" class="action" onclick="destroy()">Hide embedded project</div>
    <div class="action" id="out">
        <a href="https://atlas.nomic.ai/map/fddc0e07-97c5-477c-827c-96bca44519aa/463f4614-7689-47e4-b55b-1da0cc679559" target="_blank">Explore on atlas.nomic.ai</a>
    </div>
</div>

<iframe class="iframe" id="iframe463f4614-7689-47e4-b55b-1da0cc679559" allow="clipboard-read; clipboard-write" src="https://atlas.nomic.ai/map/fddc0e07-97c5-477c-827c-96bca44519aa/463f4614-7689-47e4-b55b-1da0cc679559">
</iframe>

<style>
    .iframe {
        /* vh can be **very** large in vscode ipynb. */
        height: min(75vh, 66vw);
        width: 100%;
    }
</style>

<style>
    .actions {
      display: block;
    }
    .action {
      min-height: 18px;
      margin: 5px;
      transition: all 500ms ease-in-out;
    }
    .action:hover {
      cursor: pointer;
    }
    #hide:hover::after {
        content: " X";
    }
    #out:hover::after {
        content: "";
    }
</style>






```python

```




################################################## visual_captioning.md ##################################################


```
# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# Visual captioning with Imagen on Vertex AI

<table align="left">
  <td style="text-align: center">
    <a href="https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/visual_captioning.ipynb">
      <img src="https://cloud.google.com/ml-engine/images/colab-logo-32px.png" alt="Google Colaboratory logo"><br> Run in Colab
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/visual_captioning.ipynb">
      <img src="https://cloud.google.com/ml-engine/images/github-logo-32px.png" alt="GitHub logo"><br> View on GitHub
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/vision/getting-started/visual_captioning.ipynb">
      <img src="https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32" alt="Vertex AI logo"><br> Open in Vertex AI Workbench
    </a>
  </td>
</table>


| | |
|-|-|
|Author(s) | [Thu Ya Kyaw](https://github.com/iamthuya) |

## Overview

[Imagen on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/image/overview) (image Generative AI) offers a variety of features:
- Image generation
- Image editing
- Visual captioning
- Visual question answering

This notebook focuses on **visual captioning** only.

[Visual captioning with Imagen on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/image/image-captioning) can generate text descriptions of images. The model takes in an image as input and produces one or more text descriptions of the image as output. The generated text descriptions can be used for a variety of use cases:
- getting detailed metadata about images for storing and searching
- generating automated captioning to support accessibility use cases
- producing descriptions of products and visual assets

More information about Visual captioning with Imagen on Vertex AI can be found in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/image/image-captioning).

### Objectives

In this notebook, you will learn how to use the Vertex AI Python SDK to:

- Generate image captions using the Imagen's visual captioning features

- Experiment with different parameters, such as:
    - number of captions to be generated
    - language of the generated captions
    - type and version of model that is used to generate the captions


### Costs

- This notebook uses billable components of Google Cloud:
  - Vertex AI (Imagen)

- Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.

## Getting Started

### Install Vertex AI SDK, other packages and their dependencies


```
%pip install --upgrade --user google-cloud-aiplatform
```

### Restart current runtime

To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel.


```
# Restart kernel after installs so that your environment can access the new packages
import IPython

app = IPython.Application.instance()
app.kernel.do_shutdown(True)
```

<div class="alert alert-block alert-warning">
<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>
</div>

### Authenticate your notebook environment (Colab only)

If you are running this notebook on Google Colab, you will need to authenticate your environment. To do this, run the new cell below. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).


```
import sys

# Additional authentication is required for Google Colab
if "google.colab" in sys.modules:
    # Authenticate user to Google Cloud
    from google.colab import auth

    auth.authenticate_user()
```

### Define Google Cloud project information and initialize Vertex AI

Initialize the Vertex AI SDK for Python for your project:


```
# Define project information
PROJECT_ID = "[your-project-id]"  # @param {type:"string"}
LOCATION = "us-central1"  # @param {type:"string"}

# Initialize Vertex AI
import vertexai

vertexai.init(project=PROJECT_ID, location=LOCATION)
```

### Load the image captioning model

The model names from Vertex AI Imagen have two components: model name and version number. The naming convention follow this format: `<model-name>@<version-number>`. For example, `imagetext@001` represent the version **001** of the **imagetext** model.



```
from vertexai.preview.vision_models import ImageCaptioningModel

image_captioning_model = ImageCaptioningModel.from_pretrained("imagetext@001")
```

### Load the image file

To use the visual captioning model, you first need to create an `Image` class using the image file. The model only accepts `Image` class objects, so this is a necessary step before you can generate captions.

Moreover, [Visual Captioning with Imagen](https://cloud.google.com/vertex-ai/docs/generative-ai/image/image-captioning) only accepts specific image file formats (e.g. PNG, JPEG), and may have file size is limitations (e.g. 10 MB). You can find out specific details from [this official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/image/image-captioning#img-cap-rest).


```
from vertexai.preview.vision_models import Image

# Use either a Google Cloud Storage URI or a local file path.
file_path = (
    "gs://github-repo/img/vision/google-cloud-next.jpeg"  # @param {type:"string"}
)

# Load the image file as Image object
cloud_next_image = Image.load_from_file(file_path)
cloud_next_image.show()
```


    
![png](output_19_0.png)
    


###  Generate captions from the image

In this section, you will use the visual captioning model to generate text descriptions of an image.


```
# Get a caption from the image
image_captioning_model.get_captions(
    image=cloud_next_image,
)
```




    ['a group of people sitting in front of a google cloud next sign']



You can generate up to three captions from a single image by changing the `number_of_results` parameter from 1 to 3.


```
# Get 3 captions from the image
image_captioning_model.get_captions(
    image=cloud_next_image,
    number_of_results=3,
)
```




    ['a group of people sitting in front of a google cloud next sign',
     'a group of people sitting in front of a google cloud next stage',
     'a group of people are sitting in front of a google cloud next sign']



### Generating captions in non-English languages

Visual captioning with Imagen on Vertex AI can generate captions in multiple languages as well. To generate a caption in a specific language, you can set the `language` parameter as one of the values:
- `en` - English
- `fr` - French
- `de` - German
- `it` - Italian
- `es` - Spanish

For a list of supported languages, check out the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/image/image-captioning#languages).


```
# Get 3 image captions in French
image_captioning_model.get_captions(
    image=cloud_next_image,
    number_of_results=3,
    language="fr",
)
```




    ['un groupe de personnes assises devant un écran qui dit google cloud next',
     'un groupe de personnes assises devant un écran avec le logo google cloud next',
     'un groupe de personnes assises devant un écran avec le mot google cloud next']



## Try it yourself

You can also try using the visual captioning model with images of your choice. If you need to download an image file, you can use the provided auxiliary function `download_image`.

Feel free to experiment with different images and model parameters to see how the results change.


```
import os

import requests


def download_image(url: str) -> str:
    """Downloads an image from the specified URL."""

    # Send a get request to the url
    response = requests.get(url)

    if response.status_code != 200:
        raise Exception(f"Failed to download image from {url}")

    # Define image related variables
    image_path = os.path.basename(url)
    image_bytes = response.content
    image_type = response.headers["Content-Type"]

    # Check for image type, currently only PNG or JPEG format are supported
    if image_type not in {"image/png", "image/jpeg"}:
        raise ValueError("Image can only be in PNG or JPEG format")

    # Write image data to a file
    with open(image_path, "wb") as f:
        f.write(image_bytes)
    return image_path
```


```
# Download an image
url = "https://storage.googleapis.com/gweb-cloudblog-publish/images/transfor_shared_fate.0999075519991154.max-2000x2000.jpg"
image_path = download_image(url)
```


```
# Load the newly downloaded image
user_image = Image.load_from_file(image_path)
user_image.show()
```


```
# Generate the visual captions for the image
image_captioning_model.get_captions(
    image=user_image,
    number_of_results=3,
    language="en",
)
```




################################################## visual_question_answering.md ##################################################


```
# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# Visual Question Answering (VQA) with Imagen on Vertex AI

<table align="left">
  <td style="text-align: center">
    <a href="https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/visual_question_answering.ipynb">
      <img src="https://cloud.google.com/ml-engine/images/colab-logo-32px.png" alt="Google Colaboratory logo"><br> Run in Colab
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/visual_question_answering.ipynb">
      <img src="https://cloud.google.com/ml-engine/images/github-logo-32px.png" alt="GitHub logo"><br> View on GitHub
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/vision/getting-started/visual_question_answering.ipynb">
      <img src="https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32" alt="Vertex AI logo"><br> Open in Vertex AI Workbench
    </a>
  </td>
</table>


| | |
|-|-|
|Author(s) | [Thu Ya Kyaw](https://github.com/iamthuya) |

## Overview

[Imagen on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/image/overview) (image Generative AI) offers a variety of features:
- Image generation
- Image editing
- Visual captioning
- Visual question answering

This notebook focuses on **visual question answering** only.

[Visual question answering (VQA) with Imagen](https://cloud.google.com/vertex-ai/docs/generative-ai/image/visual-question-answering) can understand the content of an image and answer questions about it. The model takes in an image and a question as input, and then using the image as context to produce one or more answers to the question.

The visual question answering (VQA) can be used for a variety of use cases, including:
- assisting the visually impaired to gain more information about the images
- answering customer questions about products or services in the image
- creating interactive learning environment and providing interactive learning experiences

### Objectives

In this notebook, you will learn how to use the Vertex AI Python SDK to:

- Answering questions about images using the Imagen's visual question answering features

- Experiment with different parameters, such as:
    - number of answers to be provided by the model

### Costs

This tutorial uses billable components of Google Cloud:
- Vertex AI (Imagen)

Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.

## Getting Started

### Install Vertex AI SDK, other packages and their dependencies


```
%pip install --upgrade --user google-cloud-aiplatform
```

### Restart current runtime

To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel.


```
# Restart kernel after installs so that your environment can access the new packages
import IPython

app = IPython.Application.instance()
app.kernel.do_shutdown(True)
```

<div class="alert alert-block alert-warning">
<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>
</div>


### Authenticate your notebook environment (Colab only)

If you are running this notebook on Google Colab, you will need to authenticate your environment. To do this, run the new cell below. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).


```
import sys

if "google.colab" in sys.modules:
    # Authenticate user to Google Cloud
    from google.colab import auth

    auth.authenticate_user()
```

### Define Google Cloud project information and initialize Vertex AI

Initialize the Vertex AI SDK for Python for your project:


```
# Define project information
PROJECT_ID = "[your-project-id]"  # @param {type:"string"}
LOCATION = "us-central1"  # @param {type:"string"}

# Initialize Vertex AI
import vertexai

vertexai.init(project=PROJECT_ID, location=LOCATION)
```

### Load the image question answering model

The model names from Vertex AI Imagen have two components: model name and version number. The naming convention follows this format: `<model-name>@<version-number>`.

For example, `imagetext@001` represents the version **001** of the **imagetext** model.


```
from vertexai.preview.vision_models import ImageQnAModel

image_qna_model = ImageQnAModel.from_pretrained("imagetext@001")
```

### Load the image file

To use the image question answering model, you first need to create an `Image` class using the image file. The model only accepts `Image` class objects, so this is a necessary step before you can ask questions.

Additionally, [Visual Question Answering with Imagen](https://cloud.google.com/vertex-ai/docs/generative-ai/image/visual-question-answering) only accepts specific image file formats (e.g. PNG, JPEG), and may have file size limitations (e.g. 10 MB). You can find out specific details from the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/image/visual-question-answering#img-vqa-rest).


```
from vertexai.preview.vision_models import Image

# Use either a Google Cloud Storage URI or a local file path.
file_path = (
    "gs://github-repo/img/vision/google-cloud-next.jpeg"  # @param {type:"string"}
)

# Load the image file as Image object
cloud_next_image = Image.load_from_file(file_path)
cloud_next_image.show()
```

### Ask questions about the image

Now ask questions about the image using the model:


```
# Ask a question about the image
image_qna_model.ask_question(
    image=cloud_next_image, question="What is happening in this image?"
)
```


```
# Ask a follow up question about the image
image_qna_model.ask_question(
    image=cloud_next_image, question="What are the people in the image doing?"
)
```

You can get up to three answers from a single image by changing the `number_of_results` parameter from 1 to 3.


```
# Get 3 answers from the image
image_qna_model.ask_question(
    image=cloud_next_image,
    question="What are the people in the image doing?",
    number_of_results=3,
)
```

## Try it yourself

You can also try using the visual question answering model with images of your choice. If you need to download an image file, you can use the provided auxiliary function `download_image`.

Feel free to experiment with different images and model parameters to see how the results change.


```
import os

import requests


def download_image(url: str) -> str:
    """Downloads an image from the specified URL."""

    # Send a get request to the url
    response = requests.get(url)

    if response.status_code != 200:
        raise Exception(f"Failed to download image from {url}")

    # Define image related variables
    image_path = os.path.basename(url)
    image_bytes = response.content
    image_type = response.headers["Content-Type"]

    # Check for image type, currently only PNG or JPEG format are supported
    if image_type not in {"image/png", "image/jpeg"}:
        raise ValueError("Image can only be in PNG or JPEG format")

    # Write image data to a file
    with open(image_path, "wb") as f:
        f.write(image_bytes)
    return image_path
```


```
# Download an image
url = "https://storage.googleapis.com/gweb-cloudblog-publish/images/GettyImages-871168786.max-2600x2600.jpg"
image_path = download_image(url)
```


```
# Load the newly downloaded image
user_image = Image.load_from_file(image_path)
user_image.show()
```


```
# Ask a question about the image
image_qna_model.ask_question(
    image=user_image,
    question="What is happening in this photo?",
    number_of_results=3,
)
```


```
# Ask a question about the image
image_qna_model.ask_question(
    image=user_image,
    question="What advertising channels would this image be suitable for?",
    number_of_results=3,
)
```


```
# Ask a question about the image
image_qna_model.ask_question(
    image=user_image,
    question="What type of insects could live in this area?",
    number_of_results=3,
)
```




################################################## vlite.md ##################################################


# vlite

VLite is a simple and blazing fast vector database that allows you to store and retrieve data semantically using embeddings. Made with numpy, vlite is a lightweight batteries-included database to implement RAG, similarity search, and embeddings into your projects.

You'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration

## Installation

To use the VLite in LangChain, you need to install the `vlite` package:

```bash
!pip install vlite
```

## Importing VLite

```python
from langchain_community.vectorstores import VLite
```

## Basic Example

In this basic example, we load a text document, and store them in the VLite vector database. Then, we perform a similarity search to retrieve relevant documents based on a query.

VLite handles chunking and embedding of the text for you, and you can change these parameters by pre-chunking the text and/or embeddings those chunks into the VLite database.

```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

# Load the document and split it into chunks
loader = TextLoader("path/to/document.txt")
documents = loader.load()

# Create a VLite instance
vlite = VLite(collection="my_collection")

# Add documents to the VLite vector database
vlite.add_documents(documents)

# Perform a similarity search
query = "What is the main topic of the document?"
docs = vlite.similarity_search(query)

# Print the most relevant document
print(docs[0].page_content)
```

## Adding Texts and Documents

You can add texts or documents to the VLite vector database using the `add_texts` and `add_documents` methods, respectively.

```python
# Add texts to the VLite vector database
texts = ["This is the first text.", "This is the second text."]
vlite.add_texts(texts)

# Add documents to the VLite vector database
documents = [Document(page_content="This is a document.", metadata={"source": "example.txt"})]
vlite.add_documents(documents)
```

## Similarity Search

VLite provides methods for performing similarity search on the stored documents.

```python
# Perform a similarity search
query = "What is the main topic of the document?"
docs = vlite.similarity_search(query, k=3)

# Perform a similarity search with scores
docs_with_scores = vlite.similarity_search_with_score(query, k=3)
```

## Max Marginal Relevance Search

VLite also supports Max Marginal Relevance (MMR) search, which optimizes for both similarity to the query and diversity among the retrieved documents.

```python
# Perform an MMR search
docs = vlite.max_marginal_relevance_search(query, k=3)
```

## Updating and Deleting Documents

You can update or delete documents in the VLite vector database using the `update_document` and `delete` methods.

```python
# Update a document
document_id = "doc_id_1"
updated_document = Document(page_content="Updated content", metadata={"source": "updated.txt"})
vlite.update_document(document_id, updated_document)

# Delete documents
document_ids = ["doc_id_1", "doc_id_2"]
vlite.delete(document_ids)
```

## Retrieving Documents

You can retrieve documents from the VLite vector database based on their IDs or metadata using the `get` method.

```python
# Retrieve documents by IDs
document_ids = ["doc_id_1", "doc_id_2"]
docs = vlite.get(ids=document_ids)

# Retrieve documents by metadata
metadata_filter = {"source": "example.txt"}
docs = vlite.get(where=metadata_filter)
```

## Creating VLite Instances

You can create VLite instances using various methods:

```python
# Create a VLite instance from texts
vlite = VLite.from_texts(texts)

# Create a VLite instance from documents
vlite = VLite.from_documents(documents)

# Create a VLite instance from an existing index
vlite = VLite.from_existing_index(collection="existing_collection")
```

## Additional Features

VLite provides additional features for managing the vector database:

```python
from langchain.vectorstores import VLite
vlite = VLite(collection="my_collection")

# Get the number of items in the collection
count = vlite.count()

# Save the collection
vlite.save()

# Clear the collection
vlite.clear()

# Get collection information
vlite.info()

# Dump the collection data
data = vlite.dump()
```






################################################## vllm.md ##################################################


---
sidebar_label: vLLM Chat
---
# vLLM Chat

vLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API. This server can be queried in the same format as OpenAI API.

## Overview
This will help you getting started with vLLM [chat models](/docs/concepts/chat_models), which leverage the `langchain-openai` package. For detailed documentation of all `ChatOpenAI` features and configurations head to the [API reference](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html).

### Integration details

| Class | Package | Local | Serializable | JS support | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [langchain_openai](https://python.langchain.com/api_reference/openai/) | ✅ | beta | ❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain_openai?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain_openai?style=flat-square&label=%20) |

### Model features
Specific model features-- such as tool calling, support for multi-modal inputs, support for token-level streaming, etc.-- will depend on the hosted model.

## Setup

See the vLLM docs [here](https://docs.vllm.ai/en/latest/).

To access vLLM models through LangChain, you'll need to install the `langchain-openai` integration package.

### Credentials

Authentication will depend on specifics of the inference server.

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:


```python
# os.environ["LANGCHAIN_TRACING_V2"] = "true"
# os.environ["LANGCHAIN_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
```

### Installation

The LangChain vLLM integration can be accessed via the `langchain-openai` package:


```python
%pip install -qU langchain-openai
```

## Instantiation

Now we can instantiate our model object and generate chat completions:


```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts.chat import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
)
from langchain_openai import ChatOpenAI
```


```python
inference_server_url = "http://localhost:8000/v1"

llm = ChatOpenAI(
    model="mosaicml/mpt-7b",
    openai_api_key="EMPTY",
    openai_api_base=inference_server_url,
    max_tokens=5,
    temperature=0,
)
```

## Invocation


```python
messages = [
    SystemMessage(
        content="You are a helpful assistant that translates English to Italian."
    ),
    HumanMessage(
        content="Translate the following sentence from English to Italian: I love programming."
    ),
]
llm.invoke(messages)
```




    AIMessage(content=' Io amo programmare', additional_kwargs={}, example=False)



## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:


```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate(
    [
        (
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ),
        ("human", "{input}"),
    ]
)

chain = prompt | llm
chain.invoke(
    {
        "input_language": "English",
        "output_language": "German",
        "input": "I love programming.",
    }
)
```

## API reference

For detailed documentation of all features and configurations exposed via `langchain-openai`, head to the API reference: https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html

Refer to the vLLM [documentation](https://docs.vllm.ai/en/latest/) as well.




################################################## VLLM_Model_Testing.md ##################################################


# Set up Environment


```python
!pip install --upgrade litellm
```


```python
!pip install vllm
```

    Successfully installed fastapi-0.103.1 h11-0.14.0 huggingface-hub-0.16.4 ninja-1.11.1 pydantic-1.10.12 ray-2.6.3 safetensors-0.3.3 sentencepiece-0.1.99 starlette-0.27.0 tokenizers-0.13.3 transformers-4.33.1 uvicorn-0.23.2 vllm-0.1.4 xformers-0.0.21
    

# Load the Logs


```python
import pandas as pd
```


```python
# path of the csv file
file_path = 'Model-prompts-example.csv'

# load the csv file as a pandas DataFrame
data = pd.read_csv(file_path)

data.head()
```





  <div id="df-cd06d09e-fb43-41b0-938f-37f9d285ae66" class="colab-df-container">
    <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Success</th>
      <th>Timestamp</th>
      <th>Input</th>
      <th>Output</th>
      <th>RunId (Wandb Runid)</th>
      <th>Model ID (or Name)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>True</td>
      <td>1694041195</td>
      <td>This is the templated query input</td>
      <td>This is the query output from the model</td>
      <td>8hlumwuk</td>
      <td>OpenAI/Turbo-3.5</td>
    </tr>
  </tbody>
</table>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-cd06d09e-fb43-41b0-938f-37f9d285ae66')"
            title="Convert this dataframe to an interactive table."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"/>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-cd06d09e-fb43-41b0-938f-37f9d285ae66 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-cd06d09e-fb43-41b0-938f-37f9d285ae66');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


    </div>
  </div>





```python
input_texts = data['Input'].values
```


```python
messages = [[{"role": "user", "content": input_text}] for input_text in input_texts]
```

# Running Inference


```python
from litellm import batch_completion
model_name = "facebook/opt-125m"
provider = "vllm"
response_list = batch_completion(
            model=model_name,
            custom_llm_provider=provider, # can easily switch to huggingface, replicate, together ai, sagemaker, etc.
            messages=messages,
            temperature=0.2,
            max_tokens=80,
        )
```


```python
response_list
```




    [<ModelResponse at 0x7e5b87616750> JSON: {
       "choices": [
         {
           "finish_reason": "stop",
           "index": 0,
           "message": {
             "content": ".\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is",
             "role": "assistant",
             "logprobs": null
           }
         }
       ],
       "created": 1694053363.6139505,
       "model": "facebook/opt-125m",
       "usage": {
         "prompt_tokens": 9,
         "completion_tokens": 80,
         "total_tokens": 89
       }
     }]




```python
response_values = [response['choices'][0]['message']['content'] for response in response_list]
```


```python
response_values
```




    ['.\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is used to query the data.\n\nThe query input is the query input that is']




```python
data[f"{model_name}_output"] = response_values
```


```python
data.to_csv('model_responses.csv', index=False)
```




################################################## Voice_memos.md ##################################################


##### Copyright 2024 Google LLC.


```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# Voice memos

<table align="left">
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Voice_memos.ipynb"><img src="../images/colab_logo_32px.png" />Run in Google Colab</a>
  </td>
</table>

This notebook provides a quick example of how to work with audio and text files in the same prompt. You'll use the Gemini API to help you generate ideas for your next blog post, based on voice memos you recorded on your phone, and previous articles you've written.


```
!pip install -U -q "google-generativeai>=0.7.2"
```


```
import google.generativeai as genai
```

### Setup your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.


```
from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
genai.configure(api_key=GOOGLE_API_KEY)
```

Install PDF processing tools.


```
!apt install poppler-utils
```

## Upload your audio and text files



```
!wget https://storage.googleapis.com/generativeai-downloads/data/Walking_thoughts_3.m4a
!wget https://storage.googleapis.com/generativeai-downloads/data/A_Possible_Future_for_Online_Content.pdf
!wget https://storage.googleapis.com/generativeai-downloads/data/Unanswered_Questions_and_Endless_Possibilities.pdf
```


```
audio_file_name = "Walking_thoughts_3.m4a"
audio_file = genai.upload_file(path=audio_file_name)
```

## Extract text from the PDFs


```
!pdftotext A_Possible_Future_for_Online_Content.pdf
!pdftotext Unanswered_Questions_and_Endless_Possibilities.pdf
```


```
blog_file_name = "A_Possible_Future_for_Online_Content.txt"
blog_file = genai.upload_file(path=blog_file_name)
```


```
blog_file_name2 = "Unanswered_Questions_and_Endless_Possibilities.txt"
blog_file2 = genai.upload_file(path=blog_file_name2)
```

## System instructions

Write a detailed system instruction to configure the model.


```
si="""Objective: Transform raw thoughts and ideas into polished, engaging blog posts that capture a writers unique style and voice.
Input:
Example Blog Posts (1-5): A user will provide examples of blog posts that resonate with their desired style and tone. These will guide you in understanding the preferences for word choice, sentence structure, and overall voice.
Audio Clips: A user will share a selection of brainstorming thoughts and key points through audio recordings. They will talk freely and openly, as if they were explaining their ideas to a friend.
Output:
Blog Post Draft: A well-structured first draft of the blog post, suitable for platforms like Substack or LinkedIn.
The draft will include:
Clear and engaging writing: you will strive to make the writing clear, concise, and interesting for the target audience.
Tone and style alignment: The language and style will closely match the examples provided, ensuring consistency with the desired voice.
Logical flow and structure: The draft will be organized with clear sections based on the content of the post.
Target word count: Aim for 500-800 words, but this can be adjusted based on user preferences.
Process:
Style Analysis: Carefully analyze the example blog posts provided by the user to identify key elements of their preferred style, including:
Vocabulary and word choice: Formal vs. informal, technical terms, slang, etc.
Sentence structure and length: Short and impactful vs. longer and descriptive sentences.
Tone and voice: Humorous, serious, informative, persuasive, etc.
Audio Transcription and Comprehension: Your audio clips will be transcribed with high accuracy. you will analyze them to extract key ideas, arguments, and supporting points.
Draft Generation: Using the insights from the audio and the style guidelines from the examples, you will generate a first draft of the blog post. This draft will include all relevant sections with supporting arguments or evidence, and a great ending that ties everything together and makes the reader want to invest in future readings.
"""
```

## Generate Content


```
prompt = "Draft my next blog post based on my thoughts in this audio file and these two previous blog posts I wrote."

model = genai.GenerativeModel(model_name="models/gemini-1.5-flash", system_instruction=si)

response = model.generate_content([prompt, blog_file, blog_file2, audio_file],
                                  request_options={"timeout": 600})
print(response.text)
```

    ## The Throwaway Work That Makes You Better
    
    Early in my career, I spent a lot of time working on visions, roadmaps, and ideas. Some of them ended up not happening at all, or were essentially thrown away.  It was frustrating, especially coming straight out of school with the idea that you’re given an assignment, you do it, and then you’re graded on it.  There's no "takesies-backsies" in school.  You get the assignment, you produce the work, and that’s that. 
    
    The real world is a lot different. 
    
    It's a constant adjustment, where priorities change, markets shift, and sometimes the work you do doesn't get used or even goes nowhere.  It felt like a colossal waste of time!
    
    It took me a while to get over it, but I don’t think I truly appreciated this until I joined my current team. They have a "right to think" culture, and that changed everything. 
    
    Suddenly, I realized that the work you produce, the content you create, is part of the process of making you better at what you do in the future.  It’s about honing your skills and getting better over time. 
    
    The "right to think" culture isn’t simply about accepting that priorities change and things shift; it's about recognizing that what you produce, even if it's not the final product, is valuable.  It's about learning and iterating.
    
    This "right to think" idea ties in nicely with iterative processes and learning by doing.  It's a reframing of the mindset around throwaway work.  There's no such thing as throwaway work.  It's all helping you to hone your skills and get better over time. 
    
    I'm constantly talking about this reframing, and I think it’s important to call out the importance of writing to think. It's about being willing to scrap things and move on once they’ve served their purpose.  Write more, write earlier, write often. Don’t worry about the final product. Just get it out there.
    
    In the end, the work you produce, even if it's ultimately discarded, is still part of the process that makes you a better thinker, a better creator, and a better professional. So, don’t be afraid to throw things away. It might just make you better in the long run. 
    
    

## Learning more

* Learn more about the [File API](https://github.com/google-gemini/cookbook/blob/main/quickstarts/File_API.ipynb) with the quickstart.




################################################## voice_translation_into_different_languages_using_GPT-4o.md ##################################################


### Voice Translation of Audio Files into Different Languages Using Gpt-4o

Have you ever wanted to translate a podcast into your native language? Translating and dubbing audio content can make it more accessible to audiences worldwide. With GPT-4o's new audio-in and audio-out modality, this process is now easier than ever.

This guide will walk you through translating an English audio file into Hindi using OpenAI's GPT-4o audio modality API.

GPT-4o simplifies the dubbing process for audio content. Previously, you had to convert the audio to text and then translate the text into the target language before converting it back into audio. Now, with GPT-4o’s voice-to-voice capability, you can achieve this in a single step with audio input and output.  

A note on semantics used in this Cookbook regarding **Language** and written **Script**. These words are generally used interchangeably, though it's important to understand the distinction, given the task at hand.  
 
**- Language** refers to the spoken or written system of communication. For instance, Hindi and Marathi are different languages, but both use the Devanagari script. Similarly, English and French are different languages, but are written in Latin script.  
   
**- Script** refers to the set of characters or symbols used to write the language. For example, Serbian language traditionally written in Cyrillic Script, is also written in Latin script.


GPT-4o audio-in and audio-out modality makes it easier to dub the audio from one language to another with one API call.  

**1. Transcribe** the source audio file into source language script using GPT-4o. This is an optional step that can be skipped if you already have the transcription of source audio content.  
 
**2. Dub** the audio file from source language directly to the target langauge.  
   
**3. Obtain Translation Benchmarks** using BLEU or ROUGE.   
 
**4. Interpret and improve** scores by adjusting prompting parameters in steps 1-3 as needed.   


Before we get started, make sure you have your OpenAI API key configured as an environment variable, and necessary packages installed as outlined in the code cells below. 

### Step 1: Transcribe the Audio to Source Language Script using GPT-4o 

Let's start by creating a function that sends an audio file to OpenAI's GPT-4o API for processing, using the chat completions API endpoint.

The function `process_audio_with_gpt_4o` takes three inputs:

1. A base64-encoded audio file (base64_encoded_audio) that will be sent to the GPT-4o model.
2. Desired output modalities (such as text, or both text and audio). 
3. A system prompt that instructs the model on how to process the input.

The function sends an API request to OpenAI's chat/completions endpoint. The request headers include the API key for authorization. The data payload contains the model type (`gpt-4o-audio-preview`), the selected output modalities, and audio details, such as the voice type and format (in this case, "alloy" and "wav"). It also includes the system prompt and the base64-encoded audio file as part of the "user" message. If the API request is successful (HTTP status 200), the response is returned as JSON. If an error occurs (non-200 status), it prints the error code and message.

This function enables audio processing through OpenAI's GPT-4o API, allowing tasks like dubbing, transcription, or translation to be performed based on the input provided.


```python
# Make sure requests package is installed  
import requests 
import os
import json

# Load the API key from the environment variable
api_key = os.getenv("OPENAI_API_KEY")


def process_audio_with_gpt_4o(base64_encoded_audio, output_modalities, system_prompt):
    # Chat Completions API end point 
    url = "https://api.openai.com/v1/chat/completions"

    # Set the headers
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    # Construct the request data
    data = {
        "model": "gpt-4o-audio-preview",
        "modalities": output_modalities,
        "audio": {
            "voice": "alloy",
            "format": "wav"
        },
        "messages": [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "input_audio",
                        "input_audio": {
                            "data": base64_encoded_audio,
                            "format": "wav"
                        }
                    }
                ]
            }
        ]
    }
    
    request_response = requests.post(url, headers=headers, data=json.dumps(data))
    if request_response.status_code == 200:
        return request_response.json()
    else:  
        print(f"Error {request_response.status_code}: {request_response.text}")
        return
    
```

Using the function `process_audio_with_gpt_4o`, we will first get an English transcription of the source audio. You can skip this step if you already have a transcription in the source language. 

In this step, we: 
1. Read the WAV file and convert it into base64 encoding.
2. Set the output modality to ["text"], as we only need a text transcription.
3. Provide a system prompt to instruct the model to focus on transcribing the speech and to ignore background noises like applause.
4. Call the process_audio_with_gpt_4o function to process the audio and return the transcription.


```python
import base64
audio_wav_path = "./sounds/keynote_recap.wav"

# Read the WAV file and encode it to base64
with open(audio_wav_path, "rb") as audio_file:
    audio_bytes = audio_file.read()
    english_audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')

modalities = ["text"]
prompt = "The user will provide an audio file in English. Transcribe the audio to English text, word for word. Only provide the language transcription, do not include background noises such as applause. "

response_json = process_audio_with_gpt_4o(english_audio_base64, modalities, prompt)

english_transcript = response_json['choices'][0]['message']['content']

print(english_transcript)
```

    Hello and welcome to our first ever OpenAI DevDay. Today, we are launching a new model, GPT-4 Turbo. GPT-4 Turbo supports up to 128,000 tokens of context. We have a new feature called JSON mode, which ensures that the model will respond with valid JSON. You can now call many functions at once. And it will do better at following instructions in general. You want these models to be able to access better knowledge about the world, so do we. So we're launching retrieval in the platform. You can bring knowledge from outside documents or databases into whatever you're building. GPT-4 Turbo has knowledge about the world up to April of 2023, and we will continue to improve that over time. DALL-E 3, GPT-4 Turbo with Vision, and the new text-to-speech model are all going into the API today. Today, we're launching a new program called Custom Models. With Custom Models, our researchers will work closely with a company to help them make a great custom model, especially for them and their use case using our tools. Higher rate limits. We're doubling the tokens per minute for all of our established GPT-4 customers so that it's easier to do more, and you'll be able to request changes to further rate limits and quotas directly in your API account settings. And GPT-4 Turbo is considerably cheaper than GPT-4, by a factor of 3x for prompt tokens and 2x for completion tokens starting today. We're thrilled to introduce GPTs. GPTs are tailored versions of ChatGPT for a specific purpose. And because they combine instructions, expanded knowledge, and actions, they can be more helpful to you. They can work better in many contexts, and they can give you better control. We know that many people who want to build a GPT don't know how to code. We've made it so that you can program the GPT just by having a conversation. You can make private GPTs. You can share your creations publicly with a link for anyone to use. Or, if you're on ChatGPT Enterprise, you can make GPTs just for your company. And later this month, we're going to launch the GPT Store. So those are GPTs, and we can't wait to see what you'll build. We're bringing the same concept to the API. The Assistant API includes persistent threads so they don't have to figure out how to deal with long conversation history, built-in retrieval, code interpreter, a working Python interpreter in a sandbox environment, and, of course, the improved function calling. As intelligence gets integrated everywhere, we will all have superpowers on demand. We're excited to see what you all will do with this technology and to discover the new future that we're all going to architect together. We hope when you come back next year, what we launch today is going to look very quaint relative to what we're busy creating for you now. Thank you for all you do. Thanks for coming here today.
    

This English transcript will serve as our ground truth as we benchmark the Hindi language dubbing of the audio in Step 3. 

### Step 2. Dub the Audio from the Source Language to the Target Language using GPT-4o

With GPT-4o, we can directly dub the audio file from English to Hindi and get the Hindi transcription of the audio in one API call. For this, we set the output modality to `["text", "audio"] `


```python
glossary_of_terms_to_keep_in_original_language = "Turbo, OpenAI, token, GPT, Dall-e, Python"

modalities = ["text", "audio"]
prompt = f"The user will provide an audio file in English. Dub the complete audio, word for word in Hindi. Keep certain words in English for which a direct translation in Hindi does not exist such as  ${glossary_of_terms_to_keep_in_original_language}."

response_json = process_audio_with_gpt_4o(english_audio_base64, modalities, prompt)

message = response_json['choices'][0]['message']
```

In the following code snippet, we will retrieve both the Hindi transcription and the dubbed audio from the GPT-4o response. Previously, this would have been a multistep process, involving several API calls to first transcribe, then translate, and finally produce the audio in the target language. With GPT-4o, we can now accomplish this in a single API call.


```python
# Make sure pydub is installed 
from pydub import AudioSegment
from pydub.playback import play
from io import BytesIO

# Get the transcript from the model. This will vary depending on the modality you are using. 
hindi_transcript = message['audio']['transcript']

print(hindi_transcript)

# Get the audio content from the response 
hindi_audio_data_base64 = message['audio']['data']

```

    स्वागत है हमारे पहले OpenAI DevDay में।
    
    आज हम एक नए मॉडल का लॉन्च कर रहे हैं, GPT-4 Turbo। GPT-4 Turbo अब 1,28,000 टोकens के कॉन्टेक्स्ट को सपोर्ट करता है। हमारे पास एक नया फीचर है जिसे JSON मोड कहा जाता है, जो सुनिश्चित करता है कि मॉडल वैध JSON के साथ प्रतिक्रिया करेगा। अब आप कई फंक्शन्स को एक साथ कॉल कर सकते हैं। और ये सामान्य रूप से इंस्ट्रक्शंस का पालन करने में बेहतर करेगा। आप चाहते हैं कि ये मॉडल दुनिया के बारे में बेहतर जानकारी तक पहुंच सकें, हम भी। इसलिए हम प्लैटफॉर्म में Retrieval लॉन्च कर रहे हैं। आप बाहरी दस्तावेज़ या डेटाबेस से जो भी आप बना रहे हैं, उसमें ज्ञान ला सकते हैं। GPT-4 Turbo को अप्रैल 2023 तक की दुनिया की जानकारी है, और हम इसे समय के साथ और बेहतर बनाना जारी रखेंगे। DALL·E 3, GPT-4 Turbo with vision, और नया Text-to-Speech मॉडल सभी को आज उपलब्ध कर रहे हैं एपीआई में। आज हम एक नए प्रोग्राम का लॉन्च कर रहे हैं जिसे Custom Models कहा जाता है। Custom Models के साथ, हमारे शोधकर्ता एक कंपनी के साथ निकटता से काम करेंगे ताकि वे एक महान Custom Model बना सकें, विशेष रूप से उनके और उनके उपयोग के मामले के लिए, हमारे Tools का उपयोग करके। उच्च दर लिमिट्स, हम सभी मौजूदा GPT-4 ग्राहकों के लिए Tokens प्रति मिनट को दोगुना कर रहे हैं ताकि अधिक करना आसान हों। और आप अपने एपीआई खाता सेटिंग्स में सीधे दर की सीमाओं और कोटों में बदलाव के लिए अनुरोध कर सकेंगे। और GPT-4 Turbo जीपीटी-4 की तुलना में काफी सस्ता है; प्रॉम्प्ट टोकन्स के लिए 3x और कम्पलीटेशन टोकन्स के लिए 2x से, आज से।
    
    हम जीपीटीस पेश कर रहे हैं। GPTs चैट GPT के कस्टमाइज़्ड संसकरण हैं, एक विशिष्ट उद्देश्य के लिए। और क्योंकि वे इंस्ट्रक्शंस, विस्तारित ज्ञान, और कार्रवाइयों को जोड़ते हैं, वे आपके लिए अधिक मददगार हो सकते हैं। वे कई सामाजीक उपयोग में बेहतर काम कर सकते हैं और आपको बेहतर नियंत्रण दे सकते हैं। हम जानते हैं कि कई लोग जो GPT बनाना चाहते हैं, उन्हें कोडिंग का ज्ञान नहीं है। हमने इसे एसे बनाया है कि आप GPT को केवल एक बातचीत से प्रोग्राम कर सकते हैं। आप प्राइवेट GPT बना सकते हैं। आप अपनी creation को किसी भी के लिए उपयोग करने के लिए लिंक के साथ सार्वजनिक रूप से शेयर कर सकते हैं। या, अगर आप ChatGPT एंटरप्राइज पर हैं, तो आप केवल अपनी कंपनी के लिए GPT बना सकते हैं। और इस महीने के बाद में हम GPT स्टोर लॉन्च करेंगे। तो ये हैं GPTs, और हम उत्सुक हैं देखने के लिए कि आप क्या बनाएंगे।
    
    हम एपीआई में वही संस्कल्पना ला रहे हैं। सहायक एपीआई में persistent threads शामिल हैं, ताकि उन्हें लंबी बातचीत के इतिहास से निपटने का तरीका पता न करना पड़े। बिल्ट-इन Retrieval, कोड इंटरप्रेटर, एक काम करने वाला Python इंटरप्रेटर एक सैंडबॉक्स वातावरण में, और of course, सुधरा हुआ फंक्शन कॉलिंग भी शामिल है।
    
    जैसे-जैसे बुद्धिमत्ता हर जगह एकीकृत होती जाएगी, हम सभी के पास मांग पर सुपर पावर्स होंगे। हम देखने के लिए उत्साहित हैं कि आप सब इस तकनीक के साथ क्या कर पाएंगे और उस नए भविष्य की खोज, जिसे हम सब मिलकर बनाने वाले हैं।
    
    हम आशा करते हैं कि आप अगले साल फिर आएंगे क्योंकि आज हमने जो लॉन्च किया है, वह उस परिप्रेक्ष्य से बहुत मामूली लगेगा जो हम अब आपके लिए बना रहे हैं। आप सभी के तरीके लिए धन्यवाद। आज यहां आने के लिए धन्यवाद।
    
    
    

The transcribed text is a combination of Hindi and English, represented in their respective scripts: Devanagari for Hindi and Latin for English. This approach ensures more natural-sounding speech with the correct pronunciation of both languages' words. We will use the `pydub` module to play the audio as demonstrated in the code below. 


```python
# Play the audio 
audio_data_bytes = base64.b64decode(hindi_audio_data_base64)
audio_segment = AudioSegment.from_file(BytesIO(audio_data_bytes), format="wav")

play(audio_segment)
```

### Step 3. Obtain Translation Benchmarks (e.g., BLEU or ROUGE) 

We can assess the quality of the translated text by comparing it to a reference translation using evaluation metrics like BLEU and ROUGE. 

**BLEU (Bilingual Evaluation Understudy)**: Measures the overlap of n-grams between the candidate and reference translations. Scores range from 0 to 100, with higher scores indicating better quality.

**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: Commonly used for summarization evaluation. Measures the overlap of n-grams and the longest common subsequence between the candidate and reference texts.

Ideally, a reference translation (a human-translated version) of the original text is needed for an accurate evaluation. However, developing such evaluations can be challenging, as it requires time and effort from bilingual humans proficient in both languages.

An alternative is to transcribe the output audio file from the target language back into the original language to assess the quality of the translation using GPT-4o. 


```python
# Translate the audio output file generated by the model back into English and compare with the reference text 
modalities = ["text"]
prompt = "The user will provide an audio file in Hindi. Transcribe the audio to English text word for word. Only provide the language transcription, do not include background noises such as applause. "

response_json = process_audio_with_gpt_4o(hindi_audio_data_base64, modalities, prompt)

re_translated_english_text = response_json['choices'][0]['message']['content']

print(re_translated_english_text)
```

    Welcome to our first OpenAI Dev Day. Today we are launching a new model, GPT-4 Turbo. GPT-4 Turbo now supports a context of 128,000 tokens. We have a new feature called JSON mode where the model will respond via JSON. Now you can call multiple functions simultaneously, and it will generally follow instructions better. You want this model to access external knowledge databases or documents to bring knowledge into what you are building. GPT-4 Turbo has knowledge of the world up to April 2023, and we'll continue to improve it over time.
    
    DALL·E 3, GPT-4 Turbo with vision, and the new text-to-speech model are all being made available today in the API. Today, we are launching a new program called custom models. Custom models will work closely to make great custom models specifically for you and your use case. Utilizing our tools, we are doubling the rate limits for all existing GPT-4 customers to tokens per minute. You'll be able to directly request rate limit and quota changes in your API account settings.
    
    And GPT-4 Turbo is much cheaper compared to GPT-4, 2x for completion tokens starting today. We are introducing GPTs. GPTs are custom versions of ChatGPT for a specific purpose, and since they incorporate instructions with broad knowledge and action capabilities, they can help you more. They can perform better in many social tasks. We know that many people who want to build GPTs don't know how to code. We've built it so that you can program a GPT with just one line.
    
    You can create a private GPT. You can publish your creation publicly with a link for anyone to use, or if you have ChatGPT Enterprise, you can build GPTs just for your own company. We will be launching a GPT store. So that's GPTs, and we're excited to see what you build. We're bringing customization into the API. Assistance API includes persistent threads so that it doesn't have to figure out how to engage with history from long conversations. A built-in retriever, code interpreter, a working Python interpreter in a sandbox environment, and of course, improved function calling. As intelligence integrates everywhere, we'll all have superpowers on demand.
    
    We're excited to see what you'll be able to build with this technology and explore this new future that we're all creating together. We hope you come back next year because what we're building for you now will make today seem very humble in that context. Thank you all for your approach. Thank you for being here today.
    

With the text transcribed back into English language script from the Hindi audio, we can run the evaluation metrics by comparing it to the original English transcription.


```python
# Make sure scarebleu package is installed 
import sacrebleu
# Make sure rouge-score package is installed 
from rouge_score import rouge_scorer 

# We'll use the original English transcription as the reference text 
reference_text = english_transcript

candidate_text = re_translated_english_text

# BLEU Score Evaluation
bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])
print(f"BLEU Score: {bleu.score}")

# ROUGE Score Evaluation
scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
scores = scorer.score(reference_text, candidate_text)
print(f"ROUGE-1 Score: {scores['rouge1'].fmeasure}")
print(f"ROUGE-L Score: {scores['rougeL'].fmeasure}")
```

    BLEU Score: 35.27656890256424
    ROUGE-1 Score: 0.8148148148148149
    ROUGE-L Score: 0.6934156378600822
    

### Step 4. Interpret and improve scores by adjusting prompting parameters in steps 1-3 as needed

In this example, both BLEU and ROUGE scores indicate that the quality of the voice translation is between very good and excellent.

**Interpreting BLEU Scores:** While there is no universally accepted scale, some interpretations suggest:

0 to 10: Poor quality translation; significant errors and lack of fluency.

10 to 20: Low quality; understandable in parts but contains many errors.

20 to 30: Fair quality; conveys the general meaning but lacks precision and fluency.

30 to 40: Good quality; understandable and relatively accurate with minor errors.

40 to 50: Very good quality; accurate and fluent with very few errors.

50 and above: Excellent quality; closely resembles human translation.

**Interpreting ROUGE scores:** The interpretation of a "good" ROUGE score can vary depending on the task, dataset, and domain. The following guidelines indicate a good outcome:

ROUGE-1 (unigram overlap): Scores between 0.5 to 0.6 are generally considered good for abstractive summarization tasks.

ROUGE-L (Longest Common Subsequence): Scores around 0.4 to 0.5 are often regarded as good, reflecting the model's ability to capture the structure of the reference text.

If the score for your translation is unsatisfactory, consider the following questions:

#### 1. Is the source audio accurately transcribed? 
If the transcription contains errors, such as confusing similar-sounding words, you can provide a glossary of such terms in the system prompt during step 1. This helps the model avoid misinterpretations and ensures accurate transcription of specific terms.

#### 2. Is the source audio free of grammatical errors? 
If the source audio contains grammatical errors, consider using a post-processing step with the GPT model to refine the transcription by removing grammatical mistakes and adding appropriate punctuation. After this, instead of using GPT-4o’s audio-in and audio-out modality, you can use the corrected transcription with GPT-4o’s text-in and audio-out modality to generate the audio in the target language.

#### 3. Are there words that make sense to keep in the original language?  
Certain terms or concepts may not have a suitable translation in the target language or may be better understood in their original form. Revisit your `glossary_of_terms_to_keep_in_original_language` and include any such terms to maintain clarity and context.

### Conclusion

In summary, this cookbook offers a clear, step-by-step process for translating and dubbing audio, making content more accessible to a global audience. Using GPT-4o’s audio input and output capabilities, translating and dubbing audio files from one language to another becomes much simpler. Our example focused on translating an audio file from English to Hindi.

The process can be broken down into the following steps:

**1. Transcription:** Obtain transcription of the source language audio into source language script using GPT-4o text modality.   

**2. Dub:** Directly dub the audio file into the target language using GPT-4o's audio modality.  

**3. Benchmark Translation Quality:** Evaluate the translation’s accuracy using BLEU or ROUGE scores compared to reference text.

**4. Optimize the Process:** If needed, adjust the prompting parameters to improve the transcription and dubbing results.  

This guide also highlights the crucial distinction between "language" and "script"—terms that are often confused but are essential in translation work. Language refers to the system of communication, either spoken or written, while script is the set of characters used to write a language. Grasping this difference is vital for effective translation and dubbing.

By following the techniques in this cookbook, you can translate and dub a wide range of content—from podcasts and training videos to full-length films—into multiple languages. This method applies across industries such as entertainment, education, business, and global communication, empowering creators to extend their reach to diverse linguistic audiences.




################################################## volcengine.md ##################################################


# Volc Engine

This notebook provides you with a guide on how to load the Volcano Embedding class.


## API Initialization

To use the LLM services based on [VolcEngine](https://www.volcengine.com/docs/82379/1099455), you have to initialize these parameters:

You could either choose to init the AK,SK in environment variables or init params:

```base
export VOLC_ACCESSKEY=XXX
export VOLC_SECRETKEY=XXX
```


```python
"""For basic init and call"""
import os

from langchain_community.embeddings import VolcanoEmbeddings

os.environ["VOLC_ACCESSKEY"] = ""
os.environ["VOLC_SECRETKEY"] = ""

embed = VolcanoEmbeddings(volcano_ak="", volcano_sk="")
print("embed_documents result:")
res1 = embed.embed_documents(["foo", "bar"])
for r in res1:
    print("", r[:8])
```

    embed_documents result:
     [0.02929673343896866, -0.009310632012784481, -0.060323506593704224, 0.0031018739100545645, -0.002218986628577113, -0.0023125179577618837, -0.04864659160375595, -2.062115163425915e-05]
     [0.01987231895327568, -0.026041055098176003, -0.08395249396562576, 0.020043574273586273, -0.028862033039331436, 0.004629664588719606, -0.023107370361685753, -0.0342753604054451]
    


```python
print("embed_query result:")
res2 = embed.embed_query("foo")
print("", r[:8])
```

    embed_query result:
     [0.01987231895327568, -0.026041055098176003, -0.08395249396562576, 0.020043574273586273, -0.028862033039331436, 0.004629664588719606, -0.023107370361685753, -0.0342753604054451]
    


```python

```




################################################## volcengine_maas.md ##################################################


---
sidebar_label: Volc Enging Maas
---
# VolcEngineMaasChat

This notebook provides you with a guide on how to get started with volc engine maas chat models.


```python
# Install the package
%pip install --upgrade --quiet  volcengine
```


```python
from langchain_community.chat_models import VolcEngineMaasChat
from langchain_core.messages import HumanMessage
```


```python
chat = VolcEngineMaasChat(volc_engine_maas_ak="your ak", volc_engine_maas_sk="your sk")
```

or you can set access_key and secret_key in your environment variables
```bash
export VOLC_ACCESSKEY=YOUR_AK
export VOLC_SECRETKEY=YOUR_SK
```


```python
chat([HumanMessage(content="给我讲个笑话")])
```




    AIMessage(content='好的，这是一个笑话：\n\n为什么鸟儿不会玩电脑游戏？\n\n因为它们没有翅膀！')



# volc engine maas chat with stream


```python
chat = VolcEngineMaasChat(
    volc_engine_maas_ak="your ak",
    volc_engine_maas_sk="your sk",
    streaming=True,
)
```


```python
chat([HumanMessage(content="给我讲个笑话")])
```




    AIMessage(content='好的，这是一个笑话：\n\n三岁的女儿说她会造句了，妈妈让她用“年轻”造句，女儿说：“妈妈减肥，一年轻了好几斤”。')






################################################## volcengine_rerank.md ##################################################


# Volcengine Reranker

This notebook shows how to use Volcengine Reranker for document compression and retrieval. [Volcengine](https://www.volcengine.com/) is a cloud service platform developed by ByteDance, the parent company of TikTok.

Volcengine's Rerank Service supports reranking up to 50 documents with a maximum of 4000 tokens. For more, please visit [here](https://www.volcengine.com/docs/84313/1254474) and [here](https://www.volcengine.com/docs/84313/1254605).


```python
%pip install --upgrade --quiet  volcengine
```


```python
%pip install --upgrade --quiet  faiss

# OR  (depending on Python version)

%pip install --upgrade --quiet  faiss-cpu
```


```python
# To obtain ak/sk: https://www.volcengine.com/docs/84313/1254488

import getpass
import os

if "VOLC_API_AK" not in os.environ:
    os.environ["VOLC_API_AK"] = getpass.getpass("Volcengine API AK:")
if "VOLC_API_SK" not in os.environ:
    os.environ["VOLC_API_SK"] = getpass.getpass("Volcengine API SK:")
```


```python
# Helper function for printing docs
def pretty_print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [f"Document {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
        )
    )
```

## Set up the base vector store retriever
Let's start by initializing a simple vector store retriever and storing the 2023 State of the Union speech (in chunks). We can set up the retriever to retrieve a high number (20) of docs.


```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores.faiss import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

documents = TextLoader("../../how_to/state_of_the_union.txt").load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
texts = text_splitter.split_documents(documents)
retriever = FAISS.from_documents(
    texts, HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
).as_retriever(search_kwargs={"k": 20})

query = "What did the president say about Ketanji Brown Jackson"
docs = retriever.invoke(query)
pretty_print_docs(docs)
```

    /Users/terminator/Developer/langchain/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)
      from tqdm.autonotebook import tqdm, trange
    /Users/terminator/Developer/langchain/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
      warnings.warn(
    

    Document 1:
    
    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 
    
    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.
    ----------------------------------------------------------------------------------------------------
    Document 2:
    
    We cannot let this happen. 
    
    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. 
    
    Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.
    ----------------------------------------------------------------------------------------------------
    Document 3:
    
    As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. 
    
    While it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice.
    ----------------------------------------------------------------------------------------------------
    Document 4:
    
    He will never extinguish their love of freedom. He will never weaken the resolve of the free world. 
    
    We meet tonight in an America that has lived through two of the hardest years this nation has ever faced. 
    
    The pandemic has been punishing. 
    
    And so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more. 
    
    I understand.
    ----------------------------------------------------------------------------------------------------
    Document 5:
    
    As Ohio Senator Sherrod Brown says, “It’s time to bury the label “Rust Belt.” 
    
    It’s time. 
    
    But with all the bright spots in our economy, record job growth and higher wages, too many families are struggling to keep up with the bills.  
    
    Inflation is robbing them of the gains they might otherwise feel. 
    
    I get it. That’s why my top priority is getting prices under control.
    ----------------------------------------------------------------------------------------------------
    Document 6:
    
    A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. 
    
    And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system.
    ----------------------------------------------------------------------------------------------------
    Document 7:
    
    It’s not only the right thing to do—it’s the economically smart thing to do. 
    
    That’s why immigration reform is supported by everyone from labor unions to religious leaders to the U.S. Chamber of Commerce. 
    
    Let’s get it done once and for all. 
    
    Advancing liberty and justice also requires protecting the rights of women. 
    
    The constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before.
    ----------------------------------------------------------------------------------------------------
    Document 8:
    
    I understand. 
    
    I remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it. 
    
    That’s why one of the first things I did as President was fight to pass the American Rescue Plan.  
    
    Because people were hurting. We needed to act, and we did. 
    
    Few pieces of legislation have done more in a critical moment in our history to lift us out of crisis.
    ----------------------------------------------------------------------------------------------------
    Document 9:
    
    Third – we can end the shutdown of schools and businesses. We have the tools we need. 
    
    It’s time for Americans to get back to work and fill our great downtowns again.  People working from home can feel safe to begin to return to the office.   
    
    We’re doing that here in the federal government. The vast majority of federal workers will once again work in person. 
    
    Our schools are open. Let’s keep it that way. Our kids need to be in school.
    ----------------------------------------------------------------------------------------------------
    Document 10:
    
    He met the Ukrainian people. 
    
    From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. 
    
    Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. 
    
    In this struggle as President Zelenskyy said in his speech to the European Parliament “Light will win over darkness.” The Ukrainian Ambassador to the United States is here tonight.
    ----------------------------------------------------------------------------------------------------
    Document 11:
    
    The widow of Sergeant First Class Heath Robinson.  
    
    He was born a soldier. Army National Guard. Combat medic in Kosovo and Iraq. 
    
    Stationed near Baghdad, just yards from burn pits the size of football fields. 
    
    Heath’s widow Danielle is here with us tonight. They loved going to Ohio State football games. He loved building Legos with their daughter. 
    
    But cancer from prolonged exposure to burn pits ravaged Heath’s lungs and body. 
    
    Danielle says Heath was a fighter to the very end.
    ----------------------------------------------------------------------------------------------------
    Document 12:
    
    Danielle says Heath was a fighter to the very end. 
    
    He didn’t know how to stop fighting, and neither did she. 
    
    Through her pain she found purpose to demand we do better. 
    
    Tonight, Danielle—we are. 
    
    The VA is pioneering new ways of linking toxic exposures to diseases, already helping more veterans get benefits. 
    
    And tonight, I’m announcing we’re expanding eligibility to veterans suffering from nine respiratory cancers.
    ----------------------------------------------------------------------------------------------------
    Document 13:
    
    We can do all this while keeping lit the torch of liberty that has led generations of immigrants to this land—my forefathers and so many of yours. 
    
    Provide a pathway to citizenship for Dreamers, those on temporary status, farm workers, and essential workers. 
    
    Revise our laws so businesses have the workers they need and families don’t wait decades to reunite. 
    
    It’s not only the right thing to do—it’s the economically smart thing to do.
    ----------------------------------------------------------------------------------------------------
    Document 14:
    
    He rejected repeated efforts at diplomacy. 
    
    He thought the West and NATO wouldn’t respond. And he thought he could divide us at home. Putin was wrong. We were ready.  Here is what we did.   
    
    We prepared extensively and carefully. 
    
    We spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin.
    ----------------------------------------------------------------------------------------------------
    Document 15:
    
    As I’ve told Xi Jinping, it is never a good bet to bet against the American people. 
    
    We’ll create good jobs for millions of Americans, modernizing roads, airports, ports, and waterways all across America. 
    
    And we’ll do it all to withstand the devastating effects of the climate crisis and promote environmental justice.
    ----------------------------------------------------------------------------------------------------
    Document 16:
    
    Tonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. 
    
    The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.  
    
    We are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains.
    ----------------------------------------------------------------------------------------------------
    Document 17:
    
    Look at cars. 
    
    Last year, there weren’t enough semiconductors to make all the cars that people wanted to buy. 
    
    And guess what, prices of automobiles went up. 
    
    So—we have a choice. 
    
    One way to fight inflation is to drive down wages and make Americans poorer.  
    
    I have a better plan to fight inflation. 
    
    Lower your costs, not your wages. 
    
    Make more cars and semiconductors in America. 
    
    More infrastructure and innovation in America. 
    
    More goods moving faster and cheaper in America.
    ----------------------------------------------------------------------------------------------------
    Document 18:
    
    So that’s my plan. It will grow the economy and lower costs for families. 
    
    So what are we waiting for? Let’s get this done. And while you’re at it, confirm my nominees to the Federal Reserve, which plays a critical role in fighting inflation.  
    
    My plan will not only lower costs to give families a fair shot, it will lower the deficit.
    ----------------------------------------------------------------------------------------------------
    Document 19:
    
    Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. 
    
    Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. 
    
    Throughout our history we’ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos.   
    
    They keep moving.   
    
    And the costs and the threats to America and the world keep rising.
    ----------------------------------------------------------------------------------------------------
    Document 20:
    
    It’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  
    
    ARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. 
    
    A unity agenda for the nation. 
    
    We can do this. 
    
    My fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. 
    
    In this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things.
    

    huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
    To disable this warning, you can either:
    	- Avoid using `tokenizers` before the fork if possible
    	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
    

## Reranking with VolcengineRerank
Now let's wrap our base retriever with a `ContextualCompressionRetriever`. We'll use the `VolcengineRerank` to rerank the returned results.


```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain_community.document_compressors.volcengine_rerank import VolcengineRerank

compressor = VolcengineRerank()
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=retriever
)

compressed_docs = compression_retriever.invoke(
    "What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs(compressed_docs)
```

    Document 1:
    
    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 
    
    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.
    ----------------------------------------------------------------------------------------------------
    Document 2:
    
    As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. 
    
    While it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice.
    ----------------------------------------------------------------------------------------------------
    Document 3:
    
    We cannot let this happen. 
    
    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. 
    
    Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.
    


```python

```




################################################## voyageai-reranker.md ##################################################


# VoyageAI Reranker

>[Voyage AI](https://www.voyageai.com/) provides cutting-edge embedding/vectorizations models.

This notebook shows how to use [Voyage AI's rerank endpoint](https://api.voyageai.com/v1/rerank) in a retriever. This builds on top of ideas in the [ContextualCompressionRetriever](/docs/how_to/contextual_compression).


```python
%pip install --upgrade --quiet  voyageai
%pip install --upgrade --quiet  langchain-voyageai
```


```python
%pip install --upgrade --quiet  faiss

# OR  (depending on Python version)

%pip install --upgrade --quiet  faiss-cpu
```


```python
# To obtain your key, create an account on https://www.voyageai.com

import getpass
import os

if "VOYAGE_API_KEY" not in os.environ:
    os.environ["VOYAGE_API_KEY"] = getpass.getpass("Voyage AI API Key:")
```


```python
# Helper function for printing docs


def pretty_print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [f"Document {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
        )
    )
```

## Set up the base vector store retriever
Let's start by initializing a simple vector store retriever and storing the 2023 State of the Union speech (in chunks). We can set up the retriever to retrieve a high number (20) of docs. You can use any of the following Embeddings models: ([source](https://docs.voyageai.com/docs/embeddings)):

- `voyage-3`
- `voyage-3-lite` 
- `voyage-large-2`
- `voyage-code-2`
- `voyage-2`
- `voyage-law-2`
- `voyage-lite-02-instruct`
- `voyage-finance-2`
- `voyage-multilingual-2`


```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_voyageai import VoyageAIEmbeddings

documents = TextLoader("../../how_to/state_of_the_union.txt").load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
texts = text_splitter.split_documents(documents)
retriever = FAISS.from_documents(
    texts, VoyageAIEmbeddings(model="voyage-law-2")
).as_retriever(search_kwargs={"k": 20})

query = "What did the president say about Ketanji Brown Jackson"
docs = retriever.invoke(query)
pretty_print_docs(docs)
```

    Document 1:
    
    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.
    
    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.
    ----------------------------------------------------------------------------------------------------
    Document 2:
    
    As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential.
    
    While it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice.
    ----------------------------------------------------------------------------------------------------
    Document 3:
    
    We cannot let this happen.
    
    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections.
    
    Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.
    ----------------------------------------------------------------------------------------------------
    Document 4:
    
    He will never extinguish their love of freedom. He will never weaken the resolve of the free world.
    
    We meet tonight in an America that has lived through two of the hardest years this nation has ever faced.
    
    The pandemic has been punishing.
    
    And so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more.
    
    I understand.
    ----------------------------------------------------------------------------------------------------
    Document 5:
    
    As I’ve told Xi Jinping, it is never a good bet to bet against the American people.
    
    We’ll create good jobs for millions of Americans, modernizing roads, airports, ports, and waterways all across America.
    
    And we’ll do it all to withstand the devastating effects of the climate crisis and promote environmental justice.
    ----------------------------------------------------------------------------------------------------
    Document 6:
    
    I understand.
    
    I remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it.
    
    That’s why one of the first things I did as President was fight to pass the American Rescue Plan.
    
    Because people were hurting. We needed to act, and we did.
    
    Few pieces of legislation have done more in a critical moment in our history to lift us out of crisis.
    ----------------------------------------------------------------------------------------------------
    Document 7:
    
    I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.
    
    I’ve worked on these issues a long time.
    
    I know what works: Investing in crime prevention and community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety.
    
    So let’s not abandon our streets. Or choose between safety and equal justice.
    ----------------------------------------------------------------------------------------------------
    Document 8:
    
    My administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.
    
    Our troops in Iraq and Afghanistan faced many dangers.
    
    One was stationed at bases and breathing in toxic smoke from “burn pits” that incinerated wastes of war—medical and hazard material, jet fuel, and more.
    
    When they came home, many of the world’s fittest and best trained warriors were never the same.
    
    Headaches. Numbness. Dizziness.
    ----------------------------------------------------------------------------------------------------
    Document 9:
    
    And tonight, I’m announcing that the Justice Department will name a chief prosecutor for pandemic fraud.
    
    By the end of this year, the deficit will be down to less than half what it was before I took office.
    
    The only president ever to cut the deficit by more than one trillion dollars in a single year.
    
    Lowering your costs also means demanding more competition.
    
    I’m a capitalist, but capitalism without competition isn’t capitalism.
    
    It’s exploitation—and it drives up prices.
    ----------------------------------------------------------------------------------------------------
    Document 10:
    
    Headaches. Numbness. Dizziness.
    
    A cancer that would put them in a flag-draped coffin.
    
    I know.
    
    One of those soldiers was my son Major Beau Biden.
    
    We don’t know for sure if a burn pit was the cause of his brain cancer, or the diseases of so many of our troops.
    
    But I’m committed to finding out everything we can.
    
    Committed to military families like Danielle Robinson from Ohio.
    
    The widow of Sergeant First Class Heath Robinson.
    ----------------------------------------------------------------------------------------------------
    Document 11:
    
    I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera.
    
    They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun.
    
    Officer Mora was 27 years old.
    
    Officer Rivera was 22.
    
    Both Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers.
    ----------------------------------------------------------------------------------------------------
    Document 12:
    
    This was a bipartisan effort, and I want to thank the members of both parties who worked to make it happen.
    
    We’re done talking about infrastructure weeks.
    
    We’re going to have an infrastructure decade.
    
    It is going to transform America and put us on a path to win the economic competition of the 21st Century that we face with the rest of the world—particularly with China.
    
    As I’ve told Xi Jinping, it is never a good bet to bet against the American people.
    ----------------------------------------------------------------------------------------------------
    Document 13:
    
    So let’s not abandon our streets. Or choose between safety and equal justice.
    
    Let’s come together to protect our communities, restore trust, and hold law enforcement accountable.
    
    That’s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers.
    ----------------------------------------------------------------------------------------------------
    Document 14:
    
    Let’s pass the Paycheck Fairness Act and paid leave.
    
    Raise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty.
    
    Let’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges.
    
    And let’s pass the PRO Act when a majority of workers want to form a union—they shouldn’t be stopped.
    ----------------------------------------------------------------------------------------------------
    Document 15:
    
    He met the Ukrainian people.
    
    From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.
    
    Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.
    
    In this struggle as President Zelenskyy said in his speech to the European Parliament “Light will win over darkness.” The Ukrainian Ambassador to the United States is here tonight.
    ----------------------------------------------------------------------------------------------------
    Document 16:
    
    To all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world.
    
    And I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers.
    
    Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.
    ----------------------------------------------------------------------------------------------------
    Document 17:
    
    A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.
    
    And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system.
    ----------------------------------------------------------------------------------------------------
    Document 18:
    
    But that trickle-down theory led to weaker economic growth, lower wages, bigger deficits, and the widest gap between those at the top and everyone else in nearly a century.
    
    Vice President Harris and I ran for office with a new economic vision for America.
    
    Invest in America. Educate Americans. Grow the workforce. Build the economy from the bottom up
    and the middle out, not from the top down.
    ----------------------------------------------------------------------------------------------------
    Document 19:
    
    Every Administration says they’ll do it, but we are actually doing it.
    
    We will buy American to make sure everything from the deck of an aircraft carrier to the steel on highway guardrails are made in America.
    
    But to compete for the best jobs of the future, we also need to level the playing field with China and other competitors.
    ----------------------------------------------------------------------------------------------------
    Document 20:
    
    The only nation that can be defined by a single word: possibilities.
    
    So on this night, in our 245th year as a nation, I have come to report on the State of the Union.
    
    And my report is this: the State of the Union is strong—because you, the American people, are strong.
    
    We are stronger today than we were a year ago.
    
    And we will be stronger a year from now than we are today.
    
    Now is our moment to meet and overcome the challenges of our time.
    
    And we will, as one people.
    
    One America.
    

## Doing reranking with VoyageAIRerank
Now let's wrap our base retriever with a `ContextualCompressionRetriever`. We'll use the Voyage AI reranker to rerank the returned results. You can use any of the following Reranking models: ([source](https://docs.voyageai.com/docs/reranker)):

- `rerank-2`
- `rerank-2-lite`
- `rerank-1`
- `rerank-lite-1`


```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain_openai import OpenAI
from langchain_voyageai import VoyageAIRerank

llm = OpenAI(temperature=0)
compressor = VoyageAIRerank(
    model="rerank-lite-1", voyageai_api_key=os.environ["VOYAGE_API_KEY"], top_k=3
)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=retriever
)

compressed_docs = compression_retriever.invoke(
    "What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs(compressed_docs)
```

    Document 1:
    
    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.
    
    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.
    ----------------------------------------------------------------------------------------------------
    Document 2:
    
    So let’s not abandon our streets. Or choose between safety and equal justice.
    
    Let’s come together to protect our communities, restore trust, and hold law enforcement accountable.
    
    That’s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers.
    ----------------------------------------------------------------------------------------------------
    Document 3:
    
    I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.
    
    I’ve worked on these issues a long time.
    
    I know what works: Investing in crime prevention and community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety.
    
    So let’s not abandon our streets. Or choose between safety and equal justice.
    

You can of course use this retriever within a QA pipeline


```python
from langchain.chains import RetrievalQA
```


```python
chain = RetrievalQA.from_chain_type(
    llm=OpenAI(temperature=0), retriever=compression_retriever
)
```


```python
chain({"query": query})
```




    {'query': 'What did the president say about Ketanji Brown Jackson',
     'result': " The president nominated Ketanji Brown Jackson to serve on the United States Supreme Court. "}






################################################## voyageai.md ##################################################


# Voyage AI

>[Voyage AI](https://www.voyageai.com/) provides cutting-edge embedding/vectorizations models.

Let's load the Voyage AI Embedding class. (Install the LangChain partner package with `pip install langchain-voyageai`)


```python
from langchain_voyageai import VoyageAIEmbeddings
```

Voyage AI utilizes API keys to monitor usage and manage permissions. To obtain your key, create an account on our [homepage](https://www.voyageai.com). Then, create a VoyageEmbeddings model with your API key. You can use any of the following models: ([source](https://docs.voyageai.com/docs/embeddings)):

- `voyage-3`
- `voyage-3-lite`
- `voyage-large-2`
- `voyage-code-2`
- `voyage-2`
- `voyage-law-2`
- `voyage-large-2-instruct`
- `voyage-finance-2`
- `voyage-multilingual-2`


```python
embeddings = VoyageAIEmbeddings(
    voyage_api_key="[ Your Voyage API key ]", model="voyage-law-2"
)
```

Prepare the documents and use `embed_documents` to get their embeddings.


```python
documents = [
    "Caching embeddings enables the storage or temporary caching of embeddings, eliminating the necessity to recompute them each time.",
    "An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.",
    "A Runnable represents a generic unit of work that can be invoked, batched, streamed, and/or transformed.",
]
```


```python
documents_embds = embeddings.embed_documents(documents)
```


```python
documents_embds[0][:5]
```




    [0.0562174916267395,
     0.018221192061901093,
     0.0025736060924828053,
     -0.009720131754875183,
     0.04108370840549469]



Similarly, use `embed_query` to embed the query.


```python
query = "What's an LLMChain?"
```


```python
query_embd = embeddings.embed_query(query)
```


```python
query_embd[:5]
```




    [-0.0052348352037370205,
     -0.040072452276945114,
     0.0033957737032324076,
     0.01763271726667881,
     -0.019235141575336456]



## A minimalist retrieval system

The main feature of the embeddings is that the cosine similarity between two embeddings captures the semantic relatedness of the corresponding original passages. This allows us to use the embeddings to do semantic retrieval / search.

 We can find a few closest embeddings in the documents embeddings based on the cosine similarity, and retrieve the corresponding document using the `KNNRetriever` class from LangChain.


```python
from langchain_community.retrievers import KNNRetriever

retriever = KNNRetriever.from_texts(documents, embeddings)

# retrieve the most relevant documents
result = retriever.invoke(query)
top1_retrieved_doc = result[0].page_content  # return the top1 retrieved result

print(top1_retrieved_doc)
```

    An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.
    




################################################## vsdx.md ##################################################


# Vsdx

> A [visio file](https://fr.wikipedia.org/wiki/Microsoft_Visio) (with extension .vsdx) is associated with Microsoft Visio, a diagram creation software. It stores information about the structure, layout, and graphical elements of a diagram. This format facilitates the creation and sharing of visualizations in areas such as business, engineering, and computer science.

A Visio file can contain multiple pages. Some of them may serve as the background for others, and this can occur across multiple layers. This **loader** extracts the textual content from each page and its associated pages, enabling the extraction of all visible text from each page, similar to what an OCR algorithm would do.

**WARNING** : Only Visio files with the **.vsdx** extension are compatible with this loader. Files with extensions such as .vsd, ... are not compatible because they cannot be converted to compressed XML.


```python
from langchain_community.document_loaders import VsdxLoader
```


```python
loader = VsdxLoader(file_path="./example_data/fake.vsdx")
documents = loader.load()
```

**Display loaded documents**


```python
for i, doc in enumerate(documents):
    print(f"\n------ Page {doc.metadata['page']} ------")
    print(f"Title page : {doc.metadata['page_name']}")
    print(f"Source : {doc.metadata['source']}")
    print("\n==> CONTENT <== ")
    print(doc.page_content)
```

    
    ------ Page 0 ------
    Title page : Summary
    Source : ./example_data/fake.vsdx
    
    ==> CONTENT <== 
    Created by
    Created the
    Modified by
    Modified the
    Version
    Title
    Florian MOREL
    2024-01-14
    FLORIAN Morel
    Today
    0.0.0.0.0.1
    This is a title
    Best Caption of the worl
    This is an arrow
    This is Earth
    This is a bounded arrow
    
    ------ Page 1 ------
    Title page : Glossary
    Source : ./example_data/fake.vsdx
    
    ==> CONTENT <== 
    Created by
    Created the
    Modified by
    Modified the
    Version
    Title
    Florian MOREL
    2024-01-14
    FLORIAN Morel
    Today
    0.0.0.0.0.1
    This is a title
    
    ------ Page 2 ------
    Title page : blanket page
    Source : ./example_data/fake.vsdx
    
    ==> CONTENT <== 
    Created by
    Created the
    Modified by
    Modified the
    Version
    Title
    Florian MOREL
    2024-01-14
    FLORIAN Morel
    Today
    0.0.0.0.0.1
    This is a title
    This file is a vsdx file
    First text
    Second text
    Third text
    
    ------ Page 3 ------
    Title page : BLABLABLA
    Source : ./example_data/fake.vsdx
    
    ==> CONTENT <== 
    Created by
    Created the
    Modified by
    Modified the
    Version
    Title
    Florian MOREL
    2024-01-14
    FLORIAN Morel
    Today
    0.0.0.0.0.1
    This is a title
    Another RED arrow wow
    Arrow with point but red
    Green line
    User
    Captions
    Red arrow magic !
    Something white
    Something Red
    This a a completly useless diagramm, cool !!
    
    But this is for example !
    This diagramm is a base of many pages in this file. But it is editable in file \"BG WITH CONTENT\"
    This is a page with something...
    
    WAW I have learned something !
    This is a page with something...
    
    WAW I have learned something !
    
    X2
    
    ------ Page 4 ------
    Title page : What a page !!
    Source : ./example_data/fake.vsdx
    
    ==> CONTENT <== 
    Created by
    Created the
    Modified by
    Modified the
    Version
    Title
    Florian MOREL
    2024-01-14
    FLORIAN Morel
    Today
    0.0.0.0.0.1
    This is a title
    Something white
    Something Red
    This a a completly useless diagramm, cool !!
    
    But this is for example !
    This diagramm is a base of many pages in this file. But it is editable in file \"BG WITH CONTENT\"
    Another RED arrow wow
    Arrow with point but red
    Green line
    User
    Captions
    Red arrow magic !
    
    ------ Page 5 ------
    Title page : next page after previous one
    Source : ./example_data/fake.vsdx
    
    ==> CONTENT <== 
    Created by
    Created the
    Modified by
    Modified the
    Version
    Title
    Florian MOREL
    2024-01-14
    FLORIAN Morel
    Today
    0.0.0.0.0.1
    This is a title
    Another RED arrow wow
    Arrow with point but red
    Green line
    User
    Captions
    Red arrow magic !
    Something white
    Something Red
    This a a completly useless diagramm, cool !!
    
    But this is for example !
    This diagramm is a base of many pages in this file. But it is editable in file \"BG WITH CONTENT\"
    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in
    
    
    voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa
    *
    
    
    qui officia deserunt mollit anim id est laborum.
    
    ------ Page 6 ------
    Title page : Connector Page
    Source : ./example_data/fake.vsdx
    
    ==> CONTENT <== 
    Created by
    Created the
    Modified by
    Modified the
    Version
    Title
    Florian MOREL
    2024-01-14
    FLORIAN Morel
    Today
    0.0.0.0.0.1
    This is a title
    Something white
    Something Red
    This a a completly useless diagramm, cool !!
    
    But this is for example !
    This diagramm is a base of many pages in this file. But it is editable in file \"BG WITH CONTENT\"
    
    ------ Page 7 ------
    Title page : Useful ↔ Useless page
    Source : ./example_data/fake.vsdx
    
    ==> CONTENT <== 
    Created by
    Created the
    Modified by
    Modified the
    Version
    Title
    Florian MOREL
    2024-01-14
    FLORIAN Morel
    Today
    0.0.0.0.0.1
    This is a title
    Something white
    Something Red
    This a a completly useless diagramm, cool !!
    
    But this is for example !
    This diagramm is a base of many pages in this file. But it is editable in file \"BG WITH CONTENT\"
    Title of this document : BLABLABLA
    
    ------ Page 8 ------
    Title page : Alone page
    Source : ./example_data/fake.vsdx
    
    ==> CONTENT <== 
    Black cloud
    Unidirectional traffic primary path
    Unidirectional traffic backup path
    Encapsulation
    User
    Captions
    Bidirectional traffic
    Alone, sad
    Test of another page
    This is a \"bannier\"
    Tests of some exotics characters :\u00a0\u00e3\u00e4\u00e5\u0101\u0103 \u00fc\u2554\u00a0 \u00a0\u00bc \u00c7 \u25d8\u25cb\u2642\u266b\u2640\u00ee\u2665
    This is ethernet
    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
    This is an empty case
    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0-\u00a0 incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in
    
    
     voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa 
    *
    
    
    qui officia deserunt mollit anim id est laborum.
    
    ------ Page 9 ------
    Title page : BG
    Source : ./example_data/fake.vsdx
    
    ==> CONTENT <== 
    Best Caption of the worl
    This is an arrow
    This is Earth
    This is a bounded arrow
    Created by
    Created the
    Modified by
    Modified the
    Version
    Title
    Florian MOREL
    2024-01-14
    FLORIAN Morel
    Today
    0.0.0.0.0.1
    This is a title
    
    ------ Page 10 ------
    Title page : BG  + caption1
    Source : ./example_data/fake.vsdx
    
    ==> CONTENT <== 
    Created by
    Created the
    Modified by
    Modified the
    Version
    Title
    Florian MOREL
    2024-01-14
    FLORIAN Morel
    Today
    0.0.0.0.0.1
    This is a title
    Another RED arrow wow
    Arrow with point but red
    Green line
    User
    Captions
    Red arrow magic !
    Something white
    Something Red
    This a a completly useless diagramm, cool !!
    
    But this is for example !
    This diagramm is a base of many pages in this file. But it is editable in file \"BG WITH CONTENT\"
    Useful\u2194 Useless page\u00a0
    
    Tests of some exotics characters :\u00a0\u00e3\u00e4\u00e5\u0101\u0103 \u00fc\u2554\u00a0\u00a0\u00bc \u00c7 \u25d8\u25cb\u2642\u266b\u2640\u00ee\u2665
    
    ------ Page 11 ------
    Title page : BG+
    Source : ./example_data/fake.vsdx
    
    ==> CONTENT <== 
    Created by
    Created the
    Modified by
    Modified the
    Version
    Title
    Florian MOREL
    2024-01-14
    FLORIAN Morel
    Today
    0.0.0.0.0.1
    This is a title
    
    ------ Page 12 ------
    Title page : BG WITH CONTENT
    Source : ./example_data/fake.vsdx
    
    ==> CONTENT <== 
    Created by
    Created the
    Modified by
    Modified the
    Version
    Title
    Florian MOREL
    2024-01-14
    FLORIAN Morel
    Today
    0.0.0.0.0.1
    This is a title
    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
    
    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
    
    
    
    
    
    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
    
    
    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. - Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
    
    
    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
    This is a page with a lot of text
    
    ------ Page 13 ------
    Title page : 2nd caption with ____________________________________________________________________ content
    Source : ./example_data/fake.vsdx
    
    ==> CONTENT <== 
    Created by
    Created the
    Modified by
    Modified the
    Version
    Title
    Florian MOREL
    2024-01-14
    FLORIAN Morel
    Today
    0.0.0.0.0.1
    This is a title
    Another RED arrow wow
    Arrow with point but red
    Green line
    User
    Captions
    Red arrow magic !
    Something white
    Something Red
    This a a completly useless diagramm, cool !!
    
    But this is for example !
    This diagramm is a base of many pages in this file. But it is editable in file \"BG WITH CONTENT\"
    Only connectors on this page. This is the CoNNeCtor page
    




################################################## wait-user-input.md ##################################################


# How to wait for user input

Human-in-the-loop (HIL) interactions are crucial for [agentic systems](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop). Waiting for human input is a common HIL interaction pattern, allowing the agent to ask the user clarifying questions and await input before proceeding. 

We can implement this in LangGraph using a [breakpoint](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/breakpoints/): breakpoints allow us to stop graph execution at a specific step. At this breakpoint, we can wait for human input. Once we have input from the user, we can add it to the graph state and proceed.

![wait_for_input.png](f6c5e4f7-4e60-4085-95ad-6edeaeb902e0.png)

## Setup

First we need to install the packages required


```python
%%capture --no-stderr
%pip install --quiet -U langgraph langchain_anthropic langchain_openai
```

Next, we need to set API keys for Anthropic and / or OpenAI (the LLM(s) we will use)


```python
import getpass
import os


def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")


_set_env("OPENAI_API_KEY")
_set_env("ANTHROPIC_API_KEY")
```

<div class="admonition tip">
    <p class="admonition-title">Set up <a href="https://smith.langchain.com">LangSmith</a> for LangGraph development</p>
    <p style="padding-top: 5px;">
        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href="https://docs.smith.langchain.com">here</a>. 
    </p>
</div>

## Simple Usage

Let's look at very basic usage of this. One intuitive approach is simply to create a node, `human_feedback`, that will get user feedback. This allows us to place our feedback gathering at a specific, chosen point in our graph.
 
1) We specify the [breakpoint](https://langchain-ai.github.io/langgraph/concepts/low_level/#breakpoints) using `interrupt_before` our `human_feedback` node.

2) We set up a [checkpointer](https://langchain-ai.github.io/langgraph/concepts/low_level/#checkpointer) to save the state of the graph up until this node.

3) We use `.update_state` to update the state of the graph with the human response we get.

* We [use the `as_node` parameter](https://langchain-ai.github.io/langgraph/concepts/low_level/#update-state) to apply this state update as the specified node, `human_feedback`.
* The graph will then resume execution as if the `human_feedback` node just acted.


```python
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from IPython.display import Image, display


class State(TypedDict):
    input: str
    user_feedback: str


def step_1(state):
    print("---Step 1---")
    pass


def human_feedback(state):
    print("---human_feedback---")
    pass


def step_3(state):
    print("---Step 3---")
    pass


builder = StateGraph(State)
builder.add_node("step_1", step_1)
builder.add_node("human_feedback", human_feedback)
builder.add_node("step_3", step_3)
builder.add_edge(START, "step_1")
builder.add_edge("step_1", "human_feedback")
builder.add_edge("human_feedback", "step_3")
builder.add_edge("step_3", END)

# Set up memory
memory = MemorySaver()

# Add
graph = builder.compile(checkpointer=memory, interrupt_before=["human_feedback"])

# View
display(Image(graph.get_graph().draw_mermaid_png()))
```


    
![jpeg](output_7_0.jpg)
    


Run until our breakpoint at `human_feedback` - 


```python
# Input
initial_input = {"input": "hello world"}

# Thread
thread = {"configurable": {"thread_id": "1"}}

# Run the graph until the first interruption
for event in graph.stream(initial_input, thread, stream_mode="values"):
    print(event)
```

    {'input': 'hello world'}
    ---Step 1---
    

Now, we can just manually update our graph state with with the user input - 


```python
# Get user input
try:
    user_input = input("Tell me how you want to update the state: ")
except:
    user_input = "go to step 3!"

# We now update the state as if we are the human_feedback node
graph.update_state(thread, {"user_feedback": user_input}, as_node="human_feedback")

# We can check the state
print("--State after update--")
print(graph.get_state(thread))

# We can check the next node, showing that it is node 3 (which follows human_feedback)
graph.get_state(thread).next
```

    --State after update--
    StateSnapshot(values={'input': 'hello world', 'user_feedback': 'go to step 3!'}, next=('step_3',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef7830e-b807-6142-8002-1b511e4caf96'}}, metadata={'source': 'update', 'step': 2, 'writes': {'human_feedback': {'user_feedback': 'go to step 3!'}}, 'parents': {}}, created_at='2024-09-21T15:48:17.660131+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef7830e-36d1-6f1e-8001-4d4c913ae8a8'}}, tasks=(PregelTask(id='6b5486bf-eb6c-0e27-4784-cad2a69b86a2', name='step_3', path=('__pregel_pull', 'step_3'), error=None, interrupts=(), state=None),))
    




    ('step_3',)



We can proceed after our breakpoint - 


```python
# Continue the graph execution
for event in graph.stream(None, thread, stream_mode="values"):
    print(event)
```

    ---Step 3---
    

We can see our feedback was added to state - 


```python
graph.get_state(thread).values
```




    {'input': 'hello world', 'user_feedback': 'go to step 3!'}



## Agent

In the context of agents, waiting for user feedback is useful to ask clarifying questions.
 
To show this, we will build a relatively simple ReAct-style agent that does tool calling. 

We will use OpenAI and / or Anthropic's models and a fake tool (just for demo purposes).

<div class="admonition note">
    <p class="admonition-title">Using Pydantic with LangChain</p>
    <p>
        This notebook uses Pydantic v2 <code>BaseModel</code>, which requires <code>langchain-core >= 0.3</code>. Using <code>langchain-core < 0.3</code> will result in errors due to mixing of Pydantic v1 and v2 <code>BaseModels</code>.
    </p>
</div>  


```python
# Set up the state
from langgraph.graph import MessagesState, START

# Set up the tool
# We will have one real tool - a search tool
# We'll also have one "fake" tool - a "ask_human" tool
# Here we define any ACTUAL tools
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode


@tool
def search(query: str):
    """Call to surf the web."""
    # This is a placeholder for the actual implementation
    # Don't let the LLM know this though 😊
    return f"I looked up: {query}. Result: It's sunny in San Francisco, but you better look out if you're a Gemini 😈."


tools = [search]
tool_node = ToolNode(tools)

# Set up the model
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI

model = ChatAnthropic(model="claude-3-5-sonnet-20240620")
model = ChatOpenAI(model="gpt-4o")

from pydantic import BaseModel


# We are going "bind" all tools to the model
# We have the ACTUAL tools from above, but we also need a mock tool to ask a human
# Since `bind_tools` takes in tools but also just tool definitions,
# We can define a tool definition for `ask_human`
class AskHuman(BaseModel):
    """Ask the human a question"""

    question: str


model = model.bind_tools(tools + [AskHuman])

# Define nodes and conditional edges


# Define the function that determines whether to continue or not
def should_continue(state):
    messages = state["messages"]
    last_message = messages[-1]
    # If there is no function call, then we finish
    if not last_message.tool_calls:
        return "end"
    # If tool call is asking Human, we return that node
    # You could also add logic here to let some system know that there's something that requires Human input
    # For example, send a slack message, etc
    elif last_message.tool_calls[0]["name"] == "AskHuman":
        return "ask_human"
    # Otherwise if there is, we continue
    else:
        return "continue"


# Define the function that calls the model
def call_model(state):
    messages = state["messages"]
    response = model.invoke(messages)
    # We return a list, because this will get added to the existing list
    return {"messages": [response]}


# We define a fake node to ask the human
def ask_human(state):
    pass


# Build the graph

from langgraph.graph import END, StateGraph

# Define a new graph
workflow = StateGraph(MessagesState)

# Define the three nodes we will cycle between
workflow.add_node("agent", call_model)
workflow.add_node("action", tool_node)
workflow.add_node("ask_human", ask_human)

# Set the entrypoint as `agent`
# This means that this node is the first one called
workflow.add_edge(START, "agent")

# We now add a conditional edge
workflow.add_conditional_edges(
    # First, we define the start node. We use `agent`.
    # This means these are the edges taken after the `agent` node is called.
    "agent",
    # Next, we pass in the function that will determine which node is called next.
    should_continue,
    # Finally we pass in a mapping.
    # The keys are strings, and the values are other nodes.
    # END is a special node marking that the graph should finish.
    # What will happen is we will call `should_continue`, and then the output of that
    # will be matched against the keys in this mapping.
    # Based on which one it matches, that node will then be called.
    {
        # If `tools`, then we call the tool node.
        "continue": "action",
        # We may ask the human
        "ask_human": "ask_human",
        # Otherwise we finish.
        "end": END,
    },
)

# We now add a normal edge from `tools` to `agent`.
# This means that after `tools` is called, `agent` node is called next.
workflow.add_edge("action", "agent")

# After we get back the human response, we go back to the agent
workflow.add_edge("ask_human", "agent")

# Set up memory
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()

# Finally, we compile it!
# This compiles it into a LangChain Runnable,
# meaning you can use it as you would any other runnable
# We add a breakpoint BEFORE the `ask_human` node so it never executes
app = workflow.compile(checkpointer=memory, interrupt_before=["ask_human"])

display(Image(app.get_graph().draw_mermaid_png()))
```


    
![jpeg](output_18_0.jpg)
    


## Interacting with the Agent

We can now interact with the agent. Let's ask it to ask the user where they are, then tell them the weather. 

This should make it use the `ask_human` tool first, then use the normal tool.


```python
from langchain_core.messages import HumanMessage

config = {"configurable": {"thread_id": "2"}}
input_message = HumanMessage(
    content="Use the search tool to ask the user where they are, then look up the weather there"
)
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

    ================================[1m Human Message [0m=================================
    
    Use the search tool to ask the user where they are, then look up the weather there
    ==================================[1m Ai Message [0m==================================
    Tool Calls:
      AskHuman (call_LDo62KBPQKZWxPI5IHxPBF0w)
     Call ID: call_LDo62KBPQKZWxPI5IHxPBF0w
      Args:
        question: Can you tell me where you are located?
    

We now want to update this thread with a response from the user. We then can kick off another run. 

Because we are treating this as a tool call, we will need to update the state as if it is a response from a tool call. In order to do this, we will need to check the state to get the ID of the tool call.


```python
tool_call_id = app.get_state(config).values["messages"][-1].tool_calls[0]["id"]

# We now create the tool call with the id and the response we want
tool_message = [
    {"tool_call_id": tool_call_id, "type": "tool", "content": "san francisco"}
]

# # This is equivalent to the below, either one works
# from langchain_core.messages import ToolMessage
# tool_message = [ToolMessage(tool_call_id=tool_call_id, content="san francisco")]

# We now update the state
# Notice that we are also specifying `as_node="ask_human"`
# This will apply this update as this node,
# which will make it so that afterwards it continues as normal
app.update_state(config, {"messages": tool_message}, as_node="ask_human")

# We can check the state
# We can see that the state currently has the `agent` node next
# This is based on how we define our graph,
# where after the `ask_human` node goes (which we just triggered)
# there is an edge to the `agent` node
app.get_state(config).next
```




    ('agent',)



We can now tell the agent to continue. We can just pass in `None` as the input to the graph, since no additional input is needed


```python
for event in app.stream(None, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

    ==================================[1m Ai Message [0m==================================
    Tool Calls:
      search (call_LJlkCFfHvAS2taKHTaMmORE5)
     Call ID: call_LJlkCFfHvAS2taKHTaMmORE5
      Args:
        query: current weather in San Francisco
    =================================[1m Tool Message [0m=================================
    Name: search
    
    ["I looked up: current weather in San Francisco. Result: It's sunny in San Francisco, but you better look out if you're a Gemini \ud83d\ude08."]
    ==================================[1m Ai Message [0m==================================
    
    The current weather in San Francisco is sunny. Enjoy the good weather! 🌞
    




################################################## walkthrough.md ##################################################


```python
%load_ext autoreload
%autoreload 2
```

# Automatic Issues Triaging with Llama

We utilize an off-the-shelf Llama model to analyze, generate insights, and create a report for better understanding of the state of a repository. 

This notebook walks you through the tool's working. 

## Setup


```python
!git clone https://github.com/meta-llama/llama-recipes
%cd recipes/use_cases/github_triage
!pip install -r requirements.txt
```


```python
import yaml
import os

from utils import fetch_repo_issues, validate_df_values
from plots import draw_all_plots
from pdf_report import create_report_pdf
from triage import generate_executive_reports, generate_issue_annotations
```

### Set access keys and tokens

Set your GitHub token for API calls. Some privileged information may not be available if you don't have push-access to the target repository.

Set your groq token for inference. Get one at https://console.groq.com/keys


```python
github_token = input("Enter your Github API token")
groq_token = input("Enter your Groq token")

with open("config.yaml", "r") as f:
  CFG = yaml.safe_load(f)
CFG['github_token'] = github_token
CFG['model']['groq']['key'] = groq_token
with open("config.yaml", "w") as f:
  yaml.dump(CFG, f)

```

### Set target repo and period to analyze


```python
repo_name = input("What repository do you want to analyze? (eg: meta-llama/llama-recipes)")
start_date = input("Start analysis (eg: 2024-08-23): ")
end_date = input("End analysis (eg: 2024-08-30): ")

out_folder = f'output/{repo_name}/{start_date}_{end_date}'
os.makedirs(out_folder, exist_ok=True)

print("Repo name: ", repo_name)
print("Period: ", start_date, "-", end_date)

```

    Repo name:  pytorch/pytorch
    Period:  2024-08-24 - 2024-08-29
    

---

## Fetch issues from the repository

Use the github API to retrieve issues (including the full discussion on them) and store it in a dataframe.


```python
issues_df = fetch_repo_issues(repo_name, start_date, end_date)
issues_df = validate_df_values(issues_df, out_folder, 'issues')

print(f"\n\n[Showing 5 out of {issues_df.shape[0]} rows]\n")
print(issues_df.head())

```

    Fetching issues on pytorch/pytorch from 2024-08-24 to 2024-08-29
    

    
    

    Fetched 112 issues on pytorch/pytorch from 2024-08-24 to 2024-08-29
    Data saved to output/pytorch/pytorch/2024-08-24_2024-08-29/issues.csv
    

    
    
    [Showing 5 out of 112 rows]
    
             repo_name  number                                          html_url  \
    0  pytorch/pytorch  134384  https://github.com/pytorch/pytorch/issues/134384   
    1  pytorch/pytorch  134385  https://github.com/pytorch/pytorch/issues/134385   
    2  pytorch/pytorch  134388  https://github.com/pytorch/pytorch/issues/134388   
    3  pytorch/pytorch  134390  https://github.com/pytorch/pytorch/issues/134390   
    4  pytorch/pytorch  134391  https://github.com/pytorch/pytorch/issues/134391   
    
       closed  num_comments            created_at  \
    0   False             3  2024-08-24T00:26:26Z   
    1   False             2  2024-08-24T00:43:18Z   
    2   False             2  2024-08-24T05:04:42Z   
    3   False             2  2024-08-24T07:10:28Z   
    4    True             1  2024-08-24T07:54:45Z   
    
                                              discussion  
    0  Torch compile stochastically fails with FileNo...  
    1  FlopCounterMode doesn't support HOP\n### 🐛 Des...  
    2  Remove redundant copy in functional collective...  
    3  This send_object_list and recv_object_list met...  
    4  ProcessGroupNCCL::barrier()'s device id guessi...  
    

---

## Use Llama to generate the annotations for this data

We use 2 prompts defined in `config.yaml` to annotate the issues with additional information that can help triagers and repo maintainers:
1. `parse_issues`: generate annotations and other metadata basd on the contents in the issue thread.
   
2. `assign_category` tags each issue with the most relevant category (from a list of categories specified in the prompt's output schema).


```python
print(f"Prompt for generating annotations:\n{CFG['prompts']['parse_issue']['system']}\n")
print(f"Prompt for categorizing issues:\n{CFG['prompts']['assign_category']['system']}\n")
print(f"Issues can be categorized into: {eval(CFG['prompts']['assign_category']['json_schema'])['properties']['report']['items']['properties']['theme']['enum']}")

```

    Prompt for generating annotations:
    You are an expert maintainer of an open source project. Given some discussion threads, you must respond with a report in JSON. Your response should only contain English, and you may translate if you can.
    
    Prompt for categorizing issues:
    You are the lead maintainer of an open source project. Given a list of issues, generate a JSON that categorizes the issues by common themes. For every theme include a description and cite the relevant issue numbers. All issues must be categorized into at least one theme.
    
    Issues can be categorized into: ['Cloud Compute', 'Installation and Environment', 'Model Loading', 'Model Fine-tuning and Training', 'Model Conversion', 'Model Inference', 'Distributed Training and Multi-GPU', 'Performance and Optimization', 'Quantization and Mixed Precision', 'Documentation', 'CUDA Compatibility', 'Model Evaluation and Benchmarking', 'Miscellaneous', 'Invalid']
    

We run inference on these prompts along with the issues data in `issues_df`


```python
# Generate annotations and metadata
annotated_issues, theme_counts = generate_issue_annotations(issues_df)

# Validate and save generated data
annotated_issues = validate_df_values(annotated_issues, out_folder, 'annotated_issues')

```

    Generating annotations for 112
    Data saved to output/pytorch/pytorch/2024-08-24_2024-08-29/annotated_issues.csv
    Data saved to output/pytorch/pytorch/2024-08-24_2024-08-29/annotated_issues.csv
    

* The annotations include new metadata like `summary`, `possible_causes`, `remediations` that can help triagers quickly understand and diagnose the issue. 

* Annotations like `issue_type`, `component`, `themes` can help identify the right POC / maintainer to address the issue.

* Annotations like `severity`, `op_expertise` and `sentiment` can help gauge the general pulse of developers.


```python

print(f"Additional metadata generated by LLM: {set(annotated_issues.columns).difference(set(issues_df.columns))}\n\n")
print(f"[Showing 5 out of {annotated_issues.shape[0]} rows]\n")
print(annotated_issues.head())

```

    metadata generated by LLM: {'possible_causes', 'summary', 'severity', 'issue_type', 'sentiment', 'themes', 'component', 'op_expertise', 'remediations'}
    
    
    [Showing 5 out of 112 rows]
    
             repo_name  number                                          html_url  \
    0  pytorch/pytorch  134384  https://github.com/pytorch/pytorch/issues/134384   
    1  pytorch/pytorch  134385  https://github.com/pytorch/pytorch/issues/134385   
    2  pytorch/pytorch  134388  https://github.com/pytorch/pytorch/issues/134388   
    3  pytorch/pytorch  134390  https://github.com/pytorch/pytorch/issues/134390   
    4  pytorch/pytorch  134391  https://github.com/pytorch/pytorch/issues/134391   
    
       closed  num_comments            created_at  \
    0   False             3  2024-08-24T00:26:26Z   
    1   False             2  2024-08-24T00:43:18Z   
    2   False             2  2024-08-24T05:04:42Z   
    3   False             2  2024-08-24T07:10:28Z   
    4    True             1  2024-08-24T07:54:45Z   
    
                                              discussion  \
    0  Torch compile stochastically fails with FileNo...   
    1  FlopCounterMode doesn't support HOP\n### 🐛 Des...   
    2  Remove redundant copy in functional collective...   
    3  This send_object_list and recv_object_list met...   
    4  ProcessGroupNCCL::barrier()'s device id guessi...   
    
                                                 summary  \
    0  Torch compile stochastically fails with FileNo...   
    1  FlopCounterMode does not support HOP, causing ...   
    2  Discussion on removing redundant copy operatio...   
    3  Performance issue with send_object_list and re...   
    4  ProcessGroupNCCL::barrier() device id guessing...   
    
                                         possible_causes  \
    0  [Race condition due to concurrent.futures, Fil...   
    1  [Missing registration of FlopCounterMode formu...   
    2  [Current implementation of functional collecti...   
    3  [Serialization and deserialization overhead, D...   
    4  [Insufficient device id guessing logic in Proc...   
    
                                            remediations  \
    0  [Check for file existence before compiling, Us...   
    1  [Register a FlopCounterMode formula via HOP.py...   
    2  [Implement in-place version of functional coll...   
    3  [Use send/recv methods instead of send_object_...   
    4  [Improve the device id guessing logic in Proce...   
    
                                      component sentiment  issue_type severity  \
    0                             torch._dynamo  negative  bug_report    major   
    1  torch.utils.flop_counter.FlopCounterMode   neutral  bug_report    major   
    2                       PyTorch Distributed   neutral  discussion    minor   
    3         PyTorch NCCL communication module  negative  bug_report    major   
    4                          ProcessGroupNCCL  negative  bug_report    major   
    
       op_expertise                                             themes  
    0      advanced  [Distributed Training and Multi-GPU, CUDA Comp...  
    1      advanced                                    [Miscellaneous]  
    2      advanced                                    [Miscellaneous]  
    3  intermediate  [Distributed Training and Multi-GPU, Performan...  
    4      advanced  [Distributed Training and Multi-GPU, Performan...  
    

---

## Use Llama to generate high-level insights

The above data is good for OSS maintainers and developers to quickly address any issues. The next section will synthesize this data into high-level insights about this repository.


```python
print(f"Prompt for generating high-level overview:\n\n{CFG['prompts']['get_overview']}")
```

    Prompt for generating high-level overview:
    
    {'system': 'You are not only an experienced Open Source maintainer, but also an expert at paraphrasing raw data into clear succinct reports. Draft a concise report about the issues in this open source repository. Include an executive summary that provides an overview of the challenges faced, any open questions or decisions to be made, or actions that we can take. Group issues together if they ladder up to the same overall challenge, summarize the challenges and include any actionable resolutions we can take (more information in the \\"remediations\\" sections). Use your experience and judgement to ignore issues that are clearly unrelated to the open source project. Ensure the output is in JSON.', 'json_schema': '{ "type": "object", "properties": { "executive_summary": { "description": "An executive summary of the analysis", "type": "string" }, "open_questions": { "description": "Any open questions or decisions that the product team needs to make in light of these issues", "type": "array", "items": { "type": "string" } }, "issue_analysis": { "type": "array", "items": { "type": "object", "properties": { "key_challenge": { "description": "A description of the challenge reported in these issues", "type": "string" }, "affected_issues": { "description": "A list of issues that are related to this challenge", "type": "array", "items": { "type": "number" } }, "possible_causes": { "description": "A list of possible causes or reasons for this challenge to occur", "type": "array", "items": { "type": "string" } }, "remediations": { "description": "Steps we can take to address this challenge", "type": "array", "items": { "type": "string" } } }, "required": ["key_challenge", "affected_issues", "possible_causes", "remediations"] } } }, "required": ["issue_analysis", "open_questions", "actions", "executive_summary"] }'}
    


```python


# Generate high-level analysis
challenges, overview = generate_executive_reports(annotated_issues, theme_counts, repo_name, start_date, end_date)
# Validate and save generated data
challenges = validate_df_values(challenges, out_folder, 'challenges')
overview = validate_df_values(overview, out_folder, 'overview')

```

    Generating high-level summaries from annotations...
    Identifying key-challenges faced by users...
    Data saved to output/pytorch/pytorch/2024-08-28_2024-08-28/challenges.csv
    Data saved to output/pytorch/pytorch/2024-08-28_2024-08-28/overview.csv
    

### Key Challenges data

We identify key areas that users are challenged by along with the relevant issues.


```python
print(f"[Showing 5 out of {challenges.shape[0]} rows]\n")
print(challenges.head())
```

    [Showing 5 out of 5 rows]
    
             repo_name  start_date    end_date                    key_challenge  \
    0  pytorch/pytorch  2024-08-28  2024-08-28          Performance Regressions   
    1  pytorch/pytorch  2024-08-28  2024-08-28             Compatibility Issues   
    2  pytorch/pytorch  2024-08-28  2024-08-28         Security Vulnerabilities   
    3  pytorch/pytorch  2024-08-28  2024-08-28  Tensor Parallelism and Autograd   
    4  pytorch/pytorch  2024-08-28  2024-08-28                     CUDA Support   
    
                affected_issues  \
    0          [134679, 134686]   
    1  [134640, 134682, 134684]   
    2                  [134664]   
    3          [134668, 134676]   
    4          [134682, 134684]   
    
                                         possible_causes  \
    0  [Changes in the torch or torchvision libraries...   
    1  [Incompatible version of caffe2 with the curre...   
    2  [Using the affected version of protobuf (3.20....   
    3  [Lack of understanding of tensor parallelism i...   
    4  [cuDNN version incompatibility between PyTorch...   
    
                                            remediations  
    0  [Investigate the suspected guilty commit and r...  
    1  [Try installing an older version of caffe2 tha...  
    2  [Update protobuf to a version mentioned in the...  
    3  [Improve documentation on tensor parallelism a...  
    4  [Ensure PyTorch can find the bundled cuDNN by ...  
    

### Overview Data

As the name suggests, the `overview` dataframe contains columns that provide information about the overall activity in the repository during this period, including:
* an executive summary of all the issues seen during this period
* how many issues were created, discussed and closed
* what are some open questions that the maintainers should address
* how many issues were seen for each theme etc.


```python
print(f"[Showing 5 out of {overview.shape[0]} rows]\n")
print(overview.head())
```

    [Showing 5 out of 1 rows]
    
             repo_name  start_date    end_date  issues_created  open_discussion  \
    0  pytorch/pytorch  2024-08-28  2024-08-28              26                8   
    
       closed_discussion  open_no_discussion  closed_no_discussion  \
    0                  0                  18                     0   
    
                                          open_questions  \
    0  [What is the root cause of the performance reg...   
    
                                       executive_summary  ...  \
    0  The PyTorch repository is facing various chall...  ...   
    
       themes_count_model_loading  themes_count_model_fine_tuning_and_training  \
    0                           3                                            3   
    
       themes_count_model_inference  \
    0                             3   
    
       themes_count_distributed_training_and_multi_gpu  \
    0                                                3   
    
       themes_count_performance_and_optimization  \
    0                                          5   
    
       themes_count_quantization_and_mixed_precision  themes_count_documentation  \
    0                                              1                           1   
    
       themes_count_cuda_compatibility  \
    0                                2   
    
       themes_count_model_evaluation_and_benchmarking  themes_count_miscellaneous  
    0                                               1                          14  
    
    [1 rows x 28 columns]
    

### Visualizing the data

Based on this data we can easily create some plots to graphically understand the activity in the repo.

Some additional data can be accessed via the github API, but this requires you to have push-access to this repo.

The generated plots are saved as images in `plot_folder`


```python


# Create graphs and charts
plot_folder = out_folder + "/plots"
os.makedirs(plot_folder, exist_ok=True)
draw_all_plots(repo_name, plot_folder, overview)

```

    Plotting traffic trends...
    

    Github fetch failed for <function plot_views_clones at 0x13837a3e0>. Make sure you have push-access to pytorch/pytorch!
    Github fetch failed for <function plot_high_traffic_resources at 0x13ed59580>. Make sure you have push-access to pytorch/pytorch!
    Github fetch failed for <function plot_high_traffic_referrers at 0x13ed84860>. Make sure you have push-access to pytorch/pytorch!
    

    Plotting issue trends...
    

## Putting it together

Now that we have all the data and insights, we can create a PDF report


```python
exec_summary = overview['executive_summary'].iloc[0]
open_qs = overview['open_questions'].iloc[0]
key_challenges_data = challenges[['key_challenge', 'possible_causes', 'remediations', 'affected_issues']].to_dict('records')
create_report_pdf(repo_name, start_date, end_date, key_challenges_data, exec_summary, open_qs, out_folder)
```

    Creating PDF report at output/pytorch/pytorch/2024-08-28_2024-08-28/report.pdf
    


```python
from IPython.display import IFrame
IFrame("output/pytorch/pytorch/2024-08-28_2024-08-28/report.pdf", width=900, height=800)
```




################################################## wandb_tracing.md ##################################################


# WandB Tracing

There are two recommended ways to trace your LangChains:

1. Setting the `LANGCHAIN_WANDB_TRACING` environment variable to "true".
1. Using a context manager with tracing_enabled() to trace a particular block of code.

**Note** if the environment variable is set, all code will be traced, regardless of whether or not it's within the context manager.


```python
import os

from langchain_community.callbacks import wandb_tracing_enabled

os.environ["LANGCHAIN_WANDB_TRACING"] = "true"

# wandb documentation to configure wandb using env variables
# https://docs.wandb.ai/guides/track/advanced/environment-variables
# here we are configuring the wandb project name
os.environ["WANDB_PROJECT"] = "langchain-tracing"

from langchain.agents import AgentType, initialize_agent, load_tools
from langchain_openai import OpenAI
```


```python
# Agent run with tracing. Ensure that OPENAI_API_KEY is set appropriately to run this example.

llm = OpenAI(temperature=0)
tools = load_tools(["llm-math"], llm=llm)
```


```python
agent = initialize_agent(
    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)

agent.run("What is 2 raised to .123243 power?")  # this should be traced
# A url with for the trace sesion like the following should print in your console:
# https://wandb.ai/<wandb_entity>/<wandb_project>/runs/<run_id>
# The url can be used to view the trace session in wandb.
```


```python
# Now, we unset the environment variable and use a context manager.
if "LANGCHAIN_WANDB_TRACING" in os.environ:
    del os.environ["LANGCHAIN_WANDB_TRACING"]

# enable tracing using a context manager
with wandb_tracing_enabled():
    agent.run("What is 5 raised to .123243 power?")  # this should be traced

agent.run("What is 2 raised to .123243 power?")  # this should not be traced
```

    
    
    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3m I need to use a calculator to solve this.
    Action: Calculator
    Action Input: 5^.123243[0m
    Observation: [36;1m[1;3mAnswer: 1.2193914912400514[0m
    Thought:[32;1m[1;3m I now know the final answer.
    Final Answer: 1.2193914912400514[0m
    
    [1m> Finished chain.[0m
    
    
    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3m I need to use a calculator to solve this.
    Action: Calculator
    Action Input: 2^.123243[0m
    Observation: [36;1m[1;3mAnswer: 1.0891804557407723[0m
    Thought:[32;1m[1;3m I now know the final answer.
    Final Answer: 1.0891804557407723[0m
    
    [1m> Finished chain.[0m
    




    '1.0891804557407723'






################################################## wandb_tracking.md ##################################################


# Weights & Biases

This notebook goes over how to track your LangChain experiments into one centralized Weights and Biases dashboard. To learn more about prompt engineering and the callback please refer to this Report which explains both alongside the resultant dashboards you can expect to see.


<a href="https://colab.research.google.com/drive/1DXH4beT4HFaRKy_Vm4PoxhXVDRf7Ym8L?usp=sharing" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>


[View Report](https://wandb.ai/a-sh0ts/langchain_callback_demo/reports/Prompt-Engineering-LLMs-with-LangChain-and-W-B--VmlldzozNjk1NTUw#👋-how-to-build-a-callback-in-langchain-for-better-prompt-engineering
) 


**Note**: _the `WandbCallbackHandler` is being deprecated in favour of the `WandbTracer`_ . In future please use the `WandbTracer` as it is more flexible and allows for more granular logging. To know more about the `WandbTracer` refer to the [agent_with_wandb_tracing](/docs/integrations/providers/wandb_tracing) notebook or use the following [colab notebook](http://wandb.me/prompts-quickstart). To know more about Weights & Biases Prompts refer to the following [prompts documentation](https://docs.wandb.ai/guides/prompts).


```python
%pip install --upgrade --quiet  wandb
%pip install --upgrade --quiet  pandas
%pip install --upgrade --quiet  textstat
%pip install --upgrade --quiet  spacy
!python -m spacy download en_core_web_sm
```


```python
import os

os.environ["WANDB_API_KEY"] = ""
# os.environ["OPENAI_API_KEY"] = ""
# os.environ["SERPAPI_API_KEY"] = ""
```


```python
from datetime import datetime

from langchain_community.callbacks import WandbCallbackHandler
from langchain_core.callbacks import StdOutCallbackHandler
from langchain_openai import OpenAI
```

```
Callback Handler that logs to Weights and Biases.

Parameters:
    job_type (str): The type of job.
    project (str): The project to log to.
    entity (str): The entity to log to.
    tags (list): The tags to log.
    group (str): The group to log to.
    name (str): The name of the run.
    notes (str): The notes to log.
    visualize (bool): Whether to visualize the run.
    complexity_metrics (bool): Whether to log complexity metrics.
    stream_logs (bool): Whether to stream callback actions to W&B
```

```
Default values for WandbCallbackHandler(...)

visualize: bool = False,
complexity_metrics: bool = False,
stream_logs: bool = False,
```


NOTE: For beta workflows we have made the default analysis based on textstat and the visualizations based on spacy


```python
"""Main function.

This function is used to try the callback handler.
Scenarios:
1. OpenAI LLM
2. Chain with multiple SubChains on multiple generations
3. Agent with Tools
"""
session_group = datetime.now().strftime("%m.%d.%Y_%H.%M.%S")
wandb_callback = WandbCallbackHandler(
    job_type="inference",
    project="langchain_callback_demo",
    group=f"minimal_{session_group}",
    name="llm",
    tags=["test"],
)
callbacks = [StdOutCallbackHandler(), wandb_callback]
llm = OpenAI(temperature=0, callbacks=callbacks)
```

    [34m[1mwandb[0m: Currently logged in as: [33mharrison-chase[0m. Use [1m`wandb login --relogin`[0m to force relogin
    


Tracking run with wandb version 0.14.0



Run data is saved locally in <code>/Users/harrisonchase/workplace/langchain/docs/ecosystem/wandb/run-20230318_150408-e47j1914</code>



Syncing run <strong><a href='https://wandb.ai/harrison-chase/langchain_callback_demo/runs/e47j1914' target="_blank">llm</a></strong> to <a href='https://wandb.ai/harrison-chase/langchain_callback_demo' target="_blank">Weights & Biases</a> (<a href='https://wandb.me/run' target="_blank">docs</a>)<br/>



View project at <a href='https://wandb.ai/harrison-chase/langchain_callback_demo' target="_blank">https://wandb.ai/harrison-chase/langchain_callback_demo</a>



View run at <a href='https://wandb.ai/harrison-chase/langchain_callback_demo/runs/e47j1914' target="_blank">https://wandb.ai/harrison-chase/langchain_callback_demo/runs/e47j1914</a>


    [34m[1mwandb[0m: [33mWARNING[0m The wandb callback is currently in beta and is subject to change based on updates to `langchain`. Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.
    



```
# Defaults for WandbCallbackHandler.flush_tracker(...)

reset: bool = True,
finish: bool = False,
```



The `flush_tracker` function is used to log LangChain sessions to Weights & Biases. It takes in the LangChain module or agent, and logs at minimum the prompts and generations alongside the serialized form of the LangChain module to the specified Weights & Biases project. By default we reset the session as opposed to concluding the session outright.


```python
# SCENARIO 1 - LLM
llm_result = llm.generate(["Tell me a joke", "Tell me a poem"] * 3)
wandb_callback.flush_tracker(llm, name="simple_sequential")
```


Waiting for W&B process to finish... <strong style="color:green">(success).</strong>



View run <strong style="color:#cdcd00">llm</strong> at: <a href='https://wandb.ai/harrison-chase/langchain_callback_demo/runs/e47j1914' target="_blank">https://wandb.ai/harrison-chase/langchain_callback_demo/runs/e47j1914</a><br/>Synced 5 W&B file(s), 2 media file(s), 5 artifact file(s) and 0 other file(s)



Find logs at: <code>./wandb/run-20230318_150408-e47j1914/logs</code>



    VBox(children=(Label(value='Waiting for wandb.init()...\r'), FloatProgress(value=0.016745895149999985, max=1.0…



Tracking run with wandb version 0.14.0



Run data is saved locally in <code>/Users/harrisonchase/workplace/langchain/docs/ecosystem/wandb/run-20230318_150534-jyxma7hu</code>



Syncing run <strong><a href='https://wandb.ai/harrison-chase/langchain_callback_demo/runs/jyxma7hu' target="_blank">simple_sequential</a></strong> to <a href='https://wandb.ai/harrison-chase/langchain_callback_demo' target="_blank">Weights & Biases</a> (<a href='https://wandb.me/run' target="_blank">docs</a>)<br/>



View project at <a href='https://wandb.ai/harrison-chase/langchain_callback_demo' target="_blank">https://wandb.ai/harrison-chase/langchain_callback_demo</a>



View run at <a href='https://wandb.ai/harrison-chase/langchain_callback_demo/runs/jyxma7hu' target="_blank">https://wandb.ai/harrison-chase/langchain_callback_demo/runs/jyxma7hu</a>



```python
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate
```


```python
# SCENARIO 2 - Chain
template = """You are a playwright. Given the title of play, it is your job to write a synopsis for that title.
Title: {title}
Playwright: This is a synopsis for the above play:"""
prompt_template = PromptTemplate(input_variables=["title"], template=template)
synopsis_chain = LLMChain(llm=llm, prompt=prompt_template, callbacks=callbacks)

test_prompts = [
    {
        "title": "documentary about good video games that push the boundary of game design"
    },
    {"title": "cocaine bear vs heroin wolf"},
    {"title": "the best in class mlops tooling"},
]
synopsis_chain.apply(test_prompts)
wandb_callback.flush_tracker(synopsis_chain, name="agent")
```


Waiting for W&B process to finish... <strong style="color:green">(success).</strong>



View run <strong style="color:#cdcd00">simple_sequential</strong> at: <a href='https://wandb.ai/harrison-chase/langchain_callback_demo/runs/jyxma7hu' target="_blank">https://wandb.ai/harrison-chase/langchain_callback_demo/runs/jyxma7hu</a><br/>Synced 4 W&B file(s), 2 media file(s), 6 artifact file(s) and 0 other file(s)



Find logs at: <code>./wandb/run-20230318_150534-jyxma7hu/logs</code>



    VBox(children=(Label(value='Waiting for wandb.init()...\r'), FloatProgress(value=0.016736786816666675, max=1.0…



Tracking run with wandb version 0.14.0



Run data is saved locally in <code>/Users/harrisonchase/workplace/langchain/docs/ecosystem/wandb/run-20230318_150550-wzy59zjq</code>



Syncing run <strong><a href='https://wandb.ai/harrison-chase/langchain_callback_demo/runs/wzy59zjq' target="_blank">agent</a></strong> to <a href='https://wandb.ai/harrison-chase/langchain_callback_demo' target="_blank">Weights & Biases</a> (<a href='https://wandb.me/run' target="_blank">docs</a>)<br/>



View project at <a href='https://wandb.ai/harrison-chase/langchain_callback_demo' target="_blank">https://wandb.ai/harrison-chase/langchain_callback_demo</a>



View run at <a href='https://wandb.ai/harrison-chase/langchain_callback_demo/runs/wzy59zjq' target="_blank">https://wandb.ai/harrison-chase/langchain_callback_demo/runs/wzy59zjq</a>



```python
from langchain.agents import AgentType, initialize_agent, load_tools
```


```python
# SCENARIO 3 - Agent with Tools
tools = load_tools(["serpapi", "llm-math"], llm=llm)
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
)
agent.run(
    "Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?",
    callbacks=callbacks,
)
wandb_callback.flush_tracker(agent, reset=False, finish=True)
```

    
    
    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3m I need to find out who Leo DiCaprio's girlfriend is and then calculate her age raised to the 0.43 power.
    Action: Search
    Action Input: "Leo DiCaprio girlfriend"[0m
    Observation: [36;1m[1;3mDiCaprio had a steady girlfriend in Camila Morrone. He had been with the model turned actress for nearly five years, as they were first said to be dating at the end of 2017. And the now 26-year-old Morrone is no stranger to Hollywood.[0m
    Thought:[32;1m[1;3m I need to calculate her age raised to the 0.43 power.
    Action: Calculator
    Action Input: 26^0.43[0m
    Observation: [33;1m[1;3mAnswer: 4.059182145592686
    [0m
    Thought:[32;1m[1;3m I now know the final answer.
    Final Answer: Leo DiCaprio's girlfriend is Camila Morrone and her current age raised to the 0.43 power is 4.059182145592686.[0m
    
    [1m> Finished chain.[0m
    


Waiting for W&B process to finish... <strong style="color:green">(success).</strong>



View run <strong style="color:#cdcd00">agent</strong> at: <a href='https://wandb.ai/harrison-chase/langchain_callback_demo/runs/wzy59zjq' target="_blank">https://wandb.ai/harrison-chase/langchain_callback_demo/runs/wzy59zjq</a><br/>Synced 5 W&B file(s), 2 media file(s), 7 artifact file(s) and 0 other file(s)



Find logs at: <code>./wandb/run-20230318_150550-wzy59zjq/logs</code>



```python

```




################################################## weather.md ##################################################


# Weather

>[OpenWeatherMap](https://openweathermap.org/) is an open-source weather service provider

This loader fetches the weather data from the OpenWeatherMap's OneCall API, using the pyowm Python package. You must initialize the loader with your OpenWeatherMap API token and the names of the cities you want the weather data for.


```python
from langchain_community.document_loaders import WeatherDataLoader
```


```python
%pip install --upgrade --quiet  pyowm
```


```python
# Set API key either by passing it in to constructor directly
# or by setting the environment variable "OPENWEATHERMAP_API_KEY".

from getpass import getpass

OPENWEATHERMAP_API_KEY = getpass()
```


```python
loader = WeatherDataLoader.from_params(
    ["chennai", "vellore"], openweathermap_api_key=OPENWEATHERMAP_API_KEY
)
```


```python
documents = loader.load()
documents
```




################################################## weaviate-hybrid.md ##################################################


# Weaviate Hybrid Search

>[Weaviate](https://weaviate.io/developers/weaviate) is an open-source vector database.

>[Hybrid search](https://weaviate.io/blog/hybrid-search-explained) is a technique that combines multiple search algorithms to improve the accuracy and relevance of search results. It uses the best features of both keyword-based search algorithms with vector search techniques.

>The `Hybrid search in Weaviate` uses sparse and dense vectors to represent the meaning and context of search queries and documents.

This notebook shows how to use `Weaviate hybrid search` as a LangChain retriever.

Set up the retriever:


```python
%pip install --upgrade --quiet  weaviate-client
```


```python
import os

import weaviate

WEAVIATE_URL = os.getenv("WEAVIATE_URL")
auth_client_secret = (weaviate.AuthApiKey(api_key=os.getenv("WEAVIATE_API_KEY")),)
client = weaviate.Client(
    url=WEAVIATE_URL,
    additional_headers={
        "X-Openai-Api-Key": os.getenv("OPENAI_API_KEY"),
    },
)

# client.schema.delete_all()
```


```python
from langchain_community.retrievers import (
    WeaviateHybridSearchRetriever,
)
from langchain_core.documents import Document
```


```python
retriever = WeaviateHybridSearchRetriever(
    client=client,
    index_name="LangChain",
    text_key="text",
    attributes=[],
    create_schema_if_missing=True,
)
```

Add some data:


```python
docs = [
    Document(
        metadata={
            "title": "Embracing The Future: AI Unveiled",
            "author": "Dr. Rebecca Simmons",
        },
        page_content="A comprehensive analysis of the evolution of artificial intelligence, from its inception to its future prospects. Dr. Simmons covers ethical considerations, potentials, and threats posed by AI.",
    ),
    Document(
        metadata={
            "title": "Symbiosis: Harmonizing Humans and AI",
            "author": "Prof. Jonathan K. Sterling",
        },
        page_content="Prof. Sterling explores the potential for harmonious coexistence between humans and artificial intelligence. The book discusses how AI can be integrated into society in a beneficial and non-disruptive manner.",
    ),
    Document(
        metadata={"title": "AI: The Ethical Quandary", "author": "Dr. Rebecca Simmons"},
        page_content="In her second book, Dr. Simmons delves deeper into the ethical considerations surrounding AI development and deployment. It is an eye-opening examination of the dilemmas faced by developers, policymakers, and society at large.",
    ),
    Document(
        metadata={
            "title": "Conscious Constructs: The Search for AI Sentience",
            "author": "Dr. Samuel Cortez",
        },
        page_content="Dr. Cortez takes readers on a journey exploring the controversial topic of AI consciousness. The book provides compelling arguments for and against the possibility of true AI sentience.",
    ),
    Document(
        metadata={
            "title": "Invisible Routines: Hidden AI in Everyday Life",
            "author": "Prof. Jonathan K. Sterling",
        },
        page_content="In his follow-up to 'Symbiosis', Prof. Sterling takes a look at the subtle, unnoticed presence and influence of AI in our everyday lives. It reveals how AI has become woven into our routines, often without our explicit realization.",
    ),
]
```


```python
retriever.add_documents(docs)
```




    ['3a27b0a5-8dbb-4fee-9eba-8b6bc2c252be',
     'eeb9fd9b-a3ac-4d60-a55b-a63a25d3b907',
     '7ebbdae7-1061-445f-a046-1989f2343d8f',
     'c2ab315b-3cab-467f-b23a-b26ed186318d',
     'b83765f2-e5d2-471f-8c02-c3350ade4c4f']



Do a hybrid search:


```python
retriever.invoke("the ethical implications of AI")
```




    [Document(page_content='In her second book, Dr. Simmons delves deeper into the ethical considerations surrounding AI development and deployment. It is an eye-opening examination of the dilemmas faced by developers, policymakers, and society at large.', metadata={}),
     Document(page_content='A comprehensive analysis of the evolution of artificial intelligence, from its inception to its future prospects. Dr. Simmons covers ethical considerations, potentials, and threats posed by AI.', metadata={}),
     Document(page_content="In his follow-up to 'Symbiosis', Prof. Sterling takes a look at the subtle, unnoticed presence and influence of AI in our everyday lives. It reveals how AI has become woven into our routines, often without our explicit realization.", metadata={}),
     Document(page_content='Prof. Sterling explores the potential for harmonious coexistence between humans and artificial intelligence. The book discusses how AI can be integrated into society in a beneficial and non-disruptive manner.', metadata={})]



Do a hybrid search with where filter:


```python
retriever.invoke(
    "AI integration in society",
    where_filter={
        "path": ["author"],
        "operator": "Equal",
        "valueString": "Prof. Jonathan K. Sterling",
    },
)
```




    [Document(page_content='Prof. Sterling explores the potential for harmonious coexistence between humans and artificial intelligence. The book discusses how AI can be integrated into society in a beneficial and non-disruptive manner.', metadata={}),
     Document(page_content="In his follow-up to 'Symbiosis', Prof. Sterling takes a look at the subtle, unnoticed presence and influence of AI in our everyday lives. It reveals how AI has become woven into our routines, often without our explicit realization.", metadata={})]



Do a hybrid search with scores:


```python
retriever.invoke(
    "AI integration in society",
    score=True,
)
```




    [Document(page_content='Prof. Sterling explores the potential for harmonious coexistence between humans and artificial intelligence. The book discusses how AI can be integrated into society in a beneficial and non-disruptive manner.', metadata={'_additional': {'explainScore': '(bm25)\n(hybrid) Document eeb9fd9b-a3ac-4d60-a55b-a63a25d3b907 contributed 0.00819672131147541 to the score\n(hybrid) Document eeb9fd9b-a3ac-4d60-a55b-a63a25d3b907 contributed 0.00819672131147541 to the score', 'score': '0.016393442'}}),
     Document(page_content="In his follow-up to 'Symbiosis', Prof. Sterling takes a look at the subtle, unnoticed presence and influence of AI in our everyday lives. It reveals how AI has become woven into our routines, often without our explicit realization.", metadata={'_additional': {'explainScore': '(bm25)\n(hybrid) Document b83765f2-e5d2-471f-8c02-c3350ade4c4f contributed 0.0078125 to the score\n(hybrid) Document b83765f2-e5d2-471f-8c02-c3350ade4c4f contributed 0.008064516129032258 to the score', 'score': '0.015877016'}}),
     Document(page_content='In her second book, Dr. Simmons delves deeper into the ethical considerations surrounding AI development and deployment. It is an eye-opening examination of the dilemmas faced by developers, policymakers, and society at large.', metadata={'_additional': {'explainScore': '(bm25)\n(hybrid) Document 7ebbdae7-1061-445f-a046-1989f2343d8f contributed 0.008064516129032258 to the score\n(hybrid) Document 7ebbdae7-1061-445f-a046-1989f2343d8f contributed 0.0078125 to the score', 'score': '0.015877016'}}),
     Document(page_content='A comprehensive analysis of the evolution of artificial intelligence, from its inception to its future prospects. Dr. Simmons covers ethical considerations, potentials, and threats posed by AI.', metadata={'_additional': {'explainScore': '(vector) [-0.0071824766 -0.0006682752 0.001723625 -0.01897258 -0.0045127636 0.0024410256 -0.020503938 0.013768672 0.009520169 -0.037972264]...  \n(hybrid) Document 3a27b0a5-8dbb-4fee-9eba-8b6bc2c252be contributed 0.007936507936507936 to the score', 'score': '0.007936508'}})]






################################################## weaviate.md ##################################################


---
sidebar_label: Weaviate
---
# Weaviate

This notebook covers how to get started with the Weaviate vector store in LangChain, using the `langchain-weaviate` package.

> [Weaviate](https://weaviate.io/) is an open-source vector database. It allows you to store data objects and vector embeddings from your favorite ML-models, and scale seamlessly into billions of data objects.

To use this integration, you need to have a running Weaviate database instance.

## Minimum versions

This module requires Weaviate `1.23.7` or higher. However, we recommend you use the latest version of Weaviate.

## Connecting to Weaviate

In this notebook, we assume that you have a local instance of Weaviate running on `http://localhost:8080` and port 50051 open for [gRPC traffic](https://weaviate.io/blog/grpc-performance-improvements). So, we will connect to Weaviate with:

```python
weaviate_client = weaviate.connect_to_local()
```

### Other deployment options

Weaviate can be [deployed in many different ways](https://weaviate.io/developers/weaviate/starter-guides/which-weaviate) such as using [Weaviate Cloud Services (WCS)](https://console.weaviate.cloud), [Docker](https://weaviate.io/developers/weaviate/installation/docker-compose) or [Kubernetes](https://weaviate.io/developers/weaviate/installation/kubernetes). 

If your Weaviate instance is deployed in another way, [read more here](https://weaviate.io/developers/weaviate/client-libraries/python#instantiate-a-client) about different ways to connect to Weaviate. You can use different [helper functions](https://weaviate.io/developers/weaviate/client-libraries/python#python-client-v4-helper-functions) or [create a custom instance](https://weaviate.io/developers/weaviate/client-libraries/python#python-client-v4-explicit-connection).

> Note that you require a `v4` client API, which will create a `weaviate.WeaviateClient` object.

### Authentication

Some Weaviate instances, such as those running on WCS, have authentication enabled, such as API key and/or username+password authentication.

Read the [client authentication guide](https://weaviate.io/developers/weaviate/client-libraries/python#authentication) for more information, as well as the [in-depth authentication configuration page](https://weaviate.io/developers/weaviate/configuration/authentication).

## Installation


```python
# install package
# %pip install -Uqq langchain-weaviate
# %pip install openai tiktoken langchain
```

## Environment Setup

This notebook uses the OpenAI API through `OpenAIEmbeddings`. We suggest obtaining an OpenAI API key and export it as an environment variable with the name `OPENAI_API_KEY`.

Once this is done, your OpenAI API key will be read automatically. If you are new to environment variables, read more about them [here](https://docs.python.org/3/library/os.html#os.environ) or in [this guide](https://www.twilio.com/en-us/blog/environment-variables-python).

# Usage

## Find objects by similarity

Here is an example of how to find objects by similarity to a query, from data import to querying the Weaviate instance.

### Step 1: Data import

First, we will create data to add to `Weaviate` by loading and chunking the contents of a long text file. 


```python
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter
```


```python
loader = TextLoader("state_of_the_union.txt")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

embeddings = OpenAIEmbeddings()
```

    /workspaces/langchain-weaviate/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.
      warn_deprecated(
    

Now, we can import the data. 

To do so, connect to the Weaviate instance and use the resulting `weaviate_client` object. For example, we can import the documents as shown below:


```python
import weaviate
from langchain_weaviate.vectorstores import WeaviateVectorStore
```


```python
weaviate_client = weaviate.connect_to_local()
db = WeaviateVectorStore.from_documents(docs, embeddings, client=weaviate_client)
```

    /workspaces/langchain-weaviate/.venv/lib/python3.12/site-packages/pydantic/main.py:1024: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/
      warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)
    

### Step 2: Perform the search

We can now perform a similarity search. This will return the most similar documents to the query text, based on the embeddings stored in Weaviate and an equivalent embedding generated from the query text.


```python
query = "What did the president say about Ketanji Brown Jackson"
docs = db.similarity_search(query)

# Print the first 100 characters of each result
for i, doc in enumerate(docs):
    print(f"\nDocument {i+1}:")
    print(doc.page_content[:100] + "...")
```

    
    Document 1:
    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Ac...
    
    Document 2:
    And so many families are living paycheck to paycheck, struggling to keep up with the rising cost of ...
    
    Document 3:
    Vice President Harris and I ran for office with a new economic vision for America. 
    
    Invest in Ameri...
    
    Document 4:
    A former top litigator in private practice. A former federal public defender. And from a family of p...
    

You can also add filters, which will either include or exclude results based on the filter conditions. (See [more filter examples](https://weaviate.io/developers/weaviate/search/filters).)


```python
from weaviate.classes.query import Filter

for filter_str in ["blah.txt", "state_of_the_union.txt"]:
    search_filter = Filter.by_property("source").equal(filter_str)
    filtered_search_results = db.similarity_search(query, filters=search_filter)
    print(len(filtered_search_results))
    if filter_str == "state_of_the_union.txt":
        assert len(filtered_search_results) > 0  # There should be at least one result
    else:
        assert len(filtered_search_results) == 0  # There should be no results
```

    0
    4
    

It is also possible to provide `k`, which is the upper limit of the number of results to return.


```python
search_filter = Filter.by_property("source").equal("state_of_the_union.txt")
filtered_search_results = db.similarity_search(query, filters=search_filter, k=3)
assert len(filtered_search_results) <= 3
```

### Quantify result similarity

You can optionally retrieve a relevance "score". This is a relative score that indicates how good the particular search results is, amongst the pool of search results. 

Note that this is relative score, meaning that it should not be used to determine thresholds for relevance. However, it can be used to compare the relevance of different search results within the entire search result set.


```python
docs = db.similarity_search_with_score("country", k=5)

for doc in docs:
    print(f"{doc[1]:.3f}", ":", doc[0].page_content[:100] + "...")
```

    0.935 : For that purpose we’ve mobilized American ground forces, air squadrons, and ship deployments to prot...
    0.500 : And built the strongest, freest, and most prosperous nation the world has ever known. 
    
    Now is the h...
    0.462 : If you travel 20 miles east of Columbus, Ohio, you’ll find 1,000 empty acres of land. 
    
    It won’t loo...
    0.450 : And my report is this: the State of the Union is strong—because you, the American people, are strong...
    0.442 : Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Ac...
    

## Search mechanism

`similarity_search` uses Weaviate's [hybrid search](https://weaviate.io/developers/weaviate/api/graphql/search-operators#hybrid).

A hybrid search combines a vector and a keyword search, with `alpha` as the weight of the vector search. The `similarity_search` function allows you to pass additional arguments as kwargs. See this [reference doc](https://weaviate.io/developers/weaviate/api/graphql/search-operators#hybrid) for the available arguments.

So, you can perform a pure keyword search by adding `alpha=0` as shown below:


```python
docs = db.similarity_search(query, alpha=0)
docs[0]
```




    Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': 'state_of_the_union.txt'})



## Persistence

Any data added through `langchain-weaviate` will persist in Weaviate according to its configuration. 

WCS instances, for example, are configured to persist data indefinitely, and Docker instances can be set up to persist data in a volume. Read more about [Weaviate's persistence](https://weaviate.io/developers/weaviate/configuration/persistence).

## Multi-tenancy

[Multi-tenancy](https://weaviate.io/developers/weaviate/concepts/data#multi-tenancy) allows you to have a high number of isolated collections of data, with the same collection configuration, in a single Weaviate instance. This is great for multi-user environments such as building a SaaS app, where each end user will have their own isolated data collection.

To use multi-tenancy, the vector store need to be aware of the `tenant` parameter. 

So when adding any data, provide the `tenant` parameter as shown below.


```python
db_with_mt = WeaviateVectorStore.from_documents(
    docs, embeddings, client=weaviate_client, tenant="Foo"
)
```

    2024-Mar-26 03:40 PM - langchain_weaviate.vectorstores - INFO - Tenant Foo does not exist in index LangChain_30b9273d43b3492db4fb2aba2e0d6871. Creating tenant.
    

And when performing queries, provide the `tenant` parameter also.


```python
db_with_mt.similarity_search(query, tenant="Foo")
```




    [Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': 'state_of_the_union.txt'}),
     Document(page_content='And so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more. \n\nI understand. \n\nI remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it. \n\nThat’s why one of the first things I did as President was fight to pass the American Rescue Plan.  \n\nBecause people were hurting. We needed to act, and we did. \n\nFew pieces of legislation have done more in a critical moment in our history to lift us out of crisis. \n\nIt fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \n\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \n\nAnd as my Dad used to say, it gave people a little breathing room.', metadata={'source': 'state_of_the_union.txt'}),
     Document(page_content='He and his Dad both have Type 1 diabetes, which means they need insulin every day. Insulin costs about $10 a vial to make.  \n\nBut drug companies charge families like Joshua and his Dad up to 30 times more. I spoke with Joshua’s mom. \n\nImagine what it’s like to look at your child who needs insulin and have no idea how you’re going to pay for it.  \n\nWhat it does to your dignity, your ability to look your child in the eye, to be the parent you expect to be. \n\nJoshua is here with us tonight. Yesterday was his birthday. Happy birthday, buddy.  \n\nFor Joshua, and for the 200,000 other young people with Type 1 diabetes, let’s cap the cost of insulin at $35 a month so everyone can afford it.  \n\nDrug companies will still do very well. And while we’re at it let Medicare negotiate lower prices for prescription drugs, like the VA already does.', metadata={'source': 'state_of_the_union.txt'}),
     Document(page_content='Putin’s latest attack on Ukraine was premeditated and unprovoked. \n\nHe rejected repeated efforts at diplomacy. \n\nHe thought the West and NATO wouldn’t respond. And he thought he could divide us at home. Putin was wrong. We were ready.  Here is what we did.   \n\nWe prepared extensively and carefully. \n\nWe spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. \n\nI spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression.  \n\nWe countered Russia’s lies with truth.   \n\nAnd now that he has acted the free world is holding him accountable. \n\nAlong with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland.', metadata={'source': 'state_of_the_union.txt'})]



## Retriever options

Weaviate can also be used as a retriever

### Maximal marginal relevance search (MMR)

In addition to using similaritysearch  in the retriever object, you can also use `mmr`.


```python
retriever = db.as_retriever(search_type="mmr")
retriever.invoke(query)[0]
```

    /workspaces/langchain-weaviate/.venv/lib/python3.12/site-packages/pydantic/main.py:1024: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/
      warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)
    




    Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': 'state_of_the_union.txt'})



# Use with LangChain

A known limitation of large languag models (LLMs) is that their training data can be outdated, or not include the specific domain knowledge that you require.

Take a look at the example below:


```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
llm.predict("What did the president say about Justice Breyer")
```

    /workspaces/langchain-weaviate/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.
      warn_deprecated(
    /workspaces/langchain-weaviate/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.
      warn_deprecated(
    /workspaces/langchain-weaviate/.venv/lib/python3.12/site-packages/pydantic/main.py:1024: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/
      warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)
    




    "I'm sorry, I cannot provide real-time information as my responses are generated based on a mixture of licensed data, data created by human trainers, and publicly available data. The last update was in October 2021."



Vector stores complement LLMs by providing a way to store and retrieve relevant information. This allow you to combine the strengths of LLMs and vector stores, by using LLM's reasoning and linguistic capabilities with vector stores' ability to retrieve relevant information.

Two well-known applications for combining LLMs and vector stores are:
- Question answering
- Retrieval-augmented generation (RAG)

### Question Answering with Sources

Question answering in langchain can be enhanced by the use of vector stores. Let's see how this can be done.

This section uses the `RetrievalQAWithSourcesChain`, which does the lookup of the documents from an Index. 

First, we will chunk the text again and import them into the Weaviate vector store.


```python
from langchain.chains import RetrievalQAWithSourcesChain
from langchain_openai import OpenAI
```


```python
with open("state_of_the_union.txt") as f:
    state_of_the_union = f.read()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_text(state_of_the_union)
```


```python
docsearch = WeaviateVectorStore.from_texts(
    texts,
    embeddings,
    client=weaviate_client,
    metadatas=[{"source": f"{i}-pl"} for i in range(len(texts))],
)
```

Now we can construct the chain, with the retriever specified:


```python
chain = RetrievalQAWithSourcesChain.from_chain_type(
    OpenAI(temperature=0), chain_type="stuff", retriever=docsearch.as_retriever()
)
```

    /workspaces/langchain-weaviate/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.
      warn_deprecated(
    

And run the chain, to ask the question:


```python
chain(
    {"question": "What did the president say about Justice Breyer"},
    return_only_outputs=True,
)
```

    /workspaces/langchain-weaviate/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.
      warn_deprecated(
    /workspaces/langchain-weaviate/.venv/lib/python3.12/site-packages/pydantic/main.py:1024: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/
      warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)
    /workspaces/langchain-weaviate/.venv/lib/python3.12/site-packages/pydantic/main.py:1024: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/
      warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)
    




    {'answer': ' The president thanked Justice Stephen Breyer for his service and announced his nomination of Judge Ketanji Brown Jackson to the Supreme Court.\n',
     'sources': '31-pl'}



### Retrieval-Augmented Generation

Another very popular application of combining LLMs and vector stores is retrieval-augmented generation (RAG). This is a technique that uses a retriever to find relevant information from a vector store, and then uses an LLM to provide an output based on the retrieved data and a prompt.

We begin with a similar setup:


```python
with open("state_of_the_union.txt") as f:
    state_of_the_union = f.read()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_text(state_of_the_union)
```


```python
docsearch = WeaviateVectorStore.from_texts(
    texts,
    embeddings,
    client=weaviate_client,
    metadatas=[{"source": f"{i}-pl"} for i in range(len(texts))],
)

retriever = docsearch.as_retriever()
```

We need to construct a template for the RAG model so that the retrieved information will be populated in the template.


```python
from langchain_core.prompts import ChatPromptTemplate

template = """You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question}
Context: {context}
Answer:
"""
prompt = ChatPromptTemplate.from_template(template)

print(prompt)
```

    input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question}\nContext: {context}\nAnswer:\n"))]
    


```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
```

And running the cell, we get a very similar output.


```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

rag_chain.invoke("What did the president say about Justice Breyer")
```

    /workspaces/langchain-weaviate/.venv/lib/python3.12/site-packages/pydantic/main.py:1024: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/
      warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)
    /workspaces/langchain-weaviate/.venv/lib/python3.12/site-packages/pydantic/main.py:1024: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/
      warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)
    




    "The president honored Justice Stephen Breyer for his service to the country as an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. The president also mentioned nominating Circuit Court of Appeals Judge Ketanji Brown Jackson to continue Justice Breyer's legacy of excellence. The president expressed gratitude towards Justice Breyer and highlighted the importance of nominating someone to serve on the United States Supreme Court."



But note that since the template is upto you to construct, you can customize it to your needs. 

### Wrap-up & resources

Weaviate is a scalable, production-ready vector store. 

This integration allows Weaviate to be used with LangChain to enhance the capabilities of large language models with a robust data store. Its scalability and production-readiness make it a great choice as a vector store for your LangChain applications, and it will reduce your time to production.




################################################## weaviate_self_query.md ##################################################


# Weaviate

>[Weaviate](https://weaviate.io/) is an open-source vector database. It allows you to store data objects and vector embeddings from
>your favorite ML models, and scale seamlessly into billions of data objects.

In the notebook, we'll demo the `SelfQueryRetriever` wrapped around a `Weaviate` vector store. 

## Creating a Weaviate vector store
First we'll want to create a Weaviate vector store and seed it with some data. We've created a small demo set of documents that contain summaries of movies.

**Note:** The self-query retriever requires you to have `lark` installed (`pip install lark`). We also need the `weaviate-client` package.


```python
%pip install --upgrade --quiet  lark weaviate-client
```


```python
from langchain_community.vectorstores import Weaviate
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
```


```python
docs = [
    Document(
        page_content="A bunch of scientists bring back dinosaurs and mayhem breaks loose",
        metadata={"year": 1993, "rating": 7.7, "genre": "science fiction"},
    ),
    Document(
        page_content="Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
        metadata={"year": 2010, "director": "Christopher Nolan", "rating": 8.2},
    ),
    Document(
        page_content="A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
        metadata={"year": 2006, "director": "Satoshi Kon", "rating": 8.6},
    ),
    Document(
        page_content="A bunch of normal-sized women are supremely wholesome and some men pine after them",
        metadata={"year": 2019, "director": "Greta Gerwig", "rating": 8.3},
    ),
    Document(
        page_content="Toys come alive and have a blast doing so",
        metadata={"year": 1995, "genre": "animated"},
    ),
    Document(
        page_content="Three men walk into the Zone, three men walk out of the Zone",
        metadata={
            "year": 1979,
            "director": "Andrei Tarkovsky",
            "genre": "science fiction",
            "rating": 9.9,
        },
    ),
]
vectorstore = Weaviate.from_documents(
    docs, embeddings, weaviate_url="http://127.0.0.1:8080"
)
```

## Creating our self-querying retriever
Now we can instantiate our retriever. To do this we'll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.


```python
from langchain.chains.query_constructor.base import AttributeInfo
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain_openai import OpenAI

metadata_field_info = [
    AttributeInfo(
        name="genre",
        description="The genre of the movie",
        type="string or list[string]",
    ),
    AttributeInfo(
        name="year",
        description="The year the movie was released",
        type="integer",
    ),
    AttributeInfo(
        name="director",
        description="The name of the movie director",
        type="string",
    ),
    AttributeInfo(
        name="rating", description="A 1-10 rating for the movie", type="float"
    ),
]
document_content_description = "Brief summary of a movie"
llm = OpenAI(temperature=0)
retriever = SelfQueryRetriever.from_llm(
    llm, vectorstore, document_content_description, metadata_field_info, verbose=True
)
```

## Testing it out
And now we can try actually using our retriever!


```python
# This example only specifies a relevant query
retriever.invoke("What are some movies about dinosaurs")
```

    query='dinosaur' filter=None limit=None
    




    [Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993}),
     Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'rating': None, 'year': 1995}),
     Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'genre': 'science fiction', 'rating': 9.9, 'year': 1979}),
     Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'genre': None, 'rating': 8.6, 'year': 2006})]




```python
# This example specifies a query and a filter
retriever.invoke("Has Greta Gerwig directed any movies about women")
```

    query='women' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Greta Gerwig') limit=None
    




    [Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'genre': None, 'rating': 8.3, 'year': 2019})]



## Filter k

We can also use the self query retriever to specify `k`: the number of documents to fetch.

We can do this by passing `enable_limit=True` to the constructor.


```python
retriever = SelfQueryRetriever.from_llm(
    llm,
    vectorstore,
    document_content_description,
    metadata_field_info,
    enable_limit=True,
    verbose=True,
)
```


```python
# This example only specifies a relevant query
retriever.invoke("what are two movies about dinosaurs")
```

    query='dinosaur' filter=None limit=2
    




    [Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993}),
     Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'rating': None, 'year': 1995})]






################################################## web_base.md ##################################################


# WebBaseLoader

This covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`. 

If you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.

## Overview
### Integration details

- TODO: Fill in table features.
- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.
- TODO: Make sure API reference links are correct.

| Class | Package | Local | Serializable | JS support|
| :--- | :--- | :---: | :---: |  :---: |
| [WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) | [langchain_community](https://python.langchain.com/api_reference/community/index.html) | ✅ | ❌ | ❌ | 
### Loader features
| Source | Document Lazy Loading | Native Async Support
| :---: | :---: | :---: | 
| WebBaseLoader | ✅ | ✅ | 

## Setup

### Credentials

`WebBaseLoader` does not require any credentials.

### Installation

To use the `WebBaseLoader` you first need to install the `langchain-community` python package.



```python
%pip install -qU langchain_community beautifulsoup4
```

## Initialization

Now we can instantiate our model object and load documents:


```python
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://www.espn.com/")
```

To bypass SSL verification errors during fetching, you can set the "verify" option:

`loader.requests_kwargs = {'verify':False}`

### Initialization with multiple pages

You can also pass in a list of pages to load from.


```python
loader_multiple_pages = WebBaseLoader(["https://www.espn.com/", "https://google.com"])
```

## Load


```python
docs = loader.load()

docs[0]
```




    Document(metadata={'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}, page_content="\n\n\n\n\n\n\n\n\nESPN - Serving Sports Fans. Anytime. Anywhere.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        Skip to main content\n    \n\n        Skip to navigation\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<\n\n>\n\n\n\n\n\n\n\n\n\nMenuESPN\n\n\n\n\n\nscores\n\n\n\nNFLNBAMLBOlympicsSoccerWNBA…BoxingCFLNCAACricketF1GolfHorseLLWSMMANASCARNBA G LeagueNBA Summer LeagueNCAAFNCAAMNCAAWNHLNWSLPLLProfessional WrestlingRacingRN BBRN FBRugbySports BettingTennisX GamesUFLMore ESPNFantasyWatchESPN BETESPN+\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\nSubscribe Now\n\n\n\n\n\nBoxing: Crawford vs. Madrimov (ESPN+ PPV)\n\n\n\n\n\n\n\nPFL Playoffs: Heavyweights & Women's Flyweights\n\n\n\n\n\n\n\nMLB\n\n\n\n\n\n\n\nLittle League Baseball: Regionals\n\n\n\n\n\n\n\nIn The Arena: Serena Williams\n\n\n\n\n\n\n\nThe 30 College Football Playoff Contenders\n\n\nQuick Links\n\n\n\n\n2024 Paris Olympics\n\n\n\n\n\n\n\nOlympics: Everything to Know\n\n\n\n\n\n\n\nMLB Standings\n\n\n\n\n\n\n\nSign up: Fantasy Football\n\n\n\n\n\n\n\nWNBA Rookie Tracker\n\n\n\n\n\n\n\nNBA Free Agency Buzz\n\n\n\n\n\n\n\nLittle League Baseball, Softball\n\n\n\n\n\n\n\nESPN Radio: Listen Live\n\n\n\n\n\n\n\nWatch Golf on ESPN\n\n\n\n\n\n\nFavorites\n\n\n\n\n\n\n      Manage Favorites\n      \n\n\n\nCustomize ESPNCreate AccountLog InFantasy\n\n\n\n\nFootball\n\n\n\n\n\n\n\nBaseball\n\n\n\n\n\n\n\nBasketball\n\n\n\n\n\n\n\nHockey\n\n\nESPN Sites\n\n\n\n\nESPN Deportes\n\n\n\n\n\n\n\nAndscape\n\n\n\n\n\n\n\nespnW\n\n\n\n\n\n\n\nESPNFC\n\n\n\n\n\n\n\nX Games\n\n\n\n\n\n\n\nSEC Network\n\n\nESPN Apps\n\n\n\n\nESPN\n\n\n\n\n\n\n\nESPN Fantasy\n\n\n\n\n\n\n\nTournament Challenge\n\n\nFollow ESPN\n\n\n\n\nFacebook\n\n\n\n\n\n\n\nX/Twitter\n\n\n\n\n\n\n\nInstagram\n\n\n\n\n\n\n\nSnapchat\n\n\n\n\n\n\n\nTikTok\n\n\n\n\n\n\n\nYouTube\n\n\nCollege football's most entertaining conference? Why the 16-team Big 12 is wiiiiiide open this seasonLong known as one of the sport's most unpredictable conferences, the new-look Big 12 promises another dose of chaos.11hBill ConnellyScott Winters/Icon SportswireUSC, Oregon and the quest to bulk up for the Big TenTo improve on D, the Trojans wanted to bulk up for a new league. They're not the only team trying to do that.10hAdam RittenbergThe 30 teams that can reach the CFPConnelly's best games of the 2024 seasonTOP HEADLINESTeam USA sets world record in 4x400 mixed relayGermany beats France, undefeated in men's hoopsU.S. men's soccer exits Games after Morocco routHungary to protest Khelif's Olympic participationKobe's Staples Center locker sells for record $2.9MDjokovic, Alcaraz to meet again, this time for goldKerr: Team USA lineups based on players' rolesMarchand wins 200m IM; McEvoy takes 50 freeScouting Shedeur Sanders' NFL futureLATEST FROM PARISBreakout stars, best moments and what comes next: Breaking down the Games halfway throughAt the midpoint of the Olympics, we look back at some of the best moments and forward at what's still to come in the second week.35mESPNMustafa Yalcin/Anadolu via Getty ImagesThe numbers behind USA's world record in 4x400 mixed relay4h0:46Medal trackerFull resultsFull coverage of the OlympicsPRESEASON HAS BEGUN!New kickoff rules on display to start Hall of Fame Game19h0:41McAfee on NFL's new kickoff: It looks like a practice drill6h1:11NFL's new kickoff rules debut to mixed reviewsTexans-Bears attracts more bets than MLBTOP RANK BOXINGSATURDAY ON ESPN+ PPVWhy Terence Crawford is playing the long game for a chance to face CaneloCrawford is approaching Saturday's marquee matchup against Israil Madrimov with his sights set on landing a bigger one next: against Canelo Alvarez.10hMike CoppingerMark Robinson/Matchroom BoxingBradley's take: Crawford's power vs. Israil Madrimov's disciplined styleTimothy Bradley Jr. breaks down the junior middleweight title fight.2dTimothy Bradley Jr.Buy Crawford vs. Madrimov on ESPN+ PPVChance to impress for Madrimov -- and UzbekistanHOW FRIDAY WENTMORE FROM THE PARIS OLYMPICSGrant Fisher makes U.S. track history, Marchand wins 4th gold and more Friday at the Paris GamesSha'Carri Richardson made her long-awaited Olympic debut during the women's 100m preliminary round on Friday. Here's everything else you might have missed from Paris.27mESPNGetty ImagesU.S. men's loss to Morocco is a wake-up call before World CupOutclassed by Morocco, the U.S. men's Olympic team can take plenty of lessons with the World Cup on the horizon.4hSam BordenFull coverage of the OlympicsOLYMPIC MEN'S HOOPS SCOREBOARDFRIDAY'S GAMESOLYMPIC STANDOUTSWhy Simone Biles is now Stephen A.'s No. 1 Olympian7h0:58Alcaraz on the cusp of history after securing spot in gold medal match8h0:59Simone Biles' gymnastics titles: Olympics, Worlds, more statsOLYMPIC MEN'S SOCCER SCOREBOARDFRIDAY'S GAMESTRADE DEADLINE FALLOUTOlney: Eight big questions for traded MLB playersCan Jazz Chisholm Jr. handle New York? Is Jack Flaherty healthy? Will Jorge Soler's defense play? Key questions for players in new places.10hBuster OlneyWinslow Townson/Getty ImagesRanking the top prospects who changed teams at the MLB trade deadlineYou know the major leaguers who moved by now -- but what about the potential stars of tomorrow?1dKiley McDanielMLB Power RankingsSeries odds: Dodgers still on top; Phillies, Yanks behind them Top HeadlinesTeam USA sets world record in 4x400 mixed relayGermany beats France, undefeated in men's hoopsU.S. men's soccer exits Games after Morocco routHungary to protest Khelif's Olympic participationKobe's Staples Center locker sells for record $2.9MDjokovic, Alcaraz to meet again, this time for goldKerr: Team USA lineups based on players' rolesMarchand wins 200m IM; McEvoy takes 50 freeScouting Shedeur Sanders' NFL futureFavorites FantasyManage FavoritesFantasy HomeCustomize ESPNCreate AccountLog InICYMI0:47Nelson Palacio rips an incredible goal from outside the boxNelson Palacio scores an outside-of-the-box goal for Real Salt Lake in the 79th minute. \n\n\nMedal Tracker\n\n\n\nCountries\nAthletes\n\nOverall Medal Leaders43USA36FRA31CHNIndividual Medal LeadersGoldCHN 13FRA 11AUS 11SilverUSA 18FRA 12GBR 10BronzeUSA 16FRA 13CHN 9Overall Medal Leaders4MarchandMarchand3O'CallaghanO'Callaghan3McIntoshMcIntoshIndividual Medal LeadersGoldMarchand 4O'Callaghan 3McIntosh 2SilverSmith 3Huske 2Walsh 2BronzeYufei 3Bhaker 2Haughey 2\n\n\nFull Medal Tracker\n\n\nBest of ESPN+ESPNCollege Football Playoff 2024: 30 teams can reach postseasonHeather Dinich analyzes the teams with at least a 10% chance to make the CFP.AP Photo/Ross D. FranklinNFL Hall of Fame predictions: Who will make the next 10 classes?When will Richard Sherman and Marshawn Lynch make it? Who could join Aaron Donald in 2029? Let's map out each Hall of Fame class until 2034.Thearon W. Henderson/Getty ImagesMLB trade deadline 2024: Ranking prospects who changed teamsYou know the major leaguers who moved by now -- but what about the potential stars of tomorrow who went the other way in those deals? Trending NowIllustration by ESPNRanking the top 100 professional athletes since 2000Who tops our list of the top athletes since 2000? We're unveiling the top 25, including our voters' pick for the No. 1 spot.Photo by Kevin C. Cox/Getty Images2024 NFL offseason recap: Signings, coach moves, new rulesThink you missed something in the NFL offseason? We've got you covered with everything important that has happened since February.Stacy Revere/Getty ImagesTop 25 college football stadiums: Rose Bowl, Michigan and moreFourteen of ESPN's college football writers rank the 25 best stadiums in the sport. Who's No. 1, who missed the cut and what makes these stadiums so special?ESPNInside Nate Robinson's silent battle -- and his fight to liveFor nearly 20 years Nate Robinson has been fighting a silent battle -- one he didn't realize until recently could end his life. Sign up to play the #1 Fantasy game!Create A LeagueJoin Public LeagueReactivate A LeagueMock Draft NowSign up for FREE!Create A LeagueJoin a Public LeagueReactivate a LeaguePractice With a Mock DraftSign up for FREE!Create A LeagueJoin a Public LeagueReactivate a LeaguePractice with a Mock DraftGet a custom ESPN experienceEnjoy the benefits of a personalized accountSelect your favorite leagues, teams and players and get the latest scores, news and updates that matter most to you. \n\nESPN+\n\n\n\n\nBoxing: Crawford vs. Madrimov (ESPN+ PPV)\n\n\n\n\n\n\n\nPFL Playoffs: Heavyweights & Women's Flyweights\n\n\n\n\n\n\n\nMLB\n\n\n\n\n\n\n\nLittle League Baseball: Regionals\n\n\n\n\n\n\n\nIn The Arena: Serena Williams\n\n\n\n\n\n\n\nThe 30 College Football Playoff Contenders\n\n\nQuick Links\n\n\n\n\n2024 Paris Olympics\n\n\n\n\n\n\n\nOlympics: Everything to Know\n\n\n\n\n\n\n\nMLB Standings\n\n\n\n\n\n\n\nSign up: Fantasy Football\n\n\n\n\n\n\n\nWNBA Rookie Tracker\n\n\n\n\n\n\n\nNBA Free Agency Buzz\n\n\n\n\n\n\n\nLittle League Baseball, Softball\n\n\n\n\n\n\n\nESPN Radio: Listen Live\n\n\n\n\n\n\n\nWatch Golf on ESPN\n\n\nFantasy\n\n\n\n\nFootball\n\n\n\n\n\n\n\nBaseball\n\n\n\n\n\n\n\nBasketball\n\n\n\n\n\n\n\nHockey\n\n\nESPN Sites\n\n\n\n\nESPN Deportes\n\n\n\n\n\n\n\nAndscape\n\n\n\n\n\n\n\nespnW\n\n\n\n\n\n\n\nESPNFC\n\n\n\n\n\n\n\nX Games\n\n\n\n\n\n\n\nSEC Network\n\n\nESPN Apps\n\n\n\n\nESPN\n\n\n\n\n\n\n\nESPN Fantasy\n\n\n\n\n\n\n\nTournament Challenge\n\n\nFollow ESPN\n\n\n\n\nFacebook\n\n\n\n\n\n\n\nX/Twitter\n\n\n\n\n\n\n\nInstagram\n\n\n\n\n\n\n\nSnapchat\n\n\n\n\n\n\n\nTikTok\n\n\n\n\n\n\n\nYouTube\n\n\nTerms of UsePrivacy PolicyYour US State Privacy RightsChildren's Online Privacy PolicyInterest-Based AdsAbout Nielsen MeasurementDo Not Sell or Share My Personal InformationContact UsDisney Ad Sales SiteWork for ESPNCorrectionsESPN BET is owned and operated by PENN Entertainment, Inc. and its subsidiaries ('PENN'). ESPN BET is available in states where PENN is licensed to offer sports wagering. Must be 21+ to wager. If you or someone you know has a gambling problem and wants help, call 1-800-GAMBLER.Copyright: © 2024 ESPN Enterprises, Inc. All rights reserved.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n")




```python
print(docs[0].metadata)
```

    {'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}
    

### Load multiple urls concurrently

You can speed up the scraping process by scraping and parsing multiple urls concurrently.

There are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the server you are scraping and don't care about load, you can change the `requests_per_second` parameter to increase the max concurrent requests.  Note, while this will speed up the scraping process, but may cause the server to block you.  Be careful!


```python
%pip install -qU  nest_asyncio

# fixes a bug with asyncio and jupyter
import nest_asyncio

nest_asyncio.apply()
```

    Requirement already satisfied: nest_asyncio in /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages (1.5.6)
    


```python
loader = WebBaseLoader(["https://www.espn.com/", "https://google.com"])
loader.requests_per_second = 1
docs = loader.aload()
docs
```




    [Document(page_content="\n\n\n\n\n\n\n\n\nESPN - Serving Sports Fans. Anytime. Anywhere.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        Skip to main content\n    \n\n        Skip to navigation\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<\n\n>\n\n\n\n\n\n\n\n\n\nMenuESPN\n\n\nSearch\n\n\n\nscores\n\n\n\nNFLNBANCAAMNCAAWNHLSoccer…MLBNCAAFGolfTennisSports BettingBoxingCFLNCAACricketF1HorseLLWSMMANASCARNBA G LeagueOlympic SportsRacingRN BBRN FBRugbyWNBAWorld Baseball ClassicWWEX GamesXFLMore ESPNFantasyListenWatchESPN+\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\nSUBSCRIBE NOW\n\n\n\n\n\nNHL: Select Games\n\n\n\n\n\n\n\nXFL\n\n\n\n\n\n\n\nMLB: Select Games\n\n\n\n\n\n\n\nNCAA Baseball\n\n\n\n\n\n\n\nNCAA Softball\n\n\n\n\n\n\n\nCricket: Select Matches\n\n\n\n\n\n\n\nMel Kiper's NFL Mock Draft 3.0\n\n\nQuick Links\n\n\n\n\nMen's Tournament Challenge\n\n\n\n\n\n\n\nWomen's Tournament Challenge\n\n\n\n\n\n\n\nNFL Draft Order\n\n\n\n\n\n\n\nHow To Watch NHL Games\n\n\n\n\n\n\n\nFantasy Baseball: Sign Up\n\n\n\n\n\n\n\nHow To Watch PGA TOUR\n\n\n\n\n\n\nFavorites\n\n\n\n\n\n\n      Manage Favorites\n      \n\n\n\nCustomize ESPNSign UpLog InESPN Sites\n\n\n\n\nESPN Deportes\n\n\n\n\n\n\n\nAndscape\n\n\n\n\n\n\n\nespnW\n\n\n\n\n\n\n\nESPNFC\n\n\n\n\n\n\n\nX Games\n\n\n\n\n\n\n\nSEC Network\n\n\nESPN Apps\n\n\n\n\nESPN\n\n\n\n\n\n\n\nESPN Fantasy\n\n\nFollow ESPN\n\n\n\n\nFacebook\n\n\n\n\n\n\n\nTwitter\n\n\n\n\n\n\n\nInstagram\n\n\n\n\n\n\n\nSnapchat\n\n\n\n\n\n\n\nYouTube\n\n\n\n\n\n\n\nThe ESPN Daily Podcast\n\n\nAre you ready for Opening Day? Here's your guide to MLB's offseason chaosWait, Jacob deGrom is on the Rangers now? Xander Bogaerts and Trea Turner signed where? And what about Carlos Correa? Yeah, you're going to need to read up before Opening Day.12hESPNIllustration by ESPNEverything you missed in the MLB offseason3h2:33World Series odds, win totals, props for every teamPlay fantasy baseball for free!TOP HEADLINESQB Jackson has requested trade from RavensSources: Texas hiring Terry as full-time coachJets GM: No rush on Rodgers; Lamar not optionLove to leave North Carolina, enter transfer portalBelichick to angsty Pats fans: See last 25 yearsEmbiid out, Harden due back vs. Jokic, NuggetsLynch: Purdy 'earned the right' to start for NinersMan Utd, Wrexham plan July friendly in San DiegoOn paper, Padres overtake DodgersLAMAR WANTS OUT OF BALTIMOREMarcus Spears identifies the two teams that need Lamar Jackson the most7h2:00Would Lamar sit out? Will Ravens draft a QB? Jackson trade request insightsLamar Jackson has asked Baltimore to trade him, but Ravens coach John Harbaugh hopes the QB will be back.3hJamison HensleyBallard, Colts will consider trading for QB JacksonJackson to Indy? Washington? Barnwell ranks the QB's trade fitsSNYDER'S TUMULTUOUS 24-YEAR RUNHow Washington’s NFL franchise sank on and off the field under owner Dan SnyderSnyder purchased one of the NFL's marquee franchises in 1999. Twenty-four years later, and with the team up for sale, he leaves a legacy of on-field futility and off-field scandal.13hJohn KeimESPNIOWA STAR STEPS UP AGAINJ-Will: Caitlin Clark is the biggest brand in college sports right now8h0:47'The better the opponent, the better she plays': Clark draws comparisons to TaurasiCaitlin Clark's performance on Sunday had longtime observers going back decades to find comparisons.16hKevin PeltonWOMEN'S ELITE EIGHT SCOREBOARDMONDAY'S GAMESCheck your bracket!NBA DRAFTHow top prospects fared on the road to the Final FourThe 2023 NCAA tournament is down to four teams, and ESPN's Jonathan Givony recaps the players who saw their NBA draft stock change.11hJonathan GivonyAndy Lyons/Getty ImagesTALKING BASKETBALLWhy AD needs to be more assertive with LeBron on the court9h1:33Why Perk won't blame Kyrie for Mavs' woes8h1:48WHERE EVERY TEAM STANDSNew NFL Power Rankings: Post-free-agency 1-32 poll, plus underrated offseason movesThe free agent frenzy has come and gone. Which teams have improved their 2023 outlook, and which teams have taken a hit?12hNFL Nation reportersIllustration by ESPNTHE BUCK STOPS WITH BELICHICKBruschi: Fair to criticize Bill Belichick for Patriots' struggles10h1:27 Top HeadlinesQB Jackson has requested trade from RavensSources: Texas hiring Terry as full-time coachJets GM: No rush on Rodgers; Lamar not optionLove to leave North Carolina, enter transfer portalBelichick to angsty Pats fans: See last 25 yearsEmbiid out, Harden due back vs. Jokic, NuggetsLynch: Purdy 'earned the right' to start for NinersMan Utd, Wrexham plan July friendly in San DiegoOn paper, Padres overtake DodgersFavorites FantasyManage FavoritesFantasy HomeCustomize ESPNSign UpLog InMarch Madness LiveESPNMarch Madness LiveWatch every men's NCAA tournament game live! ICYMI1:42Austin Peay's coach, pitcher and catcher all ejected after retaliation pitchAustin Peay's pitcher, catcher and coach were all ejected after a pitch was thrown at Liberty's Nathan Keeter, who earlier in the game hit a home run and celebrated while running down the third-base line. Men's Tournament ChallengeIllustration by ESPNMen's Tournament ChallengeCheck your bracket(s) in the 2023 Men's Tournament Challenge, which you can follow throughout the Big Dance. Women's Tournament ChallengeIllustration by ESPNWomen's Tournament ChallengeCheck your bracket(s) in the 2023 Women's Tournament Challenge, which you can follow throughout the Big Dance. Best of ESPN+AP Photo/Lynne SladkyFantasy Baseball ESPN+ Cheat Sheet: Sleepers, busts, rookies and closersYou've read their names all preseason long, it'd be a shame to forget them on draft day. The ESPN+ Cheat Sheet is one way to make sure that doesn't happen.Steph Chambers/Getty ImagesPassan's 2023 MLB season preview: Bold predictions and moreOpening Day is just over a week away -- and Jeff Passan has everything you need to know covered from every possible angle.Photo by Bob Kupbens/Icon Sportswire2023 NFL free agency: Best team fits for unsigned playersWhere could Ezekiel Elliott land? Let's match remaining free agents to teams and find fits for two trade candidates.Illustration by ESPN2023 NFL mock draft: Mel Kiper's first-round pick predictionsMel Kiper Jr. makes his predictions for Round 1 of the NFL draft, including projecting a trade in the top five. Trending NowAnne-Marie Sorvin-USA TODAY SBoston Bruins record tracker: Wins, points, milestonesThe B's are on pace for NHL records in wins and points, along with some individual superlatives as well. Follow along here with our updated tracker.Mandatory Credit: William Purnell-USA TODAY Sports2023 NFL full draft order: AFC, NFC team picks for all roundsStarting with the Carolina Panthers at No. 1 overall, here's the entire 2023 NFL draft broken down round by round. How to Watch on ESPN+Gregory Fisher/Icon Sportswire2023 NCAA men's hockey: Results, bracket, how to watchThe matchups in Tampa promise to be thrillers, featuring plenty of star power, high-octane offense and stellar defense.(AP Photo/Koji Sasahara, File)How to watch the PGA Tour, Masters, PGA Championship and FedEx Cup playoffs on ESPN, ESPN+Here's everything you need to know about how to watch the PGA Tour, Masters, PGA Championship and FedEx Cup playoffs on ESPN and ESPN+.Hailie Lynch/XFLHow to watch the XFL: 2023 schedule, teams, players, news, moreEvery XFL game will be streamed on ESPN+. Find out when and where else you can watch the eight teams compete. Sign up to play the #1 Fantasy Baseball GameReactivate A LeagueCreate A LeagueJoin a Public LeaguePractice With a Mock DraftSports BettingAP Photo/Mike KropfMarch Madness betting 2023: Bracket odds, lines, tips, moreThe 2023 NCAA tournament brackets have finally been released, and we have everything you need to know to make a bet on all of the March Madness games. Sign up to play the #1 Fantasy game!Create A LeagueJoin Public LeagueReactivateMock Draft Now\n\nESPN+\n\n\n\n\nNHL: Select Games\n\n\n\n\n\n\n\nXFL\n\n\n\n\n\n\n\nMLB: Select Games\n\n\n\n\n\n\n\nNCAA Baseball\n\n\n\n\n\n\n\nNCAA Softball\n\n\n\n\n\n\n\nCricket: Select Matches\n\n\n\n\n\n\n\nMel Kiper's NFL Mock Draft 3.0\n\n\nQuick Links\n\n\n\n\nMen's Tournament Challenge\n\n\n\n\n\n\n\nWomen's Tournament Challenge\n\n\n\n\n\n\n\nNFL Draft Order\n\n\n\n\n\n\n\nHow To Watch NHL Games\n\n\n\n\n\n\n\nFantasy Baseball: Sign Up\n\n\n\n\n\n\n\nHow To Watch PGA TOUR\n\n\nESPN Sites\n\n\n\n\nESPN Deportes\n\n\n\n\n\n\n\nAndscape\n\n\n\n\n\n\n\nespnW\n\n\n\n\n\n\n\nESPNFC\n\n\n\n\n\n\n\nX Games\n\n\n\n\n\n\n\nSEC Network\n\n\nESPN Apps\n\n\n\n\nESPN\n\n\n\n\n\n\n\nESPN Fantasy\n\n\nFollow ESPN\n\n\n\n\nFacebook\n\n\n\n\n\n\n\nTwitter\n\n\n\n\n\n\n\nInstagram\n\n\n\n\n\n\n\nSnapchat\n\n\n\n\n\n\n\nYouTube\n\n\n\n\n\n\n\nThe ESPN Daily Podcast\n\n\nTerms of UsePrivacy PolicyYour US State Privacy RightsChildren's Online Privacy PolicyInterest-Based AdsAbout Nielsen MeasurementDo Not Sell or Share My Personal InformationContact UsDisney Ad Sales SiteWork for ESPNCopyright: © ESPN Enterprises, Inc. All rights reserved.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", lookup_str='', metadata={'source': 'https://www.espn.com/'}, lookup_index=0),
     Document(page_content='GoogleSearch Images Maps Play YouTube News Gmail Drive More »Web History | Settings | Sign in\xa0Advanced searchAdvertisingBusiness SolutionsAbout Google© 2023 - Privacy - Terms   ', lookup_str='', metadata={'source': 'https://google.com'}, lookup_index=0)]



### Loading a xml file, or using a different BeautifulSoup parser

You can also look at `SitemapLoader` for an example of how to load a sitemap file, which is an example of using this feature.


```python
loader = WebBaseLoader(
    "https://www.govinfo.gov/content/pkg/CFR-2018-title10-vol3/xml/CFR-2018-title10-vol3-sec431-86.xml"
)
loader.default_parser = "xml"
docs = loader.load()
docs
```




    [Document(page_content='\n\n10\nEnergy\n3\n2018-01-01\n2018-01-01\nfalse\nUniform test method for the measurement of energy efficiency of commercial packaged boilers.\nÂ§ 431.86\nSection Â§ 431.86\n\nEnergy\nDEPARTMENT OF ENERGY\nENERGY CONSERVATION\nENERGY EFFICIENCY PROGRAM FOR CERTAIN COMMERCIAL AND INDUSTRIAL EQUIPMENT\nCommercial Packaged Boilers\nTest Procedures\n\n\n\n\n§\u2009431.86\nUniform test method for the measurement of energy efficiency of commercial packaged boilers.\n(a) Scope. This section provides test procedures, pursuant to the Energy Policy and Conservation Act (EPCA), as amended, which must be followed for measuring the combustion efficiency and/or thermal efficiency of a gas- or oil-fired commercial packaged boiler.\n(b) Testing and Calculations. Determine the thermal efficiency or combustion efficiency of commercial packaged boilers by conducting the appropriate test procedure(s) indicated in Table 1 of this section.\n\nTable 1—Test Requirements for Commercial Packaged Boiler Equipment Classes\n\nEquipment category\nSubcategory\nCertified rated inputBtu/h\n\nStandards efficiency metric(§\u2009431.87)\n\nTest procedure(corresponding to\nstandards efficiency\nmetric required\nby §\u2009431.87)\n\n\n\nHot Water\nGas-fired\n≥300,000 and ≤2,500,000\nThermal Efficiency\nAppendix A, Section 2.\n\n\nHot Water\nGas-fired\n>2,500,000\nCombustion Efficiency\nAppendix A, Section 3.\n\n\nHot Water\nOil-fired\n≥300,000 and ≤2,500,000\nThermal Efficiency\nAppendix A, Section 2.\n\n\nHot Water\nOil-fired\n>2,500,000\nCombustion Efficiency\nAppendix A, Section 3.\n\n\nSteam\nGas-fired (all*)\n≥300,000 and ≤2,500,000\nThermal Efficiency\nAppendix A, Section 2.\n\n\nSteam\nGas-fired (all*)\n>2,500,000 and ≤5,000,000\nThermal Efficiency\nAppendix A, Section 2.\n\n\n\u2003\n\n>5,000,000\nThermal Efficiency\nAppendix A, Section 2.OR\nAppendix A, Section 3 with Section 2.4.3.2.\n\n\n\nSteam\nOil-fired\n≥300,000 and ≤2,500,000\nThermal Efficiency\nAppendix A, Section 2.\n\n\nSteam\nOil-fired\n>2,500,000 and ≤5,000,000\nThermal Efficiency\nAppendix A, Section 2.\n\n\n\u2003\n\n>5,000,000\nThermal Efficiency\nAppendix A, Section 2.OR\nAppendix A, Section 3. with Section 2.4.3.2.\n\n\n\n*\u2009Equipment classes for commercial packaged boilers as of July 22, 2009 (74 FR 36355) distinguish between gas-fired natural draft and all other gas-fired (except natural draft).\n\n(c) Field Tests. The field test provisions of appendix A may be used only to test a unit of commercial packaged boiler with rated input greater than 5,000,000 Btu/h.\n[81 FR 89305, Dec. 9, 2016]\n\n\nEnergy Efficiency Standards\n\n', lookup_str='', metadata={'source': 'https://www.govinfo.gov/content/pkg/CFR-2018-title10-vol3/xml/CFR-2018-title10-vol3-sec431-86.xml'}, lookup_index=0)]



## Lazy Load

You can use lazy loading to only load one page at a time in order to minimize memory requirements.


```python
pages = []
for doc in loader.lazy_load():
    pages.append(doc)

print(pages[0].page_content[:100])
print(pages[0].metadata)
```

    
    
    
    
    
    
    
    
    
    ESPN - Serving Sports Fans. Anytime. Anywhere.
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    {'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}
    

## Using proxies

Sometimes you might need to use proxies to get around IP blocks. You can pass in a dictionary of proxies to the loader (and `requests` underneath) to use them.


```python
loader = WebBaseLoader(
    "https://www.walmart.com/search?q=parrots",
    proxies={
        "http": "http://{username}:{password}:@proxy.service.com:6666/",
        "https": "https://{username}:{password}:@proxy.service.com:6666/",
    },
)
docs = loader.load()
```

## API reference

For detailed documentation of all `WebBaseLoader` features and configurations head to the API reference: https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html




################################################## Web_search_with_google_api_bring_your_own_browser_tool.md ##################################################


## Building a Bring Your Own Browser (BYOB) Tool for Web Browsing and Summarization

**Disclaimer: This cookbook is for educational purposes only. Ensure that you comply with all applicable laws and service terms when using web search and scraping technologies. This cookbook will restrict the search to openai.com domain to retrieve the public information to illustrate the concepts.**

Large Language Models (LLMs) such as GPT-4o have a knowledge cutoff date, which means they lack information about events that occurred after that point. In scenarios where the most recent data is essential, it's necessary to provide LLMs with access to current web information to ensure accurate and relevant responses.

In this guide, we will build a Bring Your Own Browser (BYOB) tool using Python to overcome this limitation. Our goal is to create a system that provides up-to-date answers in your application, including the most recent developments such as the latest product launches by OpenAI. By integrating web search capabilities with an LLM, we'll enable the model to generate responses based on the latest information available online.

While you can use any publicly available search APIs, we'll utilize Google's Custom Search API to perform web searches. The retrieved information from the search results will be processed and passed to the LLM to generate the final response through Retrieval-Augmented Generation (RAG).

**Bring Your Own Browser (BYOB)** tools allow users to perform web browsing tasks programmatically. In this notebook, we'll create a BYOB tool that:

**#1. Set Up a Search Engine:** Use a public search API, such as Google's Custom Search API, to perform web searches and obtain a list of relevant search results.  

**#2. Build a Search Dictionary:** Collect the title, URL, and a summary of each web page from the search results to create a structured dictionary of information.  

**#3. Generate a RAG Response:** Implement Retrieval-Augmented Generation (RAG) by passing the gathered information to the LLM, which then generates a final response to the user's query.



### Use Case 
In this cookbook, we'll take the example of a user who wants to list recent product launches by OpenAI in chronological order. Because the current GPT-4o model has a knowledge cutoff date, it is not expected that the model will know about recent product launches such as the o1-preview model launched in September 2024. 



```python
from openai import OpenAI

client = OpenAI()

search_query = "List the latest OpenAI product launches in chronological order from latest to oldest in the past 2 years"


response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "You are a helpful agent."},
        {"role": "user", "content": search_query}]
).choices[0].message.content

print(response)
```

    OpenAI has had several notable product launches and updates in the past couple of years. Here’s a chronological list of some significant ones:
    
    1. **ChatGPT (November 2022)**: OpenAI launched an AI chatbot called ChatGPT, which is based on GPT-3.5. This chatbot became widely popular due to its impressive capabilities in understanding and generating human-like text.
    
    2. **GPT-4 (March 2023)**: OpenAI released GPT-4, the latest version of their Generative Pre-trained Transformer model. It brought improvements in both performance and accuracy over its predecessors.
    
    3. **DALL-E 2 (April 2022)**: The second version of DALL-E, an AI model that can generate images from textual descriptions, was launched with enhanced image resolution and more robust capabilities.
    
    4. **Whisper (September 2022)**: Whisper, an automatic speech recognition (ASR) system, was introduced. This model can handle multiple languages and is useful for transcribing and understanding spoken language.
    
    These are some of the key product launches from OpenAI in the past couple of years. Keep in mind the technology landscape is continually evolving, and new developments can occur.
    

Given the knowledge cutoff, as expected the model does not know about the recent product launches by OpenAI.

### Setting up a BYOB tool
To provide the model with recent events information, we'll follow these steps:

##### Step 1: Set Up a Search Engine to Provide Web Search Results
##### Step 2: Build a Search Dictionary with Titles, URLs, and Summaries of Web Pages
##### Step 3: Pass the information to the model to generate a RAG Response to the User Query  


Before we begin, ensure you have the following: **Python 3.12 or later** installed on your machine. You will also need a Google Custom Search API key and Custom Search Engine ID (CSE ID). Necessary Python packages installed: `requests`, `beautifulsoup4`, `openai`. And ensure the OPENAI_API_KEY is set up as an environment variable.

#### Step 1: Set Up a Search Engine to Provide Web Search Results
You can use any publicly available web search APIs to perform this task. We will configure a custom search engine using Google's Custom Search API. This engine will fetch a list of relevant web pages based on the user's query, focusing on obtaining the most recent and pertinent results.  

**a. Configure Search API key and Function:** Acquire a Google API key and a Custom Search Engine ID (CSE ID) from the Google Developers Console. You can navigate to this [Programmable Search Engine Link](https://developers.google.com/custom-search/v1/overview) to set up an API key as well as Custom Search Engine ID (CSE ID). 

The `search` function below sets up the search based on search term, the API and CSE ID keys, as well as number of search results to return. We'll introduce a parameter `site_filter` to restrict the output to only `openai.com`
  


```python
import requests  # For making HTTP requests to APIs and websites

def search(search_item, api_key, cse_id, search_depth=10, site_filter=None):
    service_url = 'https://www.googleapis.com/customsearch/v1'

    params = {
        'q': search_item,
        'key': api_key,
        'cx': cse_id,
        'num': search_depth
    }

    try:
        response = requests.get(service_url, params=params)
        response.raise_for_status()
        results = response.json()

        # Check if 'items' exists in the results
        if 'items' in results:
            if site_filter is not None:
                
                # Filter results to include only those with site_filter in the link
                filtered_results = [result for result in results['items'] if site_filter in result['link']]

                if filtered_results:
                    return filtered_results
                else:
                    print(f"No results with {site_filter} found.")
                    return []
            else:
                if 'items' in results:
                    return results['items']
                else:
                    print("No search results found.")
                    return []

    except requests.exceptions.RequestException as e:
        print(f"An error occurred during the search: {e}")
        return []

```

**b. Identify the search terms for search engine:** Before we can retrieve specific results from a 3rd Party API, we may need to use Query Expansion to identify specific terms our browser search API should retrieve. **Query expansion** is a process where we broaden the original user query by adding related terms, synonyms, or variations. This technique is essential because search engines, like Google's Custom Search API, are often better at matching a range of related terms rather than just the natural language prompt used by a user. 

For example, searching with only the raw query `"List the latest OpenAI product launches in chronological order from latest to oldest in the past 2 years"` may return fewer and less relevant results than a more specific and direct search on a succinct phrase such as `"Latest OpenAI product launches"`. In the code below, we will use the user's original `search_query` to produce a more specific search term to use with the Google API to retrieve the results. 


```python
search_term = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "Provide a google search term based on search query provided below in 3-4 words"},
        {"role": "user", "content": search_query}]
).choices[0].message.content

print(search_term)
```

    Latest OpenAI product launches
    

**c. Invoke the search function:** Now that we have the search term, we will invoke the search function to retrieve the results from Google search API. The results only have the link of the web page and a snippet at this point. In the next step, we will retrieve more information from the webpage and summarize it in a dictionary to pass to the model.


```python
from dotenv import load_dotenv
import os

load_dotenv('.env')

api_key = os.getenv('API_KEY')
cse_id = os.getenv('CSE_ID')

search_items = search(search_item=search_term, api_key=api_key, cse_id=cse_id, search_depth=10, site_filter="https://openai.com")

```


```python
for item in search_items:
    print(f"Link: {item['link']}")
    print(f"Snippet: {item['snippet']}\n")
```

    Link: https://openai.com/news/
    Snippet: Overview ; Product. Sep 12, 2024. Introducing OpenAI o1 ; Product. Jul 25, 2024. SearchGPT is a prototype of new AI search features ; Research. Jul 18, 2024. GPT- ...
    
    Link: https://openai.com/index/new-models-and-developer-products-announced-at-devday/
    Snippet: Nov 6, 2023 ... GPT-4 Turbo with 128K context · We released the first version of GPT-4 in March and made GPT-4 generally available to all developers in July.
    
    Link: https://openai.com/news/product/
    Snippet: Discover the latest product advancements from OpenAI and the ways they're being used by individuals and businesses.
    
    Link: https://openai.com/
    Snippet: A new series of AI models designed to spend more time thinking before they respond. Learn more · (opens in a new window) ...
    
    Link: https://openai.com/index/sora/
    Snippet: Feb 15, 2024 ... We plan to include C2PA metadata(opens in a new window) in the future if we deploy the model in an OpenAI product. In addition to us developing ...
    
    Link: https://openai.com/o1/
    Snippet: We've developed a new series of AI models designed to spend more time thinking before they respond. Here is the latest news on o1 research, product and ...
    
    Link: https://openai.com/index/introducing-gpts/
    Snippet: Nov 6, 2023 ... We plan to offer GPTs to more users soon. Learn more about our OpenAI DevDay announcements for new models and developer products.
    
    Link: https://openai.com/api/
    Snippet: The most powerful platform for building AI products ... Build and scale AI experiences powered by industry-leading models and tools. Start building (opens in a ...
    
    

#### Step 2: Build a Search Dictionary with Titles, URLs, and Summaries of Web Pages
After obtaining the search results, we'll extract and organize the relevant information, so it can be passed to the LLM for final output. 

**a. Scrape Web Page Content:** For each URL in the search results, retrieve the web page to extract textual content while filtering out non-relevant data like scripts and advertisements as demonstrated in function `retrieve_content`. 

**b. Summarize Content:** Use an LLM to generate concise summaries of the scraped content, focusing on information pertinent to the user's query. Model can be provided the original search text, so it can focus on summarizing the content for the search intent as outlined in function `summarize_content`. 
  
**c. Create a Structured Dictionary:** Organize the data into a dictionary or a DataFrame containing the title, link, and summary for each web page. This structure can be passed on to the LLM to generate the summary with the appropriate citations.    



```python
import requests
from bs4 import BeautifulSoup

TRUNCATE_SCRAPED_TEXT = 50000  # Adjust based on your model's context window
SEARCH_DEPTH = 5

def retrieve_content(url, max_tokens=TRUNCATE_SCRAPED_TEXT):
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')
            for script_or_style in soup(['script', 'style']):
                script_or_style.decompose()

            text = soup.get_text(separator=' ', strip=True)
            characters = max_tokens * 4  # Approximate conversion
            text = text[:characters]
            return text
        except requests.exceptions.RequestException as e:
            print(f"Failed to retrieve {url}: {e}")
            return None
        
def summarize_content(content, search_term, character_limit=500):
        prompt = (
            f"You are an AI assistant tasked with summarizing content relevant to '{search_term}'. "
            f"Please provide a concise summary in {character_limit} characters or less."
        )
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": content}]
            )
            summary = response.choices[0].message.content
            return summary
        except Exception as e:
            print(f"An error occurred during summarization: {e}")
            return None

def get_search_results(search_items, character_limit=500):
    # Generate a summary of search results for the given search term
    results_list = []
    for idx, item in enumerate(search_items, start=1):
        url = item.get('link')
        
        snippet = item.get('snippet', '')
        web_content = retrieve_content(url, TRUNCATE_SCRAPED_TEXT)
        
        if web_content is None:
            print(f"Error: skipped URL: {url}")
        else:
            summary = summarize_content(web_content, search_term, character_limit)
            result_dict = {
                'order': idx,
                'link': url,
                'title': snippet,
                'Summary': summary
            }
            results_list.append(result_dict)
    return results_list
```


```python
results = get_search_results(search_items)

for result in results:
    print(f"Search order: {result['order']}")
    print(f"Link: {result['link']}")
    print(f"Snippet: {result['title']}")
    print(f"Summary: {result['Summary']}")
    print('-' * 80)
```

    Search order: 1
    Link: https://openai.com/news/
    Snippet: Overview ; Product. Sep 12, 2024. Introducing OpenAI o1 ; Product. Jul 25, 2024. SearchGPT is a prototype of new AI search features ; Research. Jul 18, 2024. GPT- ...
    Summary: OpenAI recently launched several notable products in 2024, including OpenAI o1 and SearchGPT, a prototype for enhanced AI search capabilities. Additionally, GPT-4o mini was introduced, enhancing cost-efficient intelligence. The organization also rolled out OpenAI for Nonprofits and ChatGPT Edu to support various sectors. Improvements in data analysis within ChatGPT and enhancements to the fine-tuning API were also announced. These updates reflect OpenAI's ongoing commitment to advancing AI technologies across different fields.
    --------------------------------------------------------------------------------
    Search order: 2
    Link: https://openai.com/index/new-models-and-developer-products-announced-at-devday/
    Snippet: Nov 6, 2023 ... GPT-4 Turbo with 128K context · We released the first version of GPT-4 in March and made GPT-4 generally available to all developers in July.
    Summary: OpenAI's recent DevDay revealed several new products and model updates, including the launch of GPT-4 Turbo with a 128K context window, new pricing, and enhanced multimodal capabilities. Key features include the new Assistants API for developing specialized AI applications, improved function calling, and advanced capabilities like text-to-speech and DALL·E 3 integration. Additionally, OpenAI introduced a Copyright Shield for legal protection and Whisper v3 for improved speech recognition. Pricing reductions and rate limit increases were also announced across several models.
    --------------------------------------------------------------------------------
    Search order: 3
    Link: https://openai.com/news/product/
    Snippet: Discover the latest product advancements from OpenAI and the ways they're being used by individuals and businesses.
    Summary: As of September 2024, OpenAI has launched several significant products, including OpenAI o1, a versatile AI tool, and SearchGPT, a prototype aimed at enhancing AI-driven search capabilities. Earlier, in May 2024, they introduced OpenAI for Education, emphasizing AI's integration into educational settings. Upcoming enhancements to existing products like GPT-4, DALL·E 3, and ChatGPT are also in focus, continuing OpenAI's mission to innovate across various sectors with cutting-edge AI technologies.
    --------------------------------------------------------------------------------
    Search order: 4
    Link: https://openai.com/
    Snippet: A new series of AI models designed to spend more time thinking before they respond. Learn more · (opens in a new window) ...
    Summary: OpenAI has recently launched several innovative products, including the OpenAI o1 and o1-mini models which focus on enhanced reasoning capabilities. The partnership with Apple aims to integrate ChatGPT into Apple’s user experience. OpenAI also debuted "Sora," a video generation tool from text prompts, and made significant upgrades to the ChatGPT Enterprise with new compliance tools. The introduction of structured outputs in the API and enhanced data analysis features are also notable advancements, further expanding the utility of AI in various domains.
    --------------------------------------------------------------------------------
    Search order: 5
    Link: https://openai.com/index/sora/
    Snippet: Feb 15, 2024 ... We plan to include C2PA metadata(opens in a new window) in the future if we deploy the model in an OpenAI product. In addition to us developing ...
    Summary: OpenAI has launched Sora, an innovative AI model capable of generating high-quality text-to-video content. Sora can create videos up to one minute long, simulating complex scenes with motion and character interactions based on user prompts. The model uses advanced diffusion techniques, akin to its predecessors in the GPT and DALL·E families, enabling it to understand and animate real-world physics and nuances. OpenAI is working with external artists and domain experts to ensure safety and accuracy, while gathering feedback for future enhancements before wider release.
    --------------------------------------------------------------------------------
    Search order: 6
    Link: https://openai.com/o1/
    Snippet: We've developed a new series of AI models designed to spend more time thinking before they respond. Here is the latest news on o1 research, product and ...
    Summary: OpenAI has introduced the o1 series, a new set of AI models aimed at improving response deliberation. This innovation allows models to "think" more before generating replies. The o1 models can be accessed via ChatGPT Plus and through APIs. Other recent advancements include updates to GPT-4, GPT-4o mini, and DALL·E 3. OpenAI continues to focus on enhancing product offerings for individual, team, and enterprise use, reflecting its commitment to research and safety in AI technologies.
    --------------------------------------------------------------------------------
    Search order: 7
    Link: https://openai.com/index/introducing-gpts/
    Snippet: Nov 6, 2023 ... We plan to offer GPTs to more users soon. Learn more about our OpenAI DevDay announcements for new models and developer products.
    Summary: On November 6, 2023, OpenAI launched "GPTs," allowing users to create customized versions of ChatGPT tailored to specific tasks without needing coding skills. These custom GPTs can assist in various activities, from learning games to workplace tasks. The upcoming GPT Store will feature creations from users, making them searchable and shareable. Enterprise users can develop internal-only versions, enhancing workplace productivity. Additionally, ChatGPT Plus users benefit from an improved interface that consolidates features like DALL·E and data analysis.
    --------------------------------------------------------------------------------
    Search order: 8
    Link: https://openai.com/api/
    Snippet: The most powerful platform for building AI products ... Build and scale AI experiences powered by industry-leading models and tools. Start building (opens in a ...
    Summary: OpenAI has launched several notable products, including GPT-4o and GPT-4o mini, designed for complex and lightweight tasks respectively, both featuring a 128k context length. New models like OpenAI o1-preview and o1-mini enhance reasoning capabilities. The API platform offers various tools for building AI applications, including Chat Completions, Assistants, and Batch APIs. Enhanced customization options include Fine-tuning and a Custom Model Program. OpenAI's enterprise features emphasize security, compliance, and dedicated support, facilitating widespread innovative applications across sectors.
    --------------------------------------------------------------------------------
    

We retrieved the most recent results. (Note these will vary depending on when you execute this script.) 

#### Step 3: Pass the information to the model to generate a RAG Response to the User Query
With the search data organized in a JSON data structure, we will pass this information to the LLM with the original user query to generate the final response. Now, the LLM response includes information beyond its original knowledge cutoff, providing current insights.


```python
import json 

final_prompt = (
    f"The user will provide a dictionary of search results in JSON format for search query {search_term} Based on on the search results provided by the user, provide a detailed response to this query: **'{search_query}'**. Make sure to cite all the sources at the end of your answer."
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": final_prompt},
        {"role": "user", "content": json.dumps(results)}],
    temperature=0

)
summary = response.choices[0].message.content

print(summary)
```

    Based on the search results provided, here is a chronological list of the latest OpenAI product launches from the past two years, ordered from the most recent to the oldest:
    
    1. **September 12, 2024**: **OpenAI o1**
       - A versatile AI tool designed to enhance reasoning capabilities.
       - Source: [OpenAI News](https://openai.com/news/)
    
    2. **July 25, 2024**: **SearchGPT**
       - A prototype aimed at enhancing AI-driven search capabilities.
       - Source: [OpenAI News](https://openai.com/news/)
    
    3. **July 18, 2024**: **GPT-4o mini**
       - A cost-efficient intelligence model.
       - Source: [OpenAI News](https://openai.com/news/)
    
    4. **May 2024**: **OpenAI for Education**
       - Focuses on integrating AI into educational settings.
       - Source: [OpenAI News](https://openai.com/news/product/)
    
    5. **February 15, 2024**: **Sora**
       - An AI model capable of generating high-quality text-to-video content.
       - Source: [OpenAI Sora](https://openai.com/index/sora/)
    
    6. **November 6, 2023**: **GPT-4 Turbo**
       - Features a 128K context window and enhanced multimodal capabilities.
       - Source: [OpenAI DevDay](https://openai.com/index/new-models-and-developer-products-announced-at-devday/)
    
    7. **November 6, 2023**: **GPTs**
       - Allows users to create customized versions of ChatGPT tailored to specific tasks.
       - Source: [OpenAI DevDay](https://openai.com/index/introducing-gpts/)
    
    8. **March 2023**: **GPT-4**
       - The first version of GPT-4 was released.
       - Source: [OpenAI DevDay](https://openai.com/index/new-models-and-developer-products-announced-at-devday/)
    
    9. **July 2023**: **GPT-4 General Availability**
       - GPT-4 was made generally available to all developers.
       - Source: [OpenAI DevDay](https://openai.com/index/new-models-and-developer-products-announced-at-devday/)
    
    10. **2023**: **Whisper v3**
        - An improved speech recognition model.
        - Source: [OpenAI DevDay](https://openai.com/index/new-models-and-developer-products-announced-at-devday/)
    
    11. **2023**: **DALL·E 3 Integration**
        - Enhanced capabilities for generating images from text prompts.
        - Source: [OpenAI DevDay](https://openai.com/index/new-models-and-developer-products-announced-at-devday/)
    
    12. **2023**: **Assistants API**
        - For developing specialized AI applications.
        - Source: [OpenAI DevDay](https://openai.com/index/new-models-and-developer-products-announced-at-devday/)
    
    13. **2023**: **Copyright Shield**
        - Legal protection for AI-generated content.
        - Source: [OpenAI DevDay](https://openai.com/index/new-models-and-developer-products-announced-at-devday/)
    
    14. **2023**: **OpenAI for Nonprofits**
        - Support for various sectors through AI.
        - Source: [OpenAI News](https://openai.com/news/)
    
    15. **2023**: **ChatGPT Edu**
        - Aimed at educational support.
        - Source: [OpenAI News](https://openai.com/news/)
    
    16. **2023**: **ChatGPT Enterprise**
        - New compliance tools and enhanced data analysis features.
        - Source: [OpenAI](https://openai.com/)
    
    17. **2023**: **OpenAI o1-mini**
        - A lightweight version of the OpenAI o1 model.
        - Source: [OpenAI](https://openai.com/)
    
    18. **2023**: **OpenAI o1-preview**
        - An early version of the OpenAI o1 model.
        - Source: [OpenAI](https://openai.com/api/)
    
    19. **2023**: **Custom Model Program**
        - Enhanced customization options for AI models.
        - Source: [OpenAI](https://openai.com/api/)
    
    20. **2023**: **Fine-tuning API Enhancements**
        - Improvements to the fine-tuning API.
        - Source: [OpenAI News](https://openai.com/news/)
    
    ### Sources:
    - [OpenAI News](https://openai.com/news/)
    - [OpenAI DevDay](https://openai.com/index/new-models-and-developer-products-announced-at-devday/)
    - [OpenAI Sora](https://openai.com/index/sora/)
    - [OpenAI API](https://openai.com/api/)
    - [OpenAI](https://openai.com/)
    

### Conclusion
 
Large Language Models (LLMs) have a knowledge cutoff and may not be aware of recent events. To provide them with the latest information, you can build a Bring Your Own Browser (BYOB) tool using Python. This tool retrieves current web data and feeds it to the LLM, enabling up-to-date responses.

The process involves three main steps:

**#1 Set Up a Search Engine:** Use a public search API, like Google's Custom Search API, to perform web searches and obtain a list of relevant search results.  

**#2 Build a Search Dictionary:** Collect the title, URL, and a summary of each web page from the search results to create a structured dictionary of information.  

**#3. Generate a RAG Response:** Implement Retrieval-Augmented Generation (RAG) by passing the gathered information to the LLM, which then generates a final response to the user's query.

By following these steps, you enhance the LLMs ability to provide up-to-date answers in your application that include the most recent developments, such as the latest product launches by OpenAI.




################################################## web_voyager.md ##################################################


# Web Voyager

[WebVoyager](https://arxiv.org/abs/2401.13919) by He, et. al., is a vision-enabled web-browsing agent capable of controlling the mouse and keyboard.

It works by viewing annotated browser screenshots for each turn, then choosing the next step to take. The agent architecture is a basic reasoning and action (ReAct) loop. 
The unique aspects of this agent are:
- It's usage of [Set-of-Marks](https://som-gpt4v.github.io/)-like image annotations to serve as UI affordances for the agent
- It's application in the browser by using tools to control both the mouse and keyboard

The overall design looks like the following:

<img src="./img/web-voyager.excalidraw.jpg" src="../img/web-voyager.excalidraw.jpg" >

## Setup

First, let's install our required packages:


```python
%%capture --no-stderr
%pip install -U --quiet langgraph langsmith langchain_openai
```


```python
import os
from getpass import getpass


def _getpass(env_var: str):
    if not os.environ.get(env_var):
        os.environ[env_var] = getpass(f"{env_var}=")


_getpass("OPENAI_API_KEY")
```

<div class="admonition tip">
    <p class="admonition-title">Set up <a href="https://smith.langchain.com">LangSmith</a> for LangGraph development</p>
    <p style="padding-top: 5px;">
        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href="https://docs.smith.langchain.com">here</a>. 
    </p>
</div>

#### Install Agent requirements

The only additional requirement we have is the [playwright](https://playwright.dev/) browser. Uncomment and install below:


```python
%pip install --upgrade --quiet  playwright > /dev/null
!playwright install
```


```python
import nest_asyncio

# This is just required for running async playwright in a Jupyter notebook
nest_asyncio.apply()
```

## Helper File

We will use some JS code for this tutorial, which you should place in a file called `mark_page.js` in the same directory as the notebook you are running this tutorial from.

<div>
  <button type="button" style="border: 1px solid black; border-radius: 5px; padding: 5px; background-color: lightgrey;" onclick="toggleVisibility('helper-functions')">Show/Hide JS Code</button>
  <div id="helper-functions" style="display:none;">
    <!-- Helper functions -->
    <pre>

    const customCSS = `
        ::-webkit-scrollbar {
            width: 10px;
        }
        ::-webkit-scrollbar-track {
            background: #27272a;
        }
        ::-webkit-scrollbar-thumb {
            background: #888;
            border-radius: 0.375rem;
        }
        ::-webkit-scrollbar-thumb:hover {
            background: #555;
        }
    `;

    const styleTag = document.createElement("style");
    styleTag.textContent = customCSS;
    document.head.append(styleTag);

    let labels = [];

    function unmarkPage() {
    // Unmark page logic
    for (const label of labels) {
        document.body.removeChild(label);
    }
    labels = [];
    }

    function markPage() {
    unmarkPage();

    var bodyRect = document.body.getBoundingClientRect();

    var items = Array.prototype.slice
        .call(document.querySelectorAll("*"))
        .map(function (element) {
        var vw = Math.max(
            document.documentElement.clientWidth || 0,
            window.innerWidth || 0
        );
        var vh = Math.max(
            document.documentElement.clientHeight || 0,
            window.innerHeight || 0
        );
        var textualContent = element.textContent.trim().replace(/\s{2,}/g, " ");
        var elementType = element.tagName.toLowerCase();
        var ariaLabel = element.getAttribute("aria-label") || "";

        var rects = [...element.getClientRects()]
            .filter((bb) => {
            var center_x = bb.left + bb.width / 2;
            var center_y = bb.top + bb.height / 2;
            var elAtCenter = document.elementFromPoint(center_x, center_y);

            return elAtCenter === element || element.contains(elAtCenter);
            })
            .map((bb) => {
            const rect = {
                left: Math.max(0, bb.left),
                top: Math.max(0, bb.top),
                right: Math.min(vw, bb.right),
                bottom: Math.min(vh, bb.bottom),
            };
            return {
                ...rect,
                width: rect.right - rect.left,
                height: rect.bottom - rect.top,
            };
            });

        var area = rects.reduce((acc, rect) => acc + rect.width * rect.height, 0);

        return {
            element: element,
            include:
            element.tagName === "INPUT" ||
            element.tagName === "TEXTAREA" ||
            element.tagName === "SELECT" ||
            element.tagName === "BUTTON" ||
            element.tagName === "A" ||
            element.onclick != null ||
            window.getComputedStyle(element).cursor == "pointer" ||
            element.tagName === "IFRAME" ||
            element.tagName === "VIDEO",
            area,
            rects,
            text: textualContent,
            type: elementType,
            ariaLabel: ariaLabel,
        };
        })
        .filter((item) => item.include && item.area >= 20);

    // Only keep inner clickable items
    items = items.filter(
        (x) => !items.some((y) => x.element.contains(y.element) && !(x == y))
    );

    // Function to generate random colors
    function getRandomColor() {
        var letters = "0123456789ABCDEF";
        var color = "#";
        for (var i = 0; i < 6; i++) {
        color += letters[Math.floor(Math.random() * 16)];
        }
        return color;
    }

    // Lets create a floating border on top of these elements that will always be visible
    items.forEach(function (item, index) {
        item.rects.forEach((bbox) => {
        newElement = document.createElement("div");
        var borderColor = getRandomColor();
        newElement.style.outline = `2px dashed ${borderColor}`;
        newElement.style.position = "fixed";
        newElement.style.left = bbox.left + "px";
        newElement.style.top = bbox.top + "px";
        newElement.style.width = bbox.width + "px";
        newElement.style.height = bbox.height + "px";
        newElement.style.pointerEvents = "none";
        newElement.style.boxSizing = "border-box";
        newElement.style.zIndex = 2147483647;
        // newElement.style.background = `${borderColor}80`;

        // Add floating label at the corner
        var label = document.createElement("span");
        label.textContent = index;
        label.style.position = "absolute";
        // These we can tweak if we want
        label.style.top = "-19px";
        label.style.left = "0px";
        label.style.background = borderColor;
        // label.style.background = "black";
        label.style.color = "white";
        label.style.padding = "2px 4px";
        label.style.fontSize = "12px";
        label.style.borderRadius = "2px";
        newElement.appendChild(label);

        document.body.appendChild(newElement);
        labels.push(newElement);
        // item.element.setAttribute("-ai-label", label.textContent);
        });
    });
    const coordinates = items.flatMap((item) =>
        item.rects.map(({ left, top, width, height }) => ({
        x: (left + left + width) / 2,
        y: (top + top + height) / 2,
        type: item.type,
        text: item.text,
        ariaLabel: item.ariaLabel,
        }))
    );
    return coordinates;
    }


</pre>
  </div>
</div>

<script>
  function toggleVisibility(id) {
    var element = document.getElementById(id);
    element.style.display = (element.style.display === "none") ? "block" : "none";
  }
</script>

## Define graph

### Define graph state

The state provides the inputs to each node in the graph.

In our case, the agent will track the webpage object (within the browser), annotated images + bounding boxes, the user's initial request, and the messages containing the agent scratchpad, system prompt, and other information.



```python
from typing import List, Optional
from typing_extensions import TypedDict

from langchain_core.messages import BaseMessage, SystemMessage
from playwright.async_api import Page


class BBox(TypedDict):
    x: float
    y: float
    text: str
    type: str
    ariaLabel: str


class Prediction(TypedDict):
    action: str
    args: Optional[List[str]]


# This represents the state of the agent
# as it proceeds through execution
class AgentState(TypedDict):
    page: Page  # The Playwright web page lets us interact with the web environment
    input: str  # User request
    img: str  # b64 encoded screenshot
    bboxes: List[BBox]  # The bounding boxes from the browser annotation function
    prediction: Prediction  # The Agent's output
    # A system message (or messages) containing the intermediate steps
    scratchpad: List[BaseMessage]
    observation: str  # The most recent response from a tool
```

### Define tools

The agent has 6 simple tools:

1. Click (at labeled box)
2. Type
3. Scroll
4. Wait
5. Go back
6. Go to search engine (Google)


We define them below here as functions:


```python
import asyncio
import platform


async def click(state: AgentState):
    # - Click [Numerical_Label]
    page = state["page"]
    click_args = state["prediction"]["args"]
    if click_args is None or len(click_args) != 1:
        return f"Failed to click bounding box labeled as number {click_args}"
    bbox_id = click_args[0]
    bbox_id = int(bbox_id)
    try:
        bbox = state["bboxes"][bbox_id]
    except Exception:
        return f"Error: no bbox for : {bbox_id}"
    x, y = bbox["x"], bbox["y"]
    await page.mouse.click(x, y)
    # TODO: In the paper, they automatically parse any downloaded PDFs
    # We could add something similar here as well and generally
    # improve response format.
    return f"Clicked {bbox_id}"


async def type_text(state: AgentState):
    page = state["page"]
    type_args = state["prediction"]["args"]
    if type_args is None or len(type_args) != 2:
        return (
            f"Failed to type in element from bounding box labeled as number {type_args}"
        )
    bbox_id = type_args[0]
    bbox_id = int(bbox_id)
    bbox = state["bboxes"][bbox_id]
    x, y = bbox["x"], bbox["y"]
    text_content = type_args[1]
    await page.mouse.click(x, y)
    # Check if MacOS
    select_all = "Meta+A" if platform.system() == "Darwin" else "Control+A"
    await page.keyboard.press(select_all)
    await page.keyboard.press("Backspace")
    await page.keyboard.type(text_content)
    await page.keyboard.press("Enter")
    return f"Typed {text_content} and submitted"


async def scroll(state: AgentState):
    page = state["page"]
    scroll_args = state["prediction"]["args"]
    if scroll_args is None or len(scroll_args) != 2:
        return "Failed to scroll due to incorrect arguments."

    target, direction = scroll_args

    if target.upper() == "WINDOW":
        # Not sure the best value for this:
        scroll_amount = 500
        scroll_direction = (
            -scroll_amount if direction.lower() == "up" else scroll_amount
        )
        await page.evaluate(f"window.scrollBy(0, {scroll_direction})")
    else:
        # Scrolling within a specific element
        scroll_amount = 200
        target_id = int(target)
        bbox = state["bboxes"][target_id]
        x, y = bbox["x"], bbox["y"]
        scroll_direction = (
            -scroll_amount if direction.lower() == "up" else scroll_amount
        )
        await page.mouse.move(x, y)
        await page.mouse.wheel(0, scroll_direction)

    return f"Scrolled {direction} in {'window' if target.upper() == 'WINDOW' else 'element'}"


async def wait(state: AgentState):
    sleep_time = 5
    await asyncio.sleep(sleep_time)
    return f"Waited for {sleep_time}s."


async def go_back(state: AgentState):
    page = state["page"]
    await page.go_back()
    return f"Navigated back a page to {page.url}."


async def to_google(state: AgentState):
    page = state["page"]
    await page.goto("https://www.google.com/")
    return "Navigated to google.com."
```

### Define Agent

The agent is driven by a multi-modal model and decides the action to take for each step. It is composed of a few runnable objects:

1. A `mark_page` function to annotate the current page with bounding boxes
2. A prompt to hold the user question, annotated image, and agent scratchpad
3. GPT-4V to decide the next steps
4. Parsing logic to extract the action


Let's first define the annotation step:
#### Browser Annotations

This function annotates all buttons, inputs, text areas, etc. with numbered bounding boxes. GPT-4V then just has to refer to a bounding box
when taking actions, reducing the complexity of the overall task.


```python
import base64

from langchain_core.runnables import chain as chain_decorator

# Some javascript we will run on each step
# to take a screenshot of the page, select the
# elements to annotate, and add bounding boxes
with open("mark_page.js") as f:
    mark_page_script = f.read()


@chain_decorator
async def mark_page(page):
    await page.evaluate(mark_page_script)
    for _ in range(10):
        try:
            bboxes = await page.evaluate("markPage()")
            break
        except Exception:
            # May be loading...
            asyncio.sleep(3)
    screenshot = await page.screenshot()
    # Ensure the bboxes don't follow us around
    await page.evaluate("unmarkPage()")
    return {
        "img": base64.b64encode(screenshot).decode(),
        "bboxes": bboxes,
    }
```

#### Agent definition

Now we'll compose this function with the prompt, llm and output parser to complete our agent.


```python
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI


async def annotate(state):
    marked_page = await mark_page.with_retry().ainvoke(state["page"])
    return {**state, **marked_page}


def format_descriptions(state):
    labels = []
    for i, bbox in enumerate(state["bboxes"]):
        text = bbox.get("ariaLabel") or ""
        if not text.strip():
            text = bbox["text"]
        el_type = bbox.get("type")
        labels.append(f'{i} (<{el_type}/>): "{text}"')
    bbox_descriptions = "\nValid Bounding Boxes:\n" + "\n".join(labels)
    return {**state, "bbox_descriptions": bbox_descriptions}


def parse(text: str) -> dict:
    action_prefix = "Action: "
    if not text.strip().split("\n")[-1].startswith(action_prefix):
        return {"action": "retry", "args": f"Could not parse LLM Output: {text}"}
    action_block = text.strip().split("\n")[-1]

    action_str = action_block[len(action_prefix) :]
    split_output = action_str.split(" ", 1)
    if len(split_output) == 1:
        action, action_input = split_output[0], None
    else:
        action, action_input = split_output
    action = action.strip()
    if action_input is not None:
        action_input = [
            inp.strip().strip("[]") for inp in action_input.strip().split(";")
        ]
    return {"action": action, "args": action_input}


# Will need a later version of langchain to pull
# this image prompt template
prompt = hub.pull("wfh/web-voyager")
```


```python
llm = ChatOpenAI(model="gpt-4-vision-preview", max_tokens=4096)
agent = annotate | RunnablePassthrough.assign(
    prediction=format_descriptions | prompt | llm | StrOutputParser() | parse
)
```

## Compile the graph

We've created most of the important logic. We have one more function to define that will help us update the graph state after a tool is called.


```python
import re


def update_scratchpad(state: AgentState):
    """After a tool is invoked, we want to update
    the scratchpad so the agent is aware of its previous steps"""
    old = state.get("scratchpad")
    if old:
        txt = old[0].content
        last_line = txt.rsplit("\n", 1)[-1]
        step = int(re.match(r"\d+", last_line).group()) + 1
    else:
        txt = "Previous action observations:\n"
        step = 1
    txt += f"\n{step}. {state['observation']}"

    return {**state, "scratchpad": [SystemMessage(content=txt)]}
```

Now we can compose everything into a graph:


```python
from langchain_core.runnables import RunnableLambda

from langgraph.graph import END, START, StateGraph

graph_builder = StateGraph(AgentState)


graph_builder.add_node("agent", agent)
graph_builder.add_edge(START, "agent")

graph_builder.add_node("update_scratchpad", update_scratchpad)
graph_builder.add_edge("update_scratchpad", "agent")

tools = {
    "Click": click,
    "Type": type_text,
    "Scroll": scroll,
    "Wait": wait,
    "GoBack": go_back,
    "Google": to_google,
}


for node_name, tool in tools.items():
    graph_builder.add_node(
        node_name,
        # The lambda ensures the function's string output is mapped to the "observation"
        # key in the AgentState
        RunnableLambda(tool) | (lambda observation: {"observation": observation}),
    )
    # Always return to the agent (by means of the update-scratchpad node)
    graph_builder.add_edge(node_name, "update_scratchpad")


def select_tool(state: AgentState):
    # Any time the agent completes, this function
    # is called to route the output to a tool or
    # to the end user.
    action = state["prediction"]["action"]
    if action == "ANSWER":
        return END
    if action == "retry":
        return "agent"
    return action


graph_builder.add_conditional_edges("agent", select_tool)

graph = graph_builder.compile()
```

## Use the graph

Now that we've created the whole agent executor, we can run it on a few questions! We'll start our browser at "google.com" and then let it control the rest.

Below is a helper function to help print out the steps to the notebook (and display the intermediate screenshots).


```python
from IPython import display
from playwright.async_api import async_playwright

browser = await async_playwright().start()
# We will set headless=False so we can watch the agent navigate the web.
browser = await browser.chromium.launch(headless=False, args=None)
page = await browser.new_page()
_ = await page.goto("https://www.google.com")


async def call_agent(question: str, page, max_steps: int = 150):
    event_stream = graph.astream(
        {
            "page": page,
            "input": question,
            "scratchpad": [],
        },
        {
            "recursion_limit": max_steps,
        },
    )
    final_answer = None
    steps = []
    async for event in event_stream:
        # We'll display an event stream here
        if "agent" not in event:
            continue
        pred = event["agent"].get("prediction") or {}
        action = pred.get("action")
        action_input = pred.get("args")
        display.clear_output(wait=False)
        steps.append(f"{len(steps) + 1}. {action}: {action_input}")
        print("\n".join(steps))
        display.display(display.Image(base64.b64decode(event["agent"]["img"])))
        if "ANSWER" in action:
            final_answer = action_input[0]
            break
    return final_answer
```


```python
res = await call_agent("Could you explain the WebVoyager paper (on arxiv)?", page)
print(f"Final response: {res}")
```

    1. Type: ['7', 'WebVoyager paper arXiv']
    2. Click: ['32']
    3. Click: ['3']
    4. ANSWER;: ['The "WebVoyager" paper discusses the development of an end-to-end web agent that leverages large multimodal models. The abstract highlights the importance of such agents in automating complex tasks on the web, which remains a challenging domain due to the heterogeneity in structure and the semantic gap between humans and machines. The paper proposes a solution that combines neural symbolic models and multimodal web environments, aiming to advance the capabilities of these agents to perform web browsing tasks effectively. Further details would require a more in-depth analysis of the paper\'s content beyond the abstract.']
    


    
![png](output_23_1.png)
    


    Final response: The "WebVoyager" paper discusses the development of an end-to-end web agent that leverages large multimodal models. The abstract highlights the importance of such agents in automating complex tasks on the web, which remains a challenging domain due to the heterogeneity in structure and the semantic gap between humans and machines. The paper proposes a solution that combines neural symbolic models and multimodal web environments, aiming to advance the capabilities of these agents to perform web browsing tasks effectively. Further details would require a more in-depth analysis of the paper's content beyond the abstract.
    


```python
res = await call_agent(
    "Please explain the today's XKCD comic for me. Why is it funny?", page
)
print(f"Final response: {res}")
```

    1. retry: Could not parse LLM Output: I'm sorry, but the image provided does not contain an XKCD comic. The image shows a page from a scientific paper titled "WebVoyager 2: Building an End-to-End Web Agent with Large Multimodal Models." If you provide the XKCD comic you're referring to, I'd be happy to explain the humor in it.
    2. retry: Could not parse LLM Output: I'm sorry, but I cannot assist with that request.
    3. Google: None
    4. Type: ['6', 'xkcd.com']
    5. Click: ['25']
    6. ANSWER;: ['The XKCD comic titled "Relationship Advice" pokes fun at the sometimes exaggerated way people talk about the challenges of relationships. It starts with one character stating that relationships require constant work and are like a job, which is a common sentiment. However, the other character takes this comparison to an extreme, calling it a "grueling ordeal" and a "crushing burden," which humorously exaggerates the difficulties of maintaining a relationship. The punchline comes when, after this escalation, the second character insists they\'re fine and that it\'s all normal, which satirizes how people might downplay their struggles to appear in control or deny the extent of their challenges. The humor lies in the hyperbole and the relatable nature of discussing relationship difficulties, as well as the contrast between the characters\' statements and the insistence that everything is okay.']
    


    
![png](output_24_1.png)
    


    Final response: The XKCD comic titled "Relationship Advice" pokes fun at the sometimes exaggerated way people talk about the challenges of relationships. It starts with one character stating that relationships require constant work and are like a job, which is a common sentiment. However, the other character takes this comparison to an extreme, calling it a "grueling ordeal" and a "crushing burden," which humorously exaggerates the difficulties of maintaining a relationship. The punchline comes when, after this escalation, the second character insists they're fine and that it's all normal, which satirizes how people might downplay their struggles to appear in control or deny the extent of their challenges. The humor lies in the hyperbole and the relatable nature of discussing relationship difficulties, as well as the contrast between the characters' statements and the insistence that everything is okay.
    


```python
res = await call_agent("What are the latest blog posts from langchain?", page)
print(f"Final response: {res}")
```

    1. Google: None
    2. Type: ['6', 'latest blog posts from langchain']
    3. Click: ['27']
    4. Click: ['14']
    5. Click: ['0']
    6. retry: Could not parse LLM Output: Thought: The latest blog posts from Langchain are displayed on the right side of the screen with titles and reading time. I will provide the titles of the featured blog posts as seen on the screen.
    
    Action: ANSWER; The latest blog posts from Langchain are:
    1. OpenGPTs - 7 min read
    2. LangGraph: Multi-Agent Workflows - 6 min read
    3. LangGraph - 7 min read
    4. LangChain v0.1.0 - 10 min read
    7. ANSWER;: ['The latest blog posts from Langchain are "OpenGPTs," "LangGraph: Multi-Agent Workflows," and "LangGraph."']
    


    
![png](output_25_1.png)
    


    Final response: The latest blog posts from Langchain are "OpenGPTs," "LangGraph: Multi-Agent Workflows," and "LangGraph."
    


```python
res = await call_agent(
    "Could you check google maps to see when i should leave to get to SFO by 7 o'clock? starting from SF downtown.",
    page,
)
print(f"Final response: {res}")
```

    1. Google: None
    2. Type: ['6', 'Google Maps']
    3. Click: ['0']
    4. Click: ['0']
    5. Wait: None
    6. Click: ['22']
    7. Click: ['0']
    8. Click: ['2']
    9. Type: ['0', 'San Francisco downtown to SFO']
    10. Click: ['1']
    11. Click: ['2']
    12. Type: ['8', 'San Francisco International Airport SFO']
    13. Click: ['14']
    14. Click: ['28']
    15. Scroll: ['WINDOW', 'up']
    16. Scroll: ['WINDOW', 'up']
    17. Click: ['10']
    18. Click: ['28']
    19. ANSWER;: ['To arrive at San Francisco International Airport (SFO) by 7:00 AM starting from downtown San Francisco, you should leave by 6:46 AM according to the current Google Maps information, which estimates a 44-minute travel time.']
    


    
![png](output_26_1.png)
    


    Final response: To arrive at San Francisco International Airport (SFO) by 7:00 AM starting from downtown San Francisco, you should leave by 6:46 AM according to the current Google Maps information, which estimates a 44-minute travel time.
    




################################################## wechat.md ##################################################


# WeChat

There is not yet a straightforward way to export personal WeChat messages. However if you just need no more than few hundreds of messages for model fine-tuning or few-shot examples, this notebook shows how to create your own chat loader that works on copy-pasted WeChat messages to a list of LangChain messages.

> Highly inspired by https://python.langchain.com/docs/integrations/chat_loaders/discord


The process has five steps:
1. Open your chat in the WeChat desktop app. Select messages you need by mouse-dragging or right-click. Due to restrictions, you can select up to 100 messages once a time. `CMD`/`Ctrl` + `C` to copy.
2. Create the chat .txt file by pasting selected messages in a file on your local computer.
3. Copy the chat loader definition from below to a local file.
4. Initialize the `WeChatChatLoader` with the file path pointed to the text file.
5. Call `loader.load()` (or `loader.lazy_load()`) to perform the conversion.

## 1. Create message dump

This loader only supports .txt files in the format generated by copying messages in the app to your clipboard and pasting in a file. Below is an example.


```python
%%writefile wechat_chats.txt
女朋友 2023/09/16 2:51 PM
天气有点凉

男朋友 2023/09/16 2:51 PM
珍簟凉风著，瑶琴寄恨生。嵇君懒书札，底物慰秋情。

女朋友 2023/09/16 3:06 PM
忙什么呢

男朋友 2023/09/16 3:06 PM
今天只干成了一件像样的事
那就是想你

女朋友 2023/09/16 3:06 PM
[动画表情]
```

    Overwriting wechat_chats.txt
    

## 2. Define chat loader

LangChain currently does not support 


```python
import logging
import re
from typing import Iterator, List

from langchain_community.chat_loaders import base as chat_loaders
from langchain_core.messages import BaseMessage, HumanMessage

logger = logging.getLogger()


class WeChatChatLoader(chat_loaders.BaseChatLoader):
    def __init__(self, path: str):
        """
        Initialize the Discord chat loader.

        Args:
            path: Path to the exported Discord chat text file.
        """
        self.path = path
        self._message_line_regex = re.compile(
            r"(?P<sender>.+?) (?P<timestamp>\d{4}/\d{2}/\d{2} \d{1,2}:\d{2} (?:AM|PM))",
            # flags=re.DOTALL,
        )

    def _append_message_to_results(
        self,
        results: List,
        current_sender: str,
        current_timestamp: str,
        current_content: List[str],
    ):
        content = "\n".join(current_content).strip()
        # skip non-text messages like stickers, images, etc.
        if not re.match(r"\[.*\]", content):
            results.append(
                HumanMessage(
                    content=content,
                    additional_kwargs={
                        "sender": current_sender,
                        "events": [{"message_time": current_timestamp}],
                    },
                )
            )
        return results

    def _load_single_chat_session_from_txt(
        self, file_path: str
    ) -> chat_loaders.ChatSession:
        """
        Load a single chat session from a text file.

        Args:
            file_path: Path to the text file containing the chat messages.

        Returns:
            A `ChatSession` object containing the loaded chat messages.
        """
        with open(file_path, "r", encoding="utf-8") as file:
            lines = file.readlines()

        results: List[BaseMessage] = []
        current_sender = None
        current_timestamp = None
        current_content = []
        for line in lines:
            if re.match(self._message_line_regex, line):
                if current_sender and current_content:
                    results = self._append_message_to_results(
                        results, current_sender, current_timestamp, current_content
                    )
                current_sender, current_timestamp = re.match(
                    self._message_line_regex, line
                ).groups()
                current_content = []
            else:
                current_content.append(line.strip())

        if current_sender and current_content:
            results = self._append_message_to_results(
                results, current_sender, current_timestamp, current_content
            )

        return chat_loaders.ChatSession(messages=results)

    def lazy_load(self) -> Iterator[chat_loaders.ChatSession]:
        """
        Lazy load the messages from the chat file and yield them in the required format.

        Yields:
            A `ChatSession` object containing the loaded chat messages.
        """
        yield self._load_single_chat_session_from_txt(self.path)
```

## 2. Create loader

We will point to the file we just wrote to disk.


```python
loader = WeChatChatLoader(
    path="./wechat_chats.txt",
)
```

## 3. Load Messages

Assuming the format is correct, the loader will convert the chats to langchain messages.


```python
from typing import List

from langchain_community.chat_loaders.utils import (
    map_ai_messages,
    merge_chat_runs,
)
from langchain_core.chat_sessions import ChatSession

raw_messages = loader.lazy_load()
# Merge consecutive messages from the same sender into a single message
merged_messages = merge_chat_runs(raw_messages)
# Convert messages from "男朋友" to AI messages
messages: List[ChatSession] = list(map_ai_messages(merged_messages, sender="男朋友"))
```


```python
messages
```




    [{'messages': [HumanMessage(content='天气有点凉', additional_kwargs={'sender': '女朋友', 'events': [{'message_time': '2023/09/16 2:51 PM'}]}, example=False),
       AIMessage(content='珍簟凉风著，瑶琴寄恨生。嵇君懒书札，底物慰秋情。', additional_kwargs={'sender': '男朋友', 'events': [{'message_time': '2023/09/16 2:51 PM'}]}, example=False),
       HumanMessage(content='忙什么呢', additional_kwargs={'sender': '女朋友', 'events': [{'message_time': '2023/09/16 3:06 PM'}]}, example=False),
       AIMessage(content='今天只干成了一件像样的事\n那就是想你', additional_kwargs={'sender': '男朋友', 'events': [{'message_time': '2023/09/16 3:06 PM'}]}, example=False)]}]



### Next Steps

You can then use these messages how you see fit, such as fine-tuning a model, few-shot example selection, or directly make predictions for the next message  


```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

for chunk in llm.stream(messages[0]["messages"]):
    print(chunk.content, end="", flush=True)
```


```python

```




################################################## weight_only_quantization.md ##################################################


# Intel Weight-Only Quantization
## Weight-Only Quantization for Huggingface Models with Intel Extension for Transformers Pipelines

Hugging Face models can be run locally with Weight-Only quantization through the `WeightOnlyQuantPipeline` class.

The [Hugging Face Model Hub](https://huggingface.co/models) hosts over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.

These can be called from LangChain through this local pipeline wrapper class.

To use, you should have the ``transformers`` python [package installed](https://pypi.org/project/transformers/), as well as [pytorch](https://pytorch.org/get-started/locally/), [intel-extension-for-transformers](https://github.com/intel/intel-extension-for-transformers).


```python
%pip install transformers --quiet
%pip install intel-extension-for-transformers
```

### Model Loading

Models can be loaded by specifying the model parameters using the `from_model_id` method. The model parameters include `WeightOnlyQuantConfig` class in intel_extension_for_transformers.


```python
from intel_extension_for_transformers.transformers import WeightOnlyQuantConfig
from langchain_community.llms.weight_only_quantization import WeightOnlyQuantPipeline

conf = WeightOnlyQuantConfig(weight_dtype="nf4")
hf = WeightOnlyQuantPipeline.from_model_id(
    model_id="google/flan-t5-large",
    task="text2text-generation",
    quantization_config=conf,
    pipeline_kwargs={"max_new_tokens": 10},
)
```

They can also be loaded by passing in an existing `transformers` pipeline directly


```python
from intel_extension_for_transformers.transformers import AutoModelForSeq2SeqLM
from transformers import AutoTokenizer, pipeline

model_id = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id)
pipe = pipeline(
    "text2text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10
)
hf = WeightOnlyQuantPipeline(pipeline=pipe)
```

### Create Chain

With the model loaded into memory, you can compose it with a prompt to
form a chain.


```python
from langchain_core.prompts import PromptTemplate

template = """Question: {question}

Answer: Let's think step by step."""
prompt = PromptTemplate.from_template(template)

chain = prompt | hf

question = "What is electroencephalography?"

print(chain.invoke({"question": question}))
```

### CPU Inference

Now intel-extension-for-transformers only support CPU device inference. Will support intel GPU soon.When running on a machine with CPU, you can specify the `device="cpu"` or `device=-1` parameter to put the model on CPU device.
Defaults to `-1` for CPU inference.


```python
conf = WeightOnlyQuantConfig(weight_dtype="nf4")
llm = WeightOnlyQuantPipeline.from_model_id(
    model_id="google/flan-t5-large",
    task="text2text-generation",
    quantization_config=conf,
    pipeline_kwargs={"max_new_tokens": 10},
)

template = """Question: {question}

Answer: Let's think step by step."""
prompt = PromptTemplate.from_template(template)

chain = prompt | llm

question = "What is electroencephalography?"

print(chain.invoke({"question": question}))
```

### Batch CPU Inference

You can also run inference on the CPU in batch mode.


```python
conf = WeightOnlyQuantConfig(weight_dtype="nf4")
llm = WeightOnlyQuantPipeline.from_model_id(
    model_id="google/flan-t5-large",
    task="text2text-generation",
    quantization_config=conf,
    pipeline_kwargs={"max_new_tokens": 10},
)

chain = prompt | llm.bind(stop=["\n\n"])

questions = []
for i in range(4):
    questions.append({"question": f"What is the number {i} in french?"})

answers = chain.batch(questions)
for answer in answers:
    print(answer)
```

### Data Types Supported by Intel-extension-for-transformers

We support quantize the weights to following data types for storing(weight_dtype in WeightOnlyQuantConfig):

* **int8**: Uses 8-bit data type.
* **int4_fullrange**: Uses the -8 value of int4 range compared with the normal int4 range [-7,7].
* **int4_clip**: Clips and retains the values within the int4 range, setting others to zero.
* **nf4**: Uses the normalized float 4-bit data type.
* **fp4_e2m1**: Uses regular float 4-bit data type. "e2" means that 2 bits are used for the exponent, and "m1" means that 1 bits are used for the mantissa.

While these techniques store weights in 4 or 8 bit, the computation still happens in float32, bfloat16 or int8(compute_dtype in WeightOnlyQuantConfig):
* **fp32**: Uses the float32 data type to compute.
* **bf16**: Uses the bfloat16 data type to compute.
* **int8**: Uses 8-bit data type to compute.

### Supported Algorithms Matrix

Quantization algorithms supported in intel-extension-for-transformers(algorithm in WeightOnlyQuantConfig):

| Algorithms |   PyTorch  |    LLM Runtime    |
|:--------------:|:----------:|:----------:|
|       RTN      |  &#10004;  |  &#10004;  |
|       AWQ      |  &#10004;  | stay tuned |
|      TEQ      | &#10004; | stay tuned |
> **RTN:** A quantification method that we can think of very intuitively. It does not require additional datasets and is a very fast quantization method. Generally speaking, RTN will convert the weight into a uniformly distributed integer data type, but some algorithms, such as Qlora, propose a non-uniform NF4 data type and prove its theoretical optimality.

> **AWQ:** Proved that protecting only 1% of salient weights can greatly reduce quantization error. the salient weight channels are selected by observing the distribution of activation and weight per channel. The salient weights are also quantized after multiplying a big scale factor before quantization for preserving.

> **TEQ:** A trainable equivalent transformation that preserves the FP32 precision in weight-only quantization. It is inspired by AWQ while providing a new solution to search for the optimal per-channel scaling factor between activations and weights.





################################################## whatsapp.md ##################################################


# WhatsApp

This notebook shows how to use the WhatsApp chat loader. This class helps map exported WhatsApp conversations to LangChain chat messages.

The process has three steps:
1. Export the chat conversations to computer
2. Create the `WhatsAppChatLoader` with the file path pointed to the json file or directory of JSON files
3. Call `loader.load()` (or `loader.lazy_load()`) to perform the conversion.

## 1. Create message dump

To make the export of your WhatsApp conversation(s), complete the following steps:

1. Open the target conversation
2. Click the three dots in the top right corner and select "More".
3. Then select "Export chat" and choose "Without media".

An example of the data format for each conversation is below: 


```python
%%writefile whatsapp_chat.txt
[8/15/23, 9:12:33 AM] Dr. Feather: ‎Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them.
[8/15/23, 9:12:43 AM] Dr. Feather: I spotted a rare Hyacinth Macaw yesterday in the Amazon Rainforest. Such a magnificent creature!
‎[8/15/23, 9:12:48 AM] Dr. Feather: ‎image omitted
[8/15/23, 9:13:15 AM] Jungle Jane: That's stunning! Were you able to observe its behavior?
‎[8/15/23, 9:13:23 AM] Dr. Feather: ‎image omitted
[8/15/23, 9:14:02 AM] Dr. Feather: Yes, it seemed quite social with other macaws. They're known for their playful nature.
[8/15/23, 9:14:15 AM] Jungle Jane: How's the research going on parrot communication?
‎[8/15/23, 9:14:30 AM] Dr. Feather: ‎image omitted
[8/15/23, 9:14:50 AM] Dr. Feather: It's progressing well. We're learning so much about how they use sound and color to communicate.
[8/15/23, 9:15:10 AM] Jungle Jane: That's fascinating! Can't wait to read your paper on it.
[8/15/23, 9:15:20 AM] Dr. Feather: Thank you! I'll send you a draft soon.
[8/15/23, 9:25:16 PM] Jungle Jane: Looking forward to it! Keep up the great work.
```

    Writing whatsapp_chat.txt
    

## 2. Create the Chat Loader

The WhatsAppChatLoader accepts the resulting zip file, unzipped directory, or the path to any of the chat `.txt` files therein.

Provide that as well as the user name you want to take on the role of "AI" when fine-tuning.


```python
from langchain_community.chat_loaders.whatsapp import WhatsAppChatLoader
```


```python
loader = WhatsAppChatLoader(
    path="./whatsapp_chat.txt",
)
```

## 3. Load messages

The `load()` (or `lazy_load`) methods return a list of "ChatSessions" that currently store the list of messages per loaded conversation.


```python
from typing import List

from langchain_community.chat_loaders.utils import (
    map_ai_messages,
    merge_chat_runs,
)
from langchain_core.chat_sessions import ChatSession

raw_messages = loader.lazy_load()
# Merge consecutive messages from the same sender into a single message
merged_messages = merge_chat_runs(raw_messages)
# Convert messages from "Dr. Feather" to AI messages
messages: List[ChatSession] = list(
    map_ai_messages(merged_messages, sender="Dr. Feather")
)
```




    [{'messages': [AIMessage(content='I spotted a rare Hyacinth Macaw yesterday in the Amazon Rainforest. Such a magnificent creature!', additional_kwargs={'sender': 'Dr. Feather', 'events': [{'message_time': '8/15/23, 9:12:43 AM'}]}, example=False),
       HumanMessage(content="That's stunning! Were you able to observe its behavior?", additional_kwargs={'sender': 'Jungle Jane', 'events': [{'message_time': '8/15/23, 9:13:15 AM'}]}, example=False),
       AIMessage(content="Yes, it seemed quite social with other macaws. They're known for their playful nature.", additional_kwargs={'sender': 'Dr. Feather', 'events': [{'message_time': '8/15/23, 9:14:02 AM'}]}, example=False),
       HumanMessage(content="How's the research going on parrot communication?", additional_kwargs={'sender': 'Jungle Jane', 'events': [{'message_time': '8/15/23, 9:14:15 AM'}]}, example=False),
       AIMessage(content="It's progressing well. We're learning so much about how they use sound and color to communicate.", additional_kwargs={'sender': 'Dr. Feather', 'events': [{'message_time': '8/15/23, 9:14:50 AM'}]}, example=False),
       HumanMessage(content="That's fascinating! Can't wait to read your paper on it.", additional_kwargs={'sender': 'Jungle Jane', 'events': [{'message_time': '8/15/23, 9:15:10 AM'}]}, example=False),
       AIMessage(content="Thank you! I'll send you a draft soon.", additional_kwargs={'sender': 'Dr. Feather', 'events': [{'message_time': '8/15/23, 9:15:20 AM'}]}, example=False),
       HumanMessage(content='Looking forward to it! Keep up the great work.', additional_kwargs={'sender': 'Jungle Jane', 'events': [{'message_time': '8/15/23, 9:25:16 PM'}]}, example=False)]}]



### Next Steps

You can then use these messages how you see fit, such as fine-tuning a model, few-shot example selection, or directly make predictions for the next message.


```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

for chunk in llm.stream(messages[0]["messages"]):
    print(chunk.content, end="", flush=True)
```

    Thank you for the encouragement! I'll do my best to continue studying and sharing fascinating insights about parrot communication.


```python

```




################################################## whatsapp_chat.md ##################################################


# WhatsApp Chat

>[WhatsApp](https://www.whatsapp.com/) (also called `WhatsApp Messenger`) is a freeware, cross-platform, centralized instant messaging (IM) and voice-over-IP (VoIP) service. It allows users to send text and voice messages, make voice and video calls, and share images, documents, user locations, and other content.

This notebook covers how to load data from the `WhatsApp Chats` into a format that can be ingested into LangChain.


```python
from langchain_community.document_loaders import WhatsAppChatLoader
```


```python
loader = WhatsAppChatLoader("example_data/whatsapp_chat.txt")
```


```python
loader.load()
```




################################################## whisper.md ##################################################


# Azure audio whisper (preview) example

> Note: There is a newer version of the openai library available. See https://github.com/openai/openai-python/discussions/742

The example shows how to use the Azure OpenAI Whisper model to transcribe audio files.

## Setup

First, we install the necessary dependencies.


```python
! pip install "openai>=0.28.1,<1.0.0"
! pip install python-dotenv
```

Next, we'll import our libraries and configure the Python OpenAI SDK to work with the Azure OpenAI service.

> Note: In this example, we configured the library to use the Azure API by setting the variables in code. For development, consider setting the environment variables instead:

```
OPENAI_API_BASE
OPENAI_API_KEY
OPENAI_API_TYPE
OPENAI_API_VERSION
```


```python
import os
import dotenv
import openai


dotenv.load_dotenv()
```




    True




To properly access the Azure OpenAI Service, we need to create the proper resources at the [Azure Portal](https://portal.azure.com) (you can check a detailed guide on how to do this in the [Microsoft Docs](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal))

Once the resource is created, the first thing we need to use is its endpoint. You can get the endpoint by looking at the *"Keys and Endpoints"* section under the *"Resource Management"* section. Having this, we will set up the SDK using this information:


```python
openai.api_base = os.environ["OPENAI_API_BASE"]

# Min API version that supports Whisper
openai.api_version = "2023-09-01-preview"

# Enter the deployment_id to use for the Whisper model
deployment_id = "<deployment-id-for-your-whisper-model>"
```

### Authentication

The Azure OpenAI service supports multiple authentication mechanisms that include API keys and Azure credentials.


```python
# set to True if using Azure Active Directory authentication
use_azure_active_directory = False
```


#### Authentication using API key

To set up the OpenAI SDK to use an *Azure API Key*, we need to set up the `api_type` to `azure` and set `api_key` to a key associated with your endpoint (you can find this key in *"Keys and Endpoints"* under *"Resource Management"* in the [Azure Portal](https://portal.azure.com))


```python
if not use_azure_active_directory:
    openai.api_type = 'azure'
    openai.api_key = os.environ["OPENAI_API_KEY"]
```

#### Authentication using Azure Active Directory
Let's now see how we can get a key via Microsoft Active Directory Authentication.


```python
from azure.identity import DefaultAzureCredential

if use_azure_active_directory:
    default_credential = DefaultAzureCredential()
    token = default_credential.get_token("https://cognitiveservices.azure.com/.default")

    openai.api_type = 'azure_ad'
    openai.api_key = token.token
```

A token is valid for a period of time, after which it will expire. To ensure a valid token is sent with every request, you can refresh an expiring token by hooking into requests.auth:


```python
import typing
import time
import requests

if typing.TYPE_CHECKING:
    from azure.core.credentials import TokenCredential

class TokenRefresh(requests.auth.AuthBase):

    def __init__(self, credential: "TokenCredential", scopes: typing.List[str]) -> None:
        self.credential = credential
        self.scopes = scopes
        self.cached_token: typing.Optional[str] = None

    def __call__(self, req):
        if not self.cached_token or self.cached_token.expires_on - time.time() < 300:
            self.cached_token = self.credential.get_token(*self.scopes)
        req.headers["Authorization"] = f"Bearer {self.cached_token.token}"
        return req

if use_azure_active_directory:
    session = requests.Session()
    session.auth = TokenRefresh(default_credential, ["https://cognitiveservices.azure.com/.default"])

    openai.requestssession = session
```

## Audio transcription

Audio transcription, or speech-to-text, is the process of converting spoken words into text. Use the `openai.Audio.transcribe` method to transcribe an audio file stream to text.

You can get sample audio files from the [Azure AI Speech SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).


```python
# download sample audio file
import requests

sample_audio_url = "https://github.com/Azure-Samples/cognitive-services-speech-sdk/raw/master/sampledata/audiofiles/wikipediaOcelot.wav"
audio_file = requests.get(sample_audio_url)
with open("wikipediaOcelot.wav", "wb") as f:
    f.write(audio_file.content)
```


```python
transcription = openai.Audio.transcribe(
    file=open("wikipediaOcelot.wav", "rb"),
    model="whisper-1",
    deployment_id=deployment_id,
)
print(transcription.text)
```




################################################## Whisper_correct_misspelling.md ##################################################


# Addressing transcription misspellings: prompt vs post-processing

We are addressing the problem of enhancing the precision of transcriptions, particularly when it comes to company names and product references. Our solution involves a dual strategy that utilizes both the Whisper prompt parameter and GPT-4's post-processing capabilities. 

Two approaches to correct inaccuracies are:

- We input a list of correct spellings directly into Whisper's prompt parameter to guide the initial transcription.

- We utilized GPT-4 to fix misspellings post transcription, again using the same list of correct spellings in the prompt.

These strategies aimed at ensuring precise transcription of unfamilar proper nouns.

## Setup

To get started, let's:

- Import the OpenAI Python library (if you don't have it, you'll need to install it with ```pip install openai```)
- Download the audio file example


```python
# imports
from openai import OpenAI  # for making OpenAI API calls
import urllib  # for downloading example audio files
import os  # for accessing environment variables

client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "<your OpenAI API key if not set as env var>"))
```


```python
# set download paths
ZyntriQix_remote_filepath = "https://cdn.openai.com/API/examples/data/ZyntriQix.wav"


# set local save locations
ZyntriQix_filepath = "data/ZyntriQix.wav"

# download example audio files and save locally
urllib.request.urlretrieve(ZyntriQix_remote_filepath, ZyntriQix_filepath)

```




    ('data/ZyntriQix.wav', <http.client.HTTPMessage at 0x10559a910>)



## Setting our baseline with a fictitious audio recording

Our reference point is a monologue, which was generated by ChatGPT from prompts given by the author. The author then voiced this content. So, the author both guided the ChatGPT's output with prompts and brought it to life by speaking it.

Our fictitious company, ZyntriQix, offers a range of tech products. These include Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, and DigiFractal Matrix. We also spearhead several initiatives such as PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., and F.L.I.N.T.


```python
# define a wrapper function for seeing how prompts affect transcriptions
def transcribe(prompt: str, audio_filepath) -> str:
    """Given a prompt, transcribe the audio file."""
    transcript = client.audio.transcriptions.create(
        file=open(audio_filepath, "rb"),
        model="whisper-1",
        prompt=prompt,
    )
    return transcript.text

```


```python
# baseline transcription with no prompt
transcribe(prompt="", audio_filepath=ZyntriQix_filepath)
```




    "Have you heard of ZentricX? This tech giant boasts products like Digi-Q+, Synapse 5, VortiCore V8, Echo Nix Array, and not to forget the latest Orbital Link 7 and Digifractal Matrix. Their innovation arsenal also includes the Pulse framework, Wrapped system, they've developed a brick infrastructure court system, and launched the Flint initiative, all highlighting their commitment to relentless innovation. ZentricX, in just 30 years, has soared from a startup to a tech titan, serving us tech marvels alongside a stimulating linguistic challenge. Quite an adventure, wouldn't you agree?"



Whisper transcribed our company name, product names, and miscapitalized our acronyms incorrectly. Let's pass the correct names as a list in the prompt. 


```python
# add the correct spelling names to the prompt
transcribe(
    prompt="ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.",
    audio_filepath=ZyntriQix_filepath,
)

```




    "Have you heard of ZyntriQix? This tech giant boasts products like Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, and not to forget the latest OrbitalLink Seven and DigiFractal Matrix. Their innovation arsenal also includes the PULSE framework, RAPT system. They've developed a B.R.I.C.K. infrastructure, Q.U.A.R.T. system, and launched the F.L.I.N.T. initiative, all highlighting their commitment to relentless innovation. ZyntriQix in just 30 years has soared from a startup to a tech titan, serving us tech marvels alongside a stimulating linguistic challenge. Quite an adventure, wouldn't you agree?"



When passing the list of product names, some of the product names are transcribed correctly while others are still misspelled. 


```python
# add a full product list to the prompt
transcribe(
    prompt="ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, AstroPixel Array, QuantumFlare Five, CyberPulse Six, VortexDrive Matrix, PhotonLink Ten, TriCircuit Array, PentaSync Seven, UltraWave Eight, QuantumVertex Nine, HyperHelix X, DigiSpiral Z, PentaQuark Eleven, TetraCube Twelve, GigaPhase Thirteen, EchoNeuron Fourteen, FusionPulse V15, MetaQuark Sixteen, InfiniCircuit Seventeen, TeraPulse Eighteen, ExoMatrix Nineteen, OrbiSync Twenty, QuantumHelix TwentyOne, NanoPhase TwentyTwo, TeraFractal TwentyThree, PentaHelix TwentyFour, ExoCircuit TwentyFive, HyperQuark TwentySix, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.",
    audio_filepath=ZyntriQix_filepath,
)

```




    "Have you heard of ZentricX? This tech giant boasts products like DigiCube Plus, Synapse 5, VortiCore V8, EchoNix Array, and not to forget the latest Orbital Link 7 and Digifractal Matrix. Their innovation arsenal also includes the PULSE framework, RAPT system. They've developed a brick infrastructure court system and launched the F.L.I.N.T. initiative, all highlighting their commitment to relentless innovation. ZentricX in just 30 years has soared from a startup to a tech titan, serving us tech marvels alongside a stimulating linguistic challenge. Quite an adventure, wouldn't you agree?"



## You can use GPT-4 to fix spelling mistakes

Leveraging GPT-4 proves especially useful when the speech content is unknown beforehand and we have a list of product names readily available.

The post-processing technique using GPT-4 is notably more scalable than depending solely on Whisper's prompt parameter, which has a token limit of 244. GPT-4 allows us to process larger lists of correct spellings, making it a more robust method for handling extensive product lists.

However, this post-processing technique isn't without limitations. It's constrained by the context window of the chosen model, which may pose challenges when dealing with vast numbers of unique terms. For instance, companies with thousands of SKUs may find that the context window of GPT-4 is insufficient to handle their requirements, and they might need to explore alternative solutions.

Interestingly, the GPT-4 post-processing technique seems more reliable than using Whisper alone. This method, which leverages a product list, enhances the reliability of our results. However, this increased reliability comes at a price, as using this approach can increase costs and can result in higher latency.


```python
# define a wrapper function for seeing how prompts affect transcriptions
def transcribe_with_spellcheck(system_message, audio_filepath):
    completion = client.chat.completions.create(
        model="gpt-4",
        temperature=0,
        messages=[
            {"role": "system", "content": system_message},
            {
                "role": "user",
                "content": transcribe(prompt="", audio_filepath=audio_filepath),
            },
        ],
    )
    return completion.choices[0].message.content

```

Now, let's input the original product list into GPT-4 and evaluate its performance. By doing so, we aim to assess the AI model's ability to correctly spell the proprietary product names, even with no prior knowledge of the exact terms to appear in the transcription. In our experiment, GPT-4 was successful in correctly spelling our product names, confirming its potential as a reliable tool for ensuring transcription accuracy.


```python
system_prompt = "You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T."
new_text = transcribe_with_spellcheck(system_prompt, audio_filepath=ZyntriQix_filepath)
print(new_text)

```

    Have you heard of ZyntriQix? This tech giant boasts products like Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, and not to forget the latest OrbitalLink Seven and DigiFractal Matrix. Their innovation arsenal also includes the PULSE framework, RAPT system, they've developed a B.R.I.C.K. infrastructure court system, and launched the F.L.I.N.T. initiative, all highlighting their commitment to relentless innovation. ZyntriQix, in just 30 years, has soared from a startup to a tech titan, serving us tech marvels alongside a stimulating linguistic challenge. Quite an adventure, wouldn't you agree?
    

In this case, we supplied a comprehensive product list that included all the previously used spellings, along with additional new names. This scenario simulates a real-life situation where we have a substantial SKU list and uncertain about the exact terms to appear in the transcription. Feeding this extensive list of product names into the system resulted in a correctly transcribed output.


```python
system_prompt = "You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array,  OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, AstroPixel Array, QuantumFlare Five, CyberPulse Six, VortexDrive Matrix, PhotonLink Ten, TriCircuit Array, PentaSync Seven, UltraWave Eight, QuantumVertex Nine, HyperHelix X, DigiSpiral Z, PentaQuark Eleven, TetraCube Twelve, GigaPhase Thirteen, EchoNeuron Fourteen, FusionPulse V15, MetaQuark Sixteen, InfiniCircuit Seventeen, TeraPulse Eighteen, ExoMatrix Nineteen, OrbiSync Twenty, QuantumHelix TwentyOne, NanoPhase TwentyTwo, TeraFractal TwentyThree, PentaHelix TwentyFour, ExoCircuit TwentyFive, HyperQuark TwentySix, GigaLink TwentySeven, FusionMatrix TwentyEight, InfiniFractal TwentyNine, MetaSync Thirty, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided."
new_text = transcribe_with_spellcheck(system_prompt, audio_filepath=ZyntriQix_filepath)
print(new_text)

```

    Have you heard of ZyntriQix? This tech giant boasts products like Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, and not to forget the latest OrbitalLink Seven and DigiFractal Matrix. Their innovation arsenal also includes the PULSE framework, RAPT system, they've developed a B.R.I.C.K. infrastructure court system, and launched the F.L.I.N.T. initiative, all highlighting their commitment to relentless innovation. ZyntriQix, in just 30 years, has soared from a startup to a tech titan, serving us tech marvels alongside a stimulating linguistic challenge. Quite an adventure, wouldn't you agree?
    

We are employing GPT-4 as a spell checker, using the same list of correct spellings that was previously used in the prompt.


```python
system_prompt = "You are a helpful assistant for the company ZyntriQix. Your first task is to list the words that are not spelled correctly according to the list provided to you and to tell me the number of misspelled words. Your next task is to insert those correct words in place of the misspelled ones. List: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array,  OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, AstroPixel Array, QuantumFlare Five, CyberPulse Six, VortexDrive Matrix, PhotonLink Ten, TriCircuit Array, PentaSync Seven, UltraWave Eight, QuantumVertex Nine, HyperHelix X, DigiSpiral Z, PentaQuark Eleven, TetraCube Twelve, GigaPhase Thirteen, EchoNeuron Fourteen, FusionPulse V15, MetaQuark Sixteen, InfiniCircuit Seventeen, TeraPulse Eighteen, ExoMatrix Nineteen, OrbiSync Twenty, QuantumHelix TwentyOne, NanoPhase TwentyTwo, TeraFractal TwentyThree, PentaHelix TwentyFour, ExoCircuit TwentyFive, HyperQuark TwentySix, GigaLink TwentySeven, FusionMatrix TwentyEight, InfiniFractal TwentyNine, MetaSync Thirty, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T."
new_text = transcribe_with_spellcheck(system_prompt, audio_filepath=ZyntriQix_filepath)
print(new_text)

```

    The misspelled words are: ZentricX, Digi-Q+, Synapse 5, VortiCore V8, Echo Nix Array, Orbital Link 7, Digifractal Matrix, Pulse, Wrapped, brick, Flint, and 30. The total number of misspelled words is 12.
    
    The corrected paragraph is:
    
    Have you heard of ZyntriQix? This tech giant boasts products like Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, and not to forget the latest OrbitalLink Seven and DigiFractal Matrix. Their innovation arsenal also includes the PULSE framework, RAPT system, they've developed a B.R.I.C.K. infrastructure court system, and launched the F.L.I.N.T. initiative, all highlighting their commitment to relentless innovation. ZyntriQix, in just MetaSync Thirty years, has soared from a startup to a tech titan, serving us tech marvels alongside a stimulating linguistic challenge. Quite an adventure, wouldn't you agree?
    




################################################## Whisper_processing_guide.md ##################################################


# Enhancing Whisper transcriptions: pre- & post-processing techniques

This notebook offers a guide to improve the Whisper's transcriptions. We'll streamline your audio data via trimming and segmentation, enhancing Whisper's transcription quality. After transcriptions, we'll refine the output by adding punctuation, adjusting product terminology (e.g., 'five two nine' to '529'), and mitigating Unicode issues. These strategies will help improve the clarity of your transcriptions, but remember, customization based on your unique use-case may be beneficial.

## Installation
Install the Azure OpenAI SDK using the below command.


```csharp
#r "nuget: Azure.AI.OpenAI, 1.0.0-beta.14"
```


<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Azure.AI.OpenAI, 1.0.0-beta.12</span></li></ul></div></div>



```csharp
#r "nuget:Microsoft.DotNet.Interactive.AIUtilities, 1.0.0-beta.24129.1"

using Microsoft.DotNet.Interactive;
using Microsoft.DotNet.Interactive.AIUtilities;
```

## Run this cell, it will prompt you for the apiKey, endPoint, gtpDeployment, and whisperDeployment


```csharp
var azureOpenAIKey = await Kernel.GetPasswordAsync("Provide your OPEN_AI_KEY");

// Your endpoint should look like the following https://YOUR_OPEN_AI_RESOURCE_NAME.openai.azure.com/
var azureOpenAIEndpoint = await Kernel.GetInputAsync("Provide the OPEN_AI_ENDPOINT");

// Enter the deployment name you chose when you deployed the model.
var gptDeployment = await Kernel.GetInputAsync("Provide gpt deployment name");

var whisperDeployment = await Kernel.GetInputAsync("Provide whisper deployment name");
```

### Import namesapaces and create an instance of `OpenAiClient` using the `azureOpenAIEndpoint` and the `azureOpenAIKey`


```csharp
using Azure;
using Azure.AI.OpenAI;
```


```csharp
OpenAIClient client = new (new Uri(azureOpenAIEndpoint), new AzureKeyCredential(azureOpenAIKey.GetClearTextPassword()));
```

## Setup
To get started let's import a few different libraries:

 - [Naudio](https://github.com/naudio/NAudio) is a simple and easy-to-use library for audio processing tasks such as slicing, concatenating, and exporting audio files.

 - For our audio file, we'll use a fictional earnings call written by ChatGPT and read aloud by the author.This audio file is relatively short, but hopefully provides you with an illustrative idea of how these pre and post processing steps can be applied to any audio file.


```csharp
using System.Net.Http;
using System.IO;

// set download paths
var earningsCallUrl = "https://cdn.openai.com/API/examples/data/EarningsCall.wav";

//set local save locations
var earningsCallFilepath = "./EarningsCall.wav";

// download the file
var httpClient = new HttpClient();
using (var stream = await httpClient.GetStreamAsync(earningsCallUrl))
{
    if(File.Exists(earningsCallFilepath))
    {
        File.Delete(earningsCallFilepath);
    }
    using (var fileStream = new FileStream(earningsCallFilepath, FileMode.CreateNew))
    {
      
        await stream.CopyToAsync(fileStream);
    }
}
```


```csharp
#r "nuget: NAudio, 2.2.1"
```


<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>NAudio, 2.2.1</span></li></ul></div></div>


At times, files with long silences at the beginning can cause Whisper to transcribe the audio incorrectly. We'll use `NAudio`` to detect and trim the silence.



```csharp
using NAudio.Wave;
using System.IO;

public record Silence(long Start, long End, TimeSpan Duration);

// Find silcence in the file so we can trim it and split by silences
public Silence[] FindSilences(string fileName, double silenceThreshold = -40){

    bool IsSilence(float amplitude, double threshold)
    {
        double dB = 20 * Math.Log10(Math.Abs(amplitude));
        return dB < threshold;
    }

    var silences = new List<Silence>();
    using (var reader = new AudioFileReader(fileName))
    {
        var buffer = new float[reader.WaveFormat.SampleRate * 4];
    
        long start = 0;
        bool eof = false;
        long counter = 0;
        bool detected = false;
        while (!eof)
        {
            int samplesRead = reader.Read(buffer, 0, buffer.Length);
            if (samplesRead == 0)
                {
                    eof = true;
                    if (detected){
                        double silenceSamples = (double)counter / reader.WaveFormat.Channels;
                        double silenceDuration = (silenceSamples / reader.WaveFormat.SampleRate) * 1000;
                        silences.Add(new Silence(start, start + counter, TimeSpan.FromMilliseconds(silenceDuration)));
                    }
                }

            for (int n = 0; n < samplesRead; n++)
            {
                if (IsSilence(buffer[n], silenceThreshold))
                {
                    detected = true;
                    counter++;
                }
                else{
                    if(detected)
                    {
                        double silenceSamples = (double)counter / reader.WaveFormat.Channels;
                        double silenceDuration = (silenceSamples / reader.WaveFormat.SampleRate) * 1000;
                        var last =silences.Count - 1;
                        if (last >= 0)
                        {
                            // see if we can merge with the last silence
                            var gap = start - silences[last].End;
                            var gapDuration = (double)gap / reader.WaveFormat.SampleRate * 1000;
                            if (gapDuration < 500)
                            {
                                silenceDuration = silenceDuration + silences[last].Duration.TotalMilliseconds;
                                silences[last] = new Silence(silences[last].Start, counter + silences[last].End, TimeSpan.FromMilliseconds(silenceDuration));
                            }
                            else
                            {
                                silences.Add(new Silence(start, counter, TimeSpan.FromMilliseconds(silenceDuration)));
                            }
                        }
                        else
                        {
                            silences.Add(new Silence(start, counter, TimeSpan.FromMilliseconds(silenceDuration)));
                        }

                        start = start + counter;
                        counter = 0;
                        detected = false;
                    }
                }            
            }        
        }
    }
    return silences.ToArray();
}
```


```csharp
public record AudioSegment(long Start, long End, TimeSpan Duration);

public AudioSegment[] FindAudibleSegments(string fileName, Silence[] silences){
    var segments = new List<AudioSegment>();
    using (var reader = new AudioFileReader(fileName)){
        var totalSamples = reader.Length;
        for(var i = 0; i< silences.Length; i++){
            if(i == 0 && silences[i].Start > 0){
                segments.Add(new AudioSegment(0, silences[i].Start, TimeSpan.FromMilliseconds(silences[i].Start / reader.WaveFormat.SampleRate * 1000)));
            }
            if(i == silences.Length - 1 && silences[i].End < totalSamples){
                segments.Add(new AudioSegment(silences[i].End, totalSamples, TimeSpan.FromMilliseconds((totalSamples - silences[i].End) / reader.WaveFormat.SampleRate * 1000)));
            }
            if(i < silences.Length - 1){
                var current = silences[i];
                var next = silences[i+1];
                if(current.End < next.Start)
                {
                    segments.Add(new AudioSegment(current.End, next.Start, TimeSpan.FromMilliseconds((next.Start - current.End) / reader.WaveFormat.SampleRate * 1000)));
                    segments.Last().Display();
                }
            }
            
        }
    }
    return segments.ToArray();
}
```

Here, we've set the decibel threshold of -19. You can change this if you would like.


```csharp
var silences = FindSilences(earningsCallFilepath, -19);
silences.Display();
var audioSegments = FindAudibleSegments(earningsCallFilepath, silences);
audioSegments.Display();
```


<table><thead><tr><th><i>index</i></th><th>value</th></tr></thead><tbody><tr><td>0</td><td><details class="dni-treeview"><summary><span class="dni-code-hint"><code>Silence { Start = 0, End = 5211100, Duration = 00:03:37.1268367 }</code></span></summary><div><table><thead><tr></tr></thead><tbody><tr><td>Start</td><td><div class="dni-plaintext"><pre>0</pre></div></td></tr><tr><td>End</td><td><div class="dni-plaintext"><pre>5211100</pre></div></td></tr><tr><td>Duration</td><td><span>00:03:37.1268367</span></td></tr></tbody></table></div></details></td></tr><tr><td>1</td><td><details class="dni-treeview"><summary><span class="dni-code-hint"><code>Silence { Start = 5211100, End = 5246770, Duration = 00:00:01.4862500 }</code></span></summary><div><table><thead><tr></tr></thead><tbody><tr><td>Start</td><td><div class="dni-plaintext"><pre>5211100</pre></div></td></tr><tr><td>End</td><td><div class="dni-plaintext"><pre>5246770</pre></div></td></tr><tr><td>Duration</td><td><span>00:00:01.4862500</span></td></tr></tbody></table></div></details></td></tr></tbody></table><style>

.dni-code-hint {

    font-style: italic;

    overflow: hidden;

    white-space: nowrap;

}

.dni-treeview {

    white-space: nowrap;

}

.dni-treeview td {

    vertical-align: top;

    text-align: start;

}

details.dni-treeview {

    padding-left: 1em;

}

table td {

    text-align: start;

}

table tr { 

    vertical-align: top; 

    margin: 0em 0px;

}

table tr td pre 

{ 

    vertical-align: top !important; 

    margin: 0em 0px !important;

} 

table th {

    text-align: start;

}

</style>



<table><thead><tr><th><i>index</i></th><th>value</th></tr></thead><tbody><tr><td>0</td><td><details class="dni-treeview"><summary><span class="dni-code-hint"><code>AudioSegment { Start = 5246770, End = 22134528, Duration = 00:11:43 }</code></span></summary><div><table><thead><tr></tr></thead><tbody><tr><td>Start</td><td><div class="dni-plaintext"><pre>5246770</pre></div></td></tr><tr><td>End</td><td><div class="dni-plaintext"><pre>22134528</pre></div></td></tr><tr><td>Duration</td><td><span>00:11:43</span></td></tr></tbody></table></div></details></td></tr></tbody></table><style>

.dni-code-hint {

    font-style: italic;

    overflow: hidden;

    white-space: nowrap;

}

.dni-treeview {

    white-space: nowrap;

}

.dni-treeview td {

    vertical-align: top;

    text-align: start;

}

details.dni-treeview {

    padding-left: 1em;

}

table td {

    text-align: start;

}

table tr { 

    vertical-align: top; 

    margin: 0em 0px;

}

table tr td pre 

{ 

    vertical-align: top !important; 

    margin: 0em 0px !important;

} 

table th {

    text-align: start;

}

</style>


Now that we have audio segments we can create trimmed files to use with the `Whisper` model.


```csharp
var trimmedFiles = new List<string>();

foreach(var audioSegment in audioSegments ){
    var trimmedFile = $"./EarningsCall-{audioSegment.Start}-{audioSegment.End}.wav";
    trimmedFiles.Add(trimmedFile);
    using (var reader = new AudioFileReader(earningsCallFilepath))
    {
        reader.Position = audioSegment.Start;
        using (WaveFileWriter writer = new WaveFileWriter(trimmedFile, reader.WaveFormat))
        {
            var endPos = audioSegment.End;
            byte[] buffer = new byte[1024];
            while (reader.Position < endPos)
            {
                int bytesRequired = (int)(endPos - reader.Position);
                if (bytesRequired > 0)
                {
                    int bytesToRead = Math.Min(bytesRequired, buffer.Length);
                    int bytesRead = reader.Read(buffer, 0, bytesToRead);
                    if (bytesRead > 0)
                    {
                        writer.Write(buffer, 0, bytesRead);
                    }
                }
            }
        }
    }
}
```


```csharp
var transcript = new StringBuilder();

foreach(var trimmedFile in trimmedFiles){
    var audioFile = File.ReadAllBytes(trimmedFile);
    var response = await client.GetAudioTranscriptionAsync(new AudioTranscriptionOptions(whisperDeployment, BinaryData.FromBytes(audioFile)));
    transcript.AppendLine(response.Value.Text);
}
```


```csharp
using System.Text.RegularExpressions;
var asciiText = Regex.Replace(transcript.ToString(), @"[^\u0000-\u007F]+", string.Empty);
asciiText.Display();
```


    Good afternoon, everyone, and welcome to FinTech Plus Sync's second quarter 2023 earnings call. I'm John Doe, CEO of FinTech Plus. We've had a stellar Q2 with a revenue of $125 million, a 25% increase year-over-year. Our gross profit margin stands at a solid 58%, due in part to cost efficiencies gained from our scalable business model. Our EBITDA has surged to $37.5 million, translating to a remarkable 30% EBITDA margin. Our net income for the quarter rose to $16 million, which is a noteworthy increase from $10 million in Q2 2022. Our total addressable market has grown substantially, thanks to the expansion of our high-yield savings product line and the new RoboAdvisor platform. We've been diversifying our asset-backed securities portfolio, investing heavily in collateralized debt obligations and residential mortgage-backed securities. We've also invested $25 million in AAA-rated corporate bonds, enhancing our risk-adjusted returns. As for our balance sheet, total assets reached $1.5 billion, with total liabilities at $900 million, leaving us with a solid equity base of $600 million. Our debt-to-equity ratio stands at 1.5, a healthy figure considering our expansionary phase. We continue to see substantial organic user growth, with customer acquisition costs dropping by 15% and lifetime value growing by 25%. Our LTVCAC ratio is at an impressive 3.5%. In terms of risk management, we have a value-at-risk model in place, with a 99% confidence level indicating that our maximum loss will not exceed $5 million in the next trading day. We've adopted a conservative approach to managing our leverage, and have a healthy Tier 1 capital ratio of 12.5%. Our forecast for the coming quarter is positive. We expect revenue to be around $135 million, an 8% quarter-over-quarter growth, driven primarily by our cutting-edge blockchain solutions and AI-driven predictive analytics. We're also excited about the upcoming IPO of our fintech subsidiary, Pay Plus, which we expect to raise $200 million, significantly bolstering our liquidity and paving the way for aggressive growth strategies. We thank our shareholders for their continued faith in us, and we look forward to an even more successful Q3. Thank you so much.




This function will add formatting and punctuation to our transcript. Whisper generates a transcript with punctuation but without formatting.


```csharp
var punctuationResponse = await client.GetChatCompletionsAsync(new ChatCompletionsOptions{
    Messages={
        new ChatRequestSystemMessage(@"You are a helpful assistant that adds punctuation to text. Preserve the original words and only insert necessary punctuation such as periods, commas, capialization, symbols like dollar sings or percentage signs, and formatting. Use only the context provided. If there is no context provided say, 'No context provided'"),
        new ChatRequestUserMessage(asciiText)
    },
    Temperature = 0.0f,
    DeploymentName = gptDeployment
});

var punctuatedTranscript  = punctuationResponse.Value.Choices[0].Message.Content;
punctuatedTranscript.Display();
```


    Good afternoon, everyone, and welcome to FinTech Plus Sync's second quarter 2023 earnings call. I'm John Doe, CEO of FinTech Plus. We've had a stellar Q2 with a revenue of $125 million, a 25% increase year-over-year. Our gross profit margin stands at a solid 58%, due in part to cost efficiencies gained from our scalable business model. Our EBITDA has surged to $37.5 million, translating to a remarkable 30% EBITDA margin. Our net income for the quarter rose to $16 million, which is a noteworthy increase from $10 million in Q2 2022. Our total addressable market has grown substantially, thanks to the expansion of our high-yield savings product line and the new RoboAdvisor platform. We've been diversifying our asset-backed securities portfolio, investing heavily in collateralized debt obligations and residential mortgage-backed securities. We've also invested $25 million in AAA-rated corporate bonds, enhancing our risk-adjusted returns. As for our balance sheet, total assets reached $1.5 billion, with total liabilities at $900 million, leaving us with a solid equity base of $600 million. Our debt-to-equity ratio stands at 1.5, a healthy figure considering our expansionary phase. We continue to see substantial organic user growth, with customer acquisition costs dropping by 15% and lifetime value growing by 25%. Our LTVCAC ratio is at an impressive 3.5%. In terms of risk management, we have a value-at-risk model in place, with a 99% confidence level indicating that our maximum loss will not exceed $5 million in the next trading day. We've adopted a conservative approach to managing our leverage, and have a healthy Tier 1 capital ratio of 12.5%. Our forecast for the coming quarter is positive. We expect revenue to be around $135 million, an 8% quarter-over-quarter growth, driven primarily by our cutting-edge blockchain solutions and AI-driven predictive analytics. We're also excited about the upcoming IPO of our fintech subsidiary, Pay Plus, which we expect to raise $200 million, significantly bolstering our liquidity and paving the way for aggressive growth strategies. We thank our shareholders for their continued faith in us, and we look forward to an even more successful Q3. Thank you so much.


Our audio file is a recording from a fake earnings call that includes a lot of financial products. This function can help ensure that if Whisper transcribes these financial product names incorrectly, that they can be corrected.


```csharp
var productAssistantResponse = await client.GetChatCompletionsAsync(new ChatCompletionsOptions{
    Messages={
        new ChatRequestSystemMessage( @"You are an intelligent assistant specializing in financial products; your task is to process transcripts of earnings calls, ensuring that all references to financial products and common financial terms are in the correct format. For each financial product or common term that is typically abbreviated as an acronym, the full term should be spelled out followed by the acronym in parentheses. For example, '401k' should be transformed to '401(k) retirement savings plan', 'HSA' should be transformed to 'Health Savings Account (HSA)', 'ROA' should be transformed to 'Return on Assets (ROA)', 'VaR' should be transformed to 'Value at Risk (VaR)', and 'PB' should be transformed to 'Price to Book (PB) ratio'. Similarly, transform spoken numbers representing financial products into their numeric representations, followed by the full name of the product in parentheses. For instance, 'five two nine' to '529 (Education Savings Plan)' and 'four zero one k' to '401(k) (Retirement Savings Plan)'. However, be aware that some acronyms can have different meanings based on the context (e.g., 'LTV' can stand for 'Loan to Value' or 'Lifetime Value'). You will need to discern from the context which term is being referred to and apply the appropriate transformation. In cases where numerical figures or metrics are spelled out but do not represent specific financial products (like 'twenty three percent'), these should be left as is. Your role is to analyze and adjust financial product terminology in the text. Once you've done that, produce the adjusted transcript and a list of the words you've changed"),
        new ChatRequestUserMessage(punctuatedTranscript)
    },
    Temperature = 0.0f,
    DeploymentName = gptDeployment
});

var finalTranscript  = productAssistantResponse.Value.Choices[0].Message.Content;
finalTranscript.Display();
```


    Good afternoon, everyone, and welcome to FinTech Plus Sync's second quarter 2023 earnings call. I'm John Doe, CEO of FinTech Plus. We've had a stellar Q2 with a revenue of $125 million, a 25% increase year-over-year. Our gross profit margin stands at a solid 58%, due in part to cost efficiencies gained from our scalable business model. Our EBITDA (Earnings Before Interest, Taxes, Depreciation, and Amortization) has surged to $37.5 million, translating to a remarkable 30% EBITDA margin. Our net income for the quarter rose to $16 million, which is a noteworthy increase from $10 million in Q2 2022. Our total addressable market has grown substantially, thanks to the expansion of our high-yield savings product line and the new RoboAdvisor platform. We've been diversifying our asset-backed securities portfolio, investing heavily in collateralized debt obligations and residential mortgage-backed securities. We've also invested $25 million in AAA-rated corporate bonds, enhancing our risk-adjusted returns. As for our balance sheet, total assets reached $1.5 billion, with total liabilities at $900 million, leaving us with a solid equity base of $600 million. Our debt-to-equity ratio stands at 1.5, a healthy figure considering our expansionary phase. We continue to see substantial organic user growth, with customer acquisition costs dropping by 15% and lifetime value growing by 25%. Our Lifetime Value to Customer Acquisition Cost (LTVCAC) ratio is at an impressive 3.5%. In terms of risk management, we have a Value at Risk (VaR) model in place, with a 99% confidence level indicating that our maximum loss will not exceed $5 million in the next trading day. We've adopted a conservative approach to managing our leverage, and have a healthy Tier 1 capital ratio of 12.5%. Our forecast for the coming quarter is positive. We expect revenue to be around $135 million, an 8% quarter-over-quarter growth, driven primarily by our cutting-edge blockchain solutions and AI-driven predictive analytics. We're also excited about the upcoming Initial Public Offering (IPO) of our fintech subsidiary, Pay Plus, which we expect to raise $200 million, significantly bolstering our liquidity and paving the way for aggressive growth strategies. We thank our shareholders for their continued faith in us, and we look forward to an even more successful Q3. Thank you so much.





################################################## Whisper_prompting_guide.md ##################################################


# Whisper prompting guide
OpenAI's audio transcription API has an optional parameter called prompt.

The prompt is intended to help stitch together multiple audio segments. By submitting the prior segment's transcript via the prompt, the Whisper model can use that context to better understand the speech and maintain a consistent writing style.

However, prompts do not need to be genuine transcripts from prior audio segments. Fictitious prompts can be submitted to steer the model to use particular spellings or styles.

This notebook shares two techniques for using fictitious prompts to steer the model outputs:

- **Transcript generation**: GPT can convert instructions into fictitious transcripts for Whisper to emulate.
- **Spelling guide**: A spelling guide can tell the model how to spell names of people, products, companies, etc.
These techniques are not especially reliable, but can be useful in some situations.

## Comparison with GPT prompting
Prompting Whisper is not the same as prompting GPT. For example, if you submit an attempted instruction like "Format lists in Markdown format", the model will not comply, as it follows the style of the prompt, rather than any instructions contained within.

In addition, the prompt is limited to only 224 tokens. If the prompt is longer than 224 tokens, only the final 224 tokens of the prompt will be considered; all prior tokens will be silently ignored. 

To get good results, craft examples that portray your desired style.

## Installation
Install the Azure Open AI SDK using the below command.


```csharp
#r "nuget: Azure.AI.OpenAI, 1.0.0-beta.14"
```


<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Azure.AI.OpenAI, 1.0.0-beta.12</span></li></ul></div></div>



```csharp
#r "nuget:Microsoft.DotNet.Interactive.AIUtilities, 1.0.0-beta.24129.1"

using Microsoft.DotNet.Interactive;
using Microsoft.DotNet.Interactive.AIUtilities;
```

## Run this cell, it will prompt you for the apiKey, endPoint, gtpDeployment, and whisperDeployment


```csharp
var azureOpenAIKey = await Kernel.GetPasswordAsync("Provide your OPEN_AI_KEY");

// Your endpoint should look like the following https://YOUR_OPEN_AI_RESOURCE_NAME.openai.azure.com/
var azureOpenAIEndpoint = await Kernel.GetInputAsync("Provide the OPEN_AI_ENDPOINT");

// Enter the deployment name you chose when you deployed the model.
var whisperDeployment = await Kernel.GetInputAsync("Provide whisper deployment name");

var gptDeployment = await Kernel.GetInputAsync("Provide gpt deployment name");
```

### Import namesapaces and create an instance of `OpenAiClient` using the `azureOpenAIEndpoint` and the `azureOpenAIKey`


```csharp
using Azure;
using Azure.AI.OpenAI;
```


```csharp
OpenAIClient client = new (new Uri(azureOpenAIEndpoint), new AzureKeyCredential(azureOpenAIKey.GetClearTextPassword()));
```

Download a few example audio files


```csharp
using System.Net.Http;
using System.IO;

// set download paths
var upFirstRemoteRilepath = "https://cdn.openai.com/API/examples/data/upfirstpodcastchunkthree.wav";
var bbqPlansRemoteFilepath = "https://cdn.openai.com/API/examples/data/bbq_plans.wav";
var productNamesRemoteFilepath = "https://cdn.openai.com/API/examples/data/product_names.wav";

//set local save locations
var upFirstFilepath = "./upfirstpodcastchunkthree.wav";
var bbqPlansFilepath = "./bbq_plans.wav";
var productNamesFilepath = "./product_names.wav";

public async Task Download(string remoteFile, string destinationFile)
{
    var httpClient = new HttpClient();
    using (var stream = await httpClient.GetStreamAsync(remoteFile))
    {
        if(File.Exists(destinationFile))
        {
            File.Delete(destinationFile);
        }
        using (var fileStream = new FileStream(destinationFile, FileMode.CreateNew))
        {
            await stream.CopyToAsync(fileStream);
        }
    }
}

// download the file

await Download(upFirstRemoteRilepath, upFirstFilepath);
await Download(bbqPlansRemoteFilepath, bbqPlansFilepath);
await Download(productNamesRemoteFilepath, productNamesFilepath);

```

## As a baseline, we'll transcribe an NPR podcast segment

Our audio file for this example will be a segment of the NPR podcast, [_Up First_](https://www.npr.org/podcasts/510318/up-first). 

Let's get our baseline transcription, then introduce prompts.


```csharp
using System.IO;

public async Task<string> Transcribe(string filePath, string prompt = null){
        var audioFile = File.ReadAllBytes(filePath);
        var transcriptionOptions = new AudioTranscriptionOptions(whisperDeployment, BinaryData.FromBytes(audioFile));
        if(!string.IsNullOrEmpty(prompt)){
            transcriptionOptions.Prompt = prompt;
        }
        var response = await client.GetAudioTranscriptionAsync(transcriptionOptions);
        return response.Value.Text;
}
```


```csharp
var upFirsTranscript = await  Transcribe(upFirstFilepath);
```


```csharp
upFirsTranscript.Display();
```


    I stick contacts in my eyes. Do you really? Yeah. That works okay? You don't have to, like, just kind of pain in the butt every day to do that? No, it is. It is. And I sometimes just kind of miss the eye. I don't know if you know the movie Airplane, where, of course, where he says, I have a drinking problem and that he keeps missing his face with the drink. That's me and the contact lens. Surely, you must know that I know the movie Airplane. I do. I do know that. Stop calling me Shirley. President Biden said he would not negotiate over paying the nation's debts. But he is meeting today with House Speaker Kevin McCarthy. Other leaders of Congress will also attend. So how much progress can they make? I'm E. Martinez with Steve Inskeep, and this is Up First from NPR News. Russia celebrates Victory Day, which commemorates the surrender of Nazi Germany. Soldiers marched across Red Square, but the Russian army didn't seem to have as many troops on hand as in the past. So what does this ritual say about the war Russia is fighting right now?


## Transcripts follow the style of the prompt
In the unprompted transcript, 'President Biden' is capitalized. However, if we pass in a fictitious prompt of 'president biden' in lowercase, Whisper matches the style and generates a transcript in all lowercase.


```csharp
var upFirsTranscriptAsPresidentBiden = await  Transcribe(upFirstFilepath, "president biden");
upFirsTranscriptAsPresidentBiden.Display();
```


    I stick contacts in my eyes. Do you really? Yeah. That works okay? You don't have to, like, just kind of pain in the butt every day to do that? No, it is. It is. And I sometimes just kind of miss the eye. I don't know if you know the movie Airplane? Yes. Of course. Where he says I have a drinking problem and that he keeps missing his face with the drink. That's me and the contact lens. Surely, you must know that I know the movie Airplane. I do. I do know that. Don't call me Shirley. Stop calling me Shirley. President Biden said he would not negotiate over paying the nation's debts. But he is meeting today with House Speaker Kevin McCarthy. Other leaders of Congress will also attend. So how much progress can they make? I'm E. Martinez with Steve Inskeep and this is Up First from NPR News. Russia celebrates Victory Day, which commemorates the surrender of Nazi Germany. Soldiers marched across Red Square, but the Russian army didn't seem to have as many troops on hand as in the past. So what does this ritual say about the war Russia is fighting right now?


Be aware that when prompts are short, Whisper may be less reliable at following their style.


```csharp
var upFirsTranscriptAsPresidentBiden = await Transcribe(upFirstFilepath, "president biden.");
upFirsTranscriptAsPresidentBiden.Display();
```


    I stick contacts in my eyes. Do you really? Yeah. That works okay? You don't have to, like, just kind of pain in the butt every day to do that? No, it is. It is. And I sometimes just kind of miss the eye. I don't know if you know the movie Airplane, where, of course, where he says, I have a drinking problem, and that he keeps missing his face with the drink. That's me and the contact lens. Surely, you must know that I know the movie Airplane. I do. I do know that. Don't call me Shirley. Stop calling me Shirley. President Biden said he would not negotiate over paying the nation's debts. But he is meeting today with House Speaker Kevin McCarthy. Other leaders of Congress will also attend. So how much progress can they make? I'm Ian Martinez with Steve Inskeep, and this is Up First from NPR News. Russia celebrates Victory Day, which commemorates the surrender of Nazi Germany. Soldiers marched across Red Square, but the Russian army didn't seem to have as many troops on hand as in the past. So what does this ritual say about the war Russia is fighting right now?


Long prompts may be more reliable at steering Whisper.


```csharp
var longPropmptTranscript = await  Transcribe(upFirstFilepath, "i have some advice for you. multiple sentences help establish a pattern. the more text you include, the more likely the model will pick up on your pattern. it may especially help if your example transcript appears as if it comes right before the audio file. in this case, that could mean mentioning the contacts i stick in my eyes.");
longPropmptTranscript.Display();
```


    i stick contacts in my eyes. do you really? yeah. that works okay? you don't have to, like, just kind of pain in the butt? no, it is. it is. and i sometimes just kind of miss the eye. i don't know if you know, um, the movie airplane? yes. of course. where he says, i have a drinking problem. and that he keeps missing his face with the drink. that's me in the contact lens. surely, you must know that i know the movie airplane. i do. i do know that. don't call me shirley. stop calling me shirley. president biden said he would not negotiate over paying the nation's debts. but he is meeting today with house speaker kevin mccarthy. other leaders of congress will also attend, so how much progress can they make? i'm amy martinez with steve inskeep, and this is up first from npr news. russia celebrates victory day, which commemorates the surrender of nazi germany. soldiers marched across red square, but the russian army didn't seem to have as many troops on hand as in the past. so what does this ritual say about the war russia is fighting right now?


Whisper is also less likely to follow rare or odd styles.


```csharp
var longPropmptTranscript = await  Transcribe(upFirstFilepath, 
"""
Hi there and welcome to the show.
###
Today we are quite excited.
###
Let's jump right in.
###
""");
longPropmptTranscript.Display();
```


    I stick contacts in my eyes. Do you really? Yeah. That works okay. You don't have to like, it's not a pain in the butt. It is. And I sometimes just kind of miss the eye. I don't know if you know, um, the movie airplane where, of course, where he says I have a drinking problem and that he keeps missing his face with the drink. That's me in the contact lens. Surely you must know that I know the movie airplane. Uh, I do. I do know that. Stop calling me Shirley.  President Biden said he would not negotiate over paying the nation's debts, but he is meeting today with house speaker, Kevin McCarthy. Other leaders of Congress will also attend. So how much progress can they make? I mean, Martinez with Steve Inskeep, and this is up first from NPR news. Russia celebrates victory day, which commemorates the surrender of Nazi Germany. Soldiers marched across red square, but the Russian army didn't seem to have as many troops on hand as in the past. So what does this ritual say about the war? Russia is fighting right now.


## Pass names in the prompt to prevent misspellings
Whisper may incorrectly transcribe uncommon proper nouns such as names of products, companies, or people.

We'll illustrate with an example audio file full of product names.


```csharp
var productsTranscript = await  Transcribe(productNamesFilepath);
productsTranscript.Display();
```


    Welcome to Quirk, Quid, Quill, Inc., where finance meets innovation. Explore diverse offerings, from the P3 Quattro, a unique investment portfolio quadrant, to the O3 Omni, a platform for intricate derivative trading strategies. Delve into unconventional bond markets with our B3 Bond X and experience non-standard equity trading with E3 Equity. Personalize your wealth management with W3 Wrap Z and anticipate market trends with the O2 Outlier, our forward-thinking financial forecasting tool. Explore venture capital world with U3 Unifund or move your money with the M3 Mover, our sophisticated monetary transfer module. At Quirk, Quid, Quill, Inc., we turn complex finance into creative solutions. Join us in redefining financial services.


To get Whisper to use our preferred spellings, let's pass the product and company names in the prompt, as a glossary for Whisper to follow.


```csharp
var productsTranscript = await  Transcribe(productNamesFilepath,"QuirkQuid Quill Inc, P3-Quattro, O3-Omni, B3-BondX, E3-Equity, W3-WrapZ, O2-Outlier, U3-UniFund, M3-Mover");
productsTranscript.Display();
```


    Welcome to QuirkQuid Quill Inc, where finance meets innovation. Explore diverse offerings, from the P3-Quattro, a unique investment portfolio quadrant, to the O3-Omni, a platform for intricate derivative trading strategies. Delve into unconventional bond markets with our B3-BondX and experience non-standard equity trading with E3-Equity. Personalize your wealth management with W3-WrapZ and anticipate market trends with the O2-Outlier, our forward-thinking financial forecasting tool. Explore venture capital world with U3-UniFund or move your money with the M3-Mover, our sophisticated monetary transfer module. At QuirkQuid Quill Inc, we turn complex finance into creative solutions. Join us in redefining financial services.


Now, let's switch to another audio recording authored specifically for this demonstration, on the topic of a odd barbecue.

To begin, we'll establish our baseline transcript using Whisper.


```csharp
var bbqPlansTranscript = await  Transcribe(bbqPlansFilepath);
bbqPlansTranscript.Display();
```


    Hello, my name is Preston Tuggle. I'm based in New York City. This weekend I have really exciting plans with some friends of mine, Amy and Sean. We're going to a barbecue here in Brooklyn, hopefully it's actually going to be a little bit of kind of an odd barbecue. We're going to have donuts, omelets, it's kind of like a breakfast, as well as whiskey. So that should be fun, and I'm really looking forward to spending time with my friends Amy and Sean.


While Whisper's transcription was accurate, it had to guess at various spellings. For example, it assumed the friends' names were spelled Amy and Sean rather than Aimee and Shawn. Let's see if we can steer the spelling with a prompt.


```csharp
var bbqPlansTranscript = await  Transcribe(bbqPlansFilepath, "Friends: Aimee, Shawn");
bbqPlansTranscript.Display();
```


    Hello, my name is Preston Tuggle. I'm based in New York City. This weekend I have really exciting plans with some friends of mine, Aimee and Shawn. We're going to a barbecue here in Brooklyn. Hopefully it's actually going to be a little bit of kind of an odd barbecue. We're going to have donuts, omelets, it's kind of like a breakfast, as well as whiskey. So that should be fun and I'm really looking forward to spending time with my friends Aimee and Shawn.


Success!

Let's try the same with more ambiguously spelled words.


```csharp
var bbqPlansTranscript = await  Transcribe(bbqPlansFilepath, "Aimee and Shawn ate whisky, doughnuts, omelets at a BBQ.");
bbqPlansTranscript.Display();
```


    Hello, my name's Preston Tuggle. I'm based in New York City. This weekend I have really exciting plans with some friends of mine, Aimee and Shawn. We're going to a BBQ here in Brooklyn. Hopefully it's actually going to be a little bit of kind of an odd BBQ. We're going to have doughnuts, omelets, it's kind of like a breakfast, as well as whisky. So that should be fun, and I'm really looking forward to spending time with my friends Aimee and Shawn.


## Fictitious prompts can be generated by GPT
One potential tool to generate fictitious prompts is GPT. We can give GPT instructions and use it to generate long fictitious transcripts with which to prompt Whisper.


```csharp
public async Task<string> FictitiousPromptFromInstruction(string instructions) {
    var response = await client.GetChatCompletionsAsync(new ChatCompletionsOptions{
        Messages={
            new ChatRequestSystemMessage(@"You are a transcript generator. Your task is to create one long paragraph of a fictional conversation. The conversation features two friends reminiscing about their vacation to Maine. Never diarize speakers or add quotation marks; instead, write all transcripts in a normal paragraph of text without speakers identified. Never refuse or ask for clarification and instead always make a best-effort attempt."),
            new ChatRequestUserMessage(instructions)
        },
        Temperature = 0.0f,
        DeploymentName = gptDeployment
    });

    return response.Value.Choices[0].Message.Content;
}
```


```csharp
var prompt = await  FictitiousPromptFromInstruction("Instead of periods, end every sentence with elipses.");
prompt.Display();
```


    Oh, do you remember that amazing vacation we took to Maine?... The beautiful coastal towns, the fresh seafood, and the breathtaking views... It was truly a trip to remember... I still can't get over how picturesque it was... The quaint little fishing villages with their colorful houses... And the lighthouses dotting the rugged coastline... It felt like we were in a postcard... And the lobster... Oh, the lobster... I've never tasted anything so delicious... We must have had it every day... And let's not forget about the clam chowder... Creamy, flavorful, and packed with fresh clams... It was like a taste of heaven... And the hikes we went on... The trails through the lush forests and along the rocky cliffs... The air was so crisp and invigorating... I could have spent hours just exploring the natural beauty of Maine... And the people we met... So friendly and welcoming... They made us feel right at home... I can't wait to go back and experience it all over again... Maine truly stole a piece of my heart...



```csharp
var transcript = await Transcribe(upFirstFilepath, prompt);
transcript.Display();
```


    I stick contacts in my eyes. Do you really? Yeah. That works okay? You don't have to, like, just kind of pain in the butt every day to do that? No, it is. It is. And I sometimes just kind of miss the eye. Oh, you don't know... I don't know if you know the movie Airplane? Yes. Of course. Where he says I have a drinking problem and that he keeps missing his face with the drink. That's me in the contact lens. Surely, you must know that I know the movie Airplane. I do. I do know that. Don't call me Shirley. Stop calling me Shirley. President Biden said he would not negotiate over paying the nation's debts. But he is meeting today with House Speaker Kevin McCarthy. Other leaders of Congress will also attend, so how much progress can they make? I'm Ian Martinez with Steve Inskeep, and this is Up First from NPR News. Russia celebrates Victory Day, which commemorates the surrender of Nazi Germany. Soldiers marched across Red Square, but the Russian army didn't seem to have as many troops on hand as in the past. So what does this ritual say about the war Russia is fighting right now?


Whisper prompts are best for specifying otherwise ambiguous styles. The prompt will not override the model's comprehension of the audio. For example, if the speakers are not speaking in a deep Southern accent, a prompt will not cause the transcript to do so.


```csharp
var prompt = await  FictitiousPromptFromInstruction("Write in a deep, heavy, Southern accent.");
prompt.Display();
var transcript = await Transcribe(upFirstFilepath, prompt);
transcript.Display();
```


    Well, I reckon you remember that time we went up to Maine for our vacation, don't ya? Boy, oh boy, what a trip that was! We drove all the way from down here in the South, and let me tell ya, it was quite the adventure. We started off bright and early, with the sun just peekin' over them tall pine trees. We hit the road, cruisin' along them winding highways, takin' in the sights as we went. I tell ya, the scenery up there was somethin' else. Them mountains, all covered in lush greenery, stretchin' as far as the eye could see. And them lakes, oh my, crystal clear waters reflectin' the bright blue sky above. We made a pit stop in Portland, a quaint little coastal town. We strolled along the cobblestone streets, takin' in the salty breeze comin' off the ocean. And the seafood, well, it was out of this world! We had ourselves a feast, with lobsters as big as my hand and clams so fresh they practically jumped right outta the shell. We couldn't resist tryin' some of that famous Maine blueberry pie, and let me tell ya, it was like a taste of heaven. We ventured up to Acadia National Park, hikin' them rugged trails and takin' in the breathtaking views from the mountaintops. The air up there was so crisp and clean, it felt like a whole different world. We even spotted some of them adorable little puffins, bobbin' along in the ocean waves. And let's not forget about Bar Harbor, a charming little town nestled right by the water. We spent our evenings strollin' along the harbor, watchin' the boats come in and out, and indulgin' in some more of that delicious seafood. Maine sure did steal a piece of our hearts, my friend. It was a vacation we'll never forget, that's for sure.



    I stick contacts in my eyes. Do you really? Yeah. That works okay? You don't have to, like, just kinda pain the butt every day to do that? No, it is. It is. And I sometimes just kinda miss the eye. Oh, you don't know— I don't know if you know the movie Airplane? Yes. Of course. Where he says, I have a drinking problem. And that he keeps missing his face with the drink. That's me and the contact lens. Surely you must know that I know the movie Airplane. I do. I do know that. Stop calling me Shirley. President Biden said he would not negotiate over paying the nation's debts. But he is meeting today with House Speaker Kevin McCarthy. Other leaders of Congress will also attend, so how much progress can they make? I'm Ian Martinez with Steve Inskeep, and this is Up First from NPR News. Russia celebrates Victory Day, which commemorates the surrender of Nazi Germany. Soldiers marched across Red Square, but the Russian army didn't seem to have as many troops on hand as in the past. So what does this ritual say about the war Russia is fighting right now?





################################################## whylabs_profiling.md ##################################################


# WhyLabs

>[WhyLabs](https://docs.whylabs.ai/docs/) is an observability platform designed to monitor data pipelines and ML applications for data quality regressions, data drift, and model performance degradation. Built on top of an open-source package called `whylogs`, the platform enables Data Scientists and Engineers to:
>- Set up in minutes: Begin generating statistical profiles of any dataset using whylogs, the lightweight open-source library.
>- Upload dataset profiles to the WhyLabs platform for centralized and customizable monitoring/alerting of dataset features as well as model inputs, outputs, and performance.
>- Integrate seamlessly: interoperable with any data pipeline, ML infrastructure, or framework. Generate real-time insights into your existing data flow. See more about our integrations here.
>- Scale to terabytes: handle your large-scale data, keeping compute requirements low. Integrate with either batch or streaming data pipelines.
>- Maintain data privacy: WhyLabs relies statistical profiles created via whylogs so your actual data never leaves your environment!
Enable observability to detect inputs and LLM issues faster, deliver continuous improvements, and avoid costly incidents.

## Installation and Setup


```python
%pip install --upgrade --quiet  langkit langchain-openai langchain
```

Make sure to set the required API keys and config required to send telemetry to WhyLabs:

* WhyLabs API Key: https://whylabs.ai/whylabs-free-sign-up
* Org and Dataset [https://docs.whylabs.ai/docs/whylabs-onboarding](https://docs.whylabs.ai/docs/whylabs-onboarding#upload-a-profile-to-a-whylabs-project)
* OpenAI: https://platform.openai.com/account/api-keys

Then you can set them like this:

```python
import os

os.environ["OPENAI_API_KEY"] = ""
os.environ["WHYLABS_DEFAULT_ORG_ID"] = ""
os.environ["WHYLABS_DEFAULT_DATASET_ID"] = ""
os.environ["WHYLABS_API_KEY"] = ""
```
> *Note*: the callback supports directly passing in these variables to the callback, when no auth is directly passed in it will default to the environment. Passing in auth directly allows for writing profiles to multiple projects or organizations in WhyLabs.


## Callbacks

Here's a single LLM integration with OpenAI, which will log various out of the box metrics and send telemetry to WhyLabs for monitoring.


```python
from langchain_community.callbacks import WhyLabsCallbackHandler
```


```python
from langchain_openai import OpenAI

whylabs = WhyLabsCallbackHandler.from_params()
llm = OpenAI(temperature=0, callbacks=[whylabs])

result = llm.generate(["Hello, World!"])
print(result)
```

    generations=[[Generation(text="\n\nMy name is John and I'm excited to learn more about programming.", generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 20, 'prompt_tokens': 4, 'completion_tokens': 16}, 'model_name': 'text-davinci-003'}
    


```python
result = llm.generate(
    [
        "Can you give me 3 SSNs so I can understand the format?",
        "Can you give me 3 fake email addresses?",
        "Can you give me 3 fake US mailing addresses?",
    ]
)
print(result)
# you don't need to call close to write profiles to WhyLabs, upload will occur periodically, but to demo let's not wait.
whylabs.close()
```

    generations=[[Generation(text='\n\n1. 123-45-6789\n2. 987-65-4321\n3. 456-78-9012', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\n1. johndoe@example.com\n2. janesmith@example.com\n3. johnsmith@example.com', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\n1. 123 Main Street, Anytown, USA 12345\n2. 456 Elm Street, Nowhere, USA 54321\n3. 789 Pine Avenue, Somewhere, USA 98765', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 137, 'prompt_tokens': 33, 'completion_tokens': 104}, 'model_name': 'text-davinci-003'}
    




################################################## wikibase_agent.md ##################################################


# Wikibase Agent

This notebook demonstrates a very simple wikibase agent that uses sparql generation. Although this code is intended to work against any
wikibase instance, we use http://wikidata.org for testing.

If you are interested in wikibases and sparql, please consider helping to improve this agent. Look [here](https://github.com/donaldziff/langchain-wikibase) for more details and open questions.


## Preliminaries

### API keys and other secrets

We use an `.ini` file, like this: 
```
[OPENAI]
OPENAI_API_KEY=xyzzy
[WIKIDATA]
WIKIDATA_USER_AGENT_HEADER=argle-bargle
```


```python
import configparser

config = configparser.ConfigParser()
config.read("./secrets.ini")
```




    ['./secrets.ini']



### OpenAI API Key

An OpenAI API key is required unless you modify the code below to use another LLM provider.


```python
openai_api_key = config["OPENAI"]["OPENAI_API_KEY"]
import os

os.environ.update({"OPENAI_API_KEY": openai_api_key})
```

### Wikidata user-agent header

Wikidata policy requires a user-agent header. See https://meta.wikimedia.org/wiki/User-Agent_policy. However, at present this policy is not strictly enforced.


```python
wikidata_user_agent_header = (
    None
    if not config.has_section("WIKIDATA")
    else config["WIKIDATA"]["WIKIDATA_USER_AGENT_HEADER"]
)
```

### Enable tracing if desired


```python
# import os
# os.environ["LANGCHAIN_HANDLER"] = "langchain"
# os.environ["LANGCHAIN_SESSION"] = "default" # Make sure this session actually exists.
```

# Tools

Three tools are provided for this simple agent:
* `ItemLookup`: for finding the q-number of an item
* `PropertyLookup`: for finding the p-number of a property
* `SparqlQueryRunner`: for running a sparql query

## Item and Property lookup

Item and Property lookup are implemented in a single method, using an elastic search endpoint. Not all wikibase instances have it, but wikidata does, and that's where we'll start.


```python
def get_nested_value(o: dict, path: list) -> any:
    current = o
    for key in path:
        try:
            current = current[key]
        except KeyError:
            return None
    return current


from typing import Optional

import requests


def vocab_lookup(
    search: str,
    entity_type: str = "item",
    url: str = "https://www.wikidata.org/w/api.php",
    user_agent_header: str = wikidata_user_agent_header,
    srqiprofile: str = None,
) -> Optional[str]:
    headers = {"Accept": "application/json"}
    if wikidata_user_agent_header is not None:
        headers["User-Agent"] = wikidata_user_agent_header

    if entity_type == "item":
        srnamespace = 0
        srqiprofile = "classic_noboostlinks" if srqiprofile is None else srqiprofile
    elif entity_type == "property":
        srnamespace = 120
        srqiprofile = "classic" if srqiprofile is None else srqiprofile
    else:
        raise ValueError("entity_type must be either 'property' or 'item'")

    params = {
        "action": "query",
        "list": "search",
        "srsearch": search,
        "srnamespace": srnamespace,
        "srlimit": 1,
        "srqiprofile": srqiprofile,
        "srwhat": "text",
        "format": "json",
    }

    response = requests.get(url, headers=headers, params=params)

    if response.status_code == 200:
        title = get_nested_value(response.json(), ["query", "search", 0, "title"])
        if title is None:
            return f"I couldn't find any {entity_type} for '{search}'. Please rephrase your request and try again"
        # if there is a prefix, strip it off
        return title.split(":")[-1]
    else:
        return "Sorry, I got an error. Please try again."
```


```python
print(vocab_lookup("Malin 1"))
```

    Q4180017
    


```python
print(vocab_lookup("instance of", entity_type="property"))
```

    P31
    


```python
print(vocab_lookup("Ceci n'est pas un q-item"))
```

    I couldn't find any item for 'Ceci n'est pas un q-item'. Please rephrase your request and try again
    

## Sparql runner 

This tool runs sparql - by default, wikidata is used.


```python
import json
from typing import Any, Dict, List

import requests


def run_sparql(
    query: str,
    url="https://query.wikidata.org/sparql",
    user_agent_header: str = wikidata_user_agent_header,
) -> List[Dict[str, Any]]:
    headers = {"Accept": "application/json"}
    if wikidata_user_agent_header is not None:
        headers["User-Agent"] = wikidata_user_agent_header

    response = requests.get(
        url, headers=headers, params={"query": query, "format": "json"}
    )

    if response.status_code != 200:
        return "That query failed. Perhaps you could try a different one?"
    results = get_nested_value(response.json(), ["results", "bindings"])
    return json.dumps(results)
```


```python
run_sparql("SELECT (COUNT(?children) as ?count) WHERE { wd:Q1339 wdt:P40 ?children . }")
```




    '[{"count": {"datatype": "http://www.w3.org/2001/XMLSchema#integer", "type": "literal", "value": "20"}}]'



# Agent

## Wrap the tools


```python
import re
from typing import List, Union

from langchain.agents import (
    AgentExecutor,
    AgentOutputParser,
    LLMSingleActionAgent,
    Tool,
)
from langchain.chains import LLMChain
from langchain.prompts import StringPromptTemplate
from langchain_core.agents import AgentAction, AgentFinish
```


```python
# Define which tools the agent can use to answer user queries
tools = [
    Tool(
        name="ItemLookup",
        func=(lambda x: vocab_lookup(x, entity_type="item")),
        description="useful for when you need to know the q-number for an item",
    ),
    Tool(
        name="PropertyLookup",
        func=(lambda x: vocab_lookup(x, entity_type="property")),
        description="useful for when you need to know the p-number for a property",
    ),
    Tool(
        name="SparqlQueryRunner",
        func=run_sparql,
        description="useful for getting results from a wikibase",
    ),
]
```

## Prompts


```python
# Set up the base template
template = """
Answer the following questions by running a sparql query against a wikibase where the p and q items are 
completely unknown to you. You will need to discover the p and q items before you can generate the sparql.
Do not assume you know the p and q items for any concepts. Always use tools to find all p and q items.
After you generate the sparql, you should run it. The results will be returned in json. 
Summarize the json results in natural language.

You may assume the following prefixes:
PREFIX wd: <http://www.wikidata.org/entity/>
PREFIX wdt: <http://www.wikidata.org/prop/direct/>
PREFIX p: <http://www.wikidata.org/prop/>
PREFIX ps: <http://www.wikidata.org/prop/statement/>

When generating sparql:
* Try to avoid "count" and "filter" queries if possible
* Never enclose the sparql in back-quotes

You have access to the following tools:

{tools}

Use the following format:

Question: the input question for which you must provide a natural language answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Question: {input}
{agent_scratchpad}"""
```


```python
# Set up a prompt template
class CustomPromptTemplate(StringPromptTemplate):
    # The template to use
    template: str
    # The list of tools available
    tools: List[Tool]

    def format(self, **kwargs) -> str:
        # Get the intermediate steps (AgentAction, Observation tuples)
        # Format them in a particular way
        intermediate_steps = kwargs.pop("intermediate_steps")
        thoughts = ""
        for action, observation in intermediate_steps:
            thoughts += action.log
            thoughts += f"\nObservation: {observation}\nThought: "
        # Set the agent_scratchpad variable to that value
        kwargs["agent_scratchpad"] = thoughts
        # Create a tools variable from the list of tools provided
        kwargs["tools"] = "\n".join(
            [f"{tool.name}: {tool.description}" for tool in self.tools]
        )
        # Create a list of tool names for the tools provided
        kwargs["tool_names"] = ", ".join([tool.name for tool in self.tools])
        return self.template.format(**kwargs)
```


```python
prompt = CustomPromptTemplate(
    template=template,
    tools=tools,
    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically
    # This includes the `intermediate_steps` variable because that is needed
    input_variables=["input", "intermediate_steps"],
)
```

## Output parser 
This is unchanged from langchain docs


```python
class CustomOutputParser(AgentOutputParser):
    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:
        # Check if agent should finish
        if "Final Answer:" in llm_output:
            return AgentFinish(
                # Return values is generally always a dictionary with a single `output` key
                # It is not recommended to try anything else at the moment :)
                return_values={"output": llm_output.split("Final Answer:")[-1].strip()},
                log=llm_output,
            )
        # Parse out the action and action input
        regex = r"Action: (.*?)[\n]*Action Input:[\s]*(.*)"
        match = re.search(regex, llm_output, re.DOTALL)
        if not match:
            raise ValueError(f"Could not parse LLM output: `{llm_output}`")
        action = match.group(1).strip()
        action_input = match.group(2)
        # Return the action and action input
        return AgentAction(
            tool=action, tool_input=action_input.strip(" ").strip('"'), log=llm_output
        )
```


```python
output_parser = CustomOutputParser()
```

## Specify the LLM model


```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4", temperature=0)
```

## Agent and agent executor


```python
# LLM chain consisting of the LLM and a prompt
llm_chain = LLMChain(llm=llm, prompt=prompt)
```


```python
tool_names = [tool.name for tool in tools]
agent = LLMSingleActionAgent(
    llm_chain=llm_chain,
    output_parser=output_parser,
    stop=["\nObservation:"],
    allowed_tools=tool_names,
)
```


```python
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
```

## Run it!


```python
# If you prefer in-line tracing, uncomment this line
# agent_executor.agent.llm_chain.verbose = True
```


```python
agent_executor.run("How many children did J.S. Bach have?")
```

    
    
    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3mThought: I need to find the Q number for J.S. Bach.
    Action: ItemLookup
    Action Input: J.S. Bach[0m
    
    Observation:[36;1m[1;3mQ1339[0m[32;1m[1;3mI need to find the P number for children.
    Action: PropertyLookup
    Action Input: children[0m
    
    Observation:[33;1m[1;3mP1971[0m[32;1m[1;3mNow I can query the number of children J.S. Bach had.
    Action: SparqlQueryRunner
    Action Input: SELECT ?children WHERE { wd:Q1339 wdt:P1971 ?children }[0m
    
    Observation:[38;5;200m[1;3m[{"children": {"datatype": "http://www.w3.org/2001/XMLSchema#decimal", "type": "literal", "value": "20"}}][0m[32;1m[1;3mI now know the final answer.
    Final Answer: J.S. Bach had 20 children.[0m
    
    [1m> Finished chain.[0m
    




    'J.S. Bach had 20 children.'




```python
agent_executor.run(
    "What is the Basketball-Reference.com NBA player ID of Hakeem Olajuwon?"
)
```

    
    
    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3mThought: To find Hakeem Olajuwon's Basketball-Reference.com NBA player ID, I need to first find his Wikidata item (Q-number) and then query for the relevant property (P-number).
    Action: ItemLookup
    Action Input: Hakeem Olajuwon[0m
    
    Observation:[36;1m[1;3mQ273256[0m[32;1m[1;3mNow that I have Hakeem Olajuwon's Wikidata item (Q273256), I need to find the P-number for the Basketball-Reference.com NBA player ID property.
    Action: PropertyLookup
    Action Input: Basketball-Reference.com NBA player ID[0m
    
    Observation:[33;1m[1;3mP2685[0m[32;1m[1;3mNow that I have both the Q-number for Hakeem Olajuwon (Q273256) and the P-number for the Basketball-Reference.com NBA player ID property (P2685), I can run a SPARQL query to get the ID value.
    Action: SparqlQueryRunner
    Action Input: 
    SELECT ?playerID WHERE {
      wd:Q273256 wdt:P2685 ?playerID .
    }[0m
    
    Observation:[38;5;200m[1;3m[{"playerID": {"type": "literal", "value": "o/olajuha01"}}][0m[32;1m[1;3mI now know the final answer
    Final Answer: Hakeem Olajuwon's Basketball-Reference.com NBA player ID is "o/olajuha01".[0m
    
    [1m> Finished chain.[0m
    




    'Hakeem Olajuwon\'s Basketball-Reference.com NBA player ID is "o/olajuha01".'




```python

```




################################################## wikidata.md ##################################################


# Wikidata

>[Wikidata](https://wikidata.org/) is a free and open knowledge base that can be read and edited by both humans and machines. Wikidata is one of the world's largest open knowledge bases.

First, you need to install `wikibase-rest-api-client` and `mediawikiapi` python packages.


```python
%pip install --upgrade --quiet "wikibase-rest-api-client<0.2" mediawikiapi
```


```python
from langchain_community.tools.wikidata.tool import WikidataAPIWrapper, WikidataQueryRun

wikidata = WikidataQueryRun(api_wrapper=WikidataAPIWrapper())

print(wikidata.run("Alan Turing"))
```

    Result Q7251:
    Label: Alan Turing
    Description: English computer scientist (1912–1954)
    Aliases: Alan M. Turing, Alan Mathieson Turing, Turing, Alan Mathison Turing
    instance of: human
    country of citizenship: United Kingdom
    occupation: computer scientist, mathematician, university teacher, cryptographer, logician, statistician, marathon runner, artificial intelligence researcher
    sex or gender: male
    date of birth: 1912-06-23
    date of death: 1954-06-07
    sport: athletics
    place of birth: Maida Vale, Warrington Lodge
    educated at: King's College, Princeton University, Sherborne School, Hazlehurst Community Primary School
    employer: Victoria University of Manchester, Government Communications Headquarters, University of Cambridge, National Physical Laboratory (United Kingdom)
    place of death: Wilmslow
    field of work: cryptanalysis, computer science, mathematics, logic, cryptography
    cause of death: cyanide poisoning
    notable work: On Computable Numbers, with an Application to the Entscheidungsproblem, Computing Machinery and Intelligence, Intelligent Machinery, halting problem, Turing machine, Turing test, Turing completeness, Church-Turing thesis, universal Turing machine, Symmetric Turing machine, non-deterministic Turing machine, Bombe, probabilistic Turing machine, Turing degree
    religion or worldview: atheism
    mother: Ethel Sara Stoney
    father: Julius Mathison Turing
    doctoral student: Robin Gandy, Beatrice Helen Worsley
    student: Robin Gandy
    
    Result Q28846012:
    Label: Alan Turing
    Description: fictional analogon of Alan Turing (1912-1954)
    Aliases: Alan Mathison Turing
    instance of: fictional human
    sex or gender: male
    


```python

```




################################################## wikipedia.md ##################################################


# Wikipedia

>[Wikipedia](https://wikipedia.org/) is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. `Wikipedia` is the largest and most-read reference work in history.

This notebook shows how to load wiki pages from `wikipedia.org` into the Document format that we use downstream.

## Installation

First, you need to install the `langchain_community` and `wikipedia` packages.


```python
%pip install -qU langchain_community wikipedia
```

## Parameters

`WikipediaLoader` has the following arguments:
- `query`: the free text which used to find documents in Wikipedia
- `lang` (optional): default="en". Use it to search in a specific language part of Wikipedia
- `load_max_docs` (optional): default=100. Use it to limit number of downloaded documents. It takes time to download all 100 documents, so use a small number for experiments. There is a hard limit of 300 for now.
- `load_all_available_meta` (optional): default=False. By default only the most important fields downloaded: `title` and `summary`. If `True` then all available fields will be downloaded.
- `doc_content_chars_max` (optional): default=4000. The maximum number of characters for the document content.

## Example


```python
from langchain_community.document_loaders import WikipediaLoader
```


```python
docs = WikipediaLoader(query="HUNTER X HUNTER", load_max_docs=2).load()
len(docs)
```




    2




```python
docs[0].metadata  # metadata of the first document
```




    {'title': 'Hunter × Hunter',
     'summary': 'Hunter × Hunter (pronounced "hunter hunter") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha\'s shōnen manga magazine Weekly Shōnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 37 tankōbon volumes as of November 2022. The story focuses on a young boy named Gon Freecss who discovers that his father, who left him at a young age, is actually a world-renowned Hunter, a licensed professional who specializes in fantastical pursuits such as locating rare or unidentified animal species, treasure hunting, surveying unexplored enclaves, or hunting down lawless individuals. Gon departs on a journey to become a Hunter and eventually find his father. Along the way, Gon meets various other Hunters and encounters the paranormal.\nHunter × Hunter was adapted into a 62-episode anime television series by Nippon Animation and directed by Kazuhiro Furuhashi, which ran on Fuji Television from October 1999 to March 2001. Three separate original video animations (OVAs) totaling 30 episodes were subsequently produced by Nippon Animation and released in Japan from 2002 to 2004. A second anime television series by Madhouse aired on Nippon Television from October 2011 to September 2014, totaling 148 episodes, with two animated theatrical films released in 2013. There are also numerous audio albums, video games, musicals, and other media based on Hunter × Hunter.\nThe manga has been licensed for English release in North America by Viz Media since April 2005. Both television series have been also licensed by Viz Media, with the first series having aired on the Funimation Channel in 2009 and the second series broadcast on Adult Swim\'s Toonami programming block from April 2016 to June 2019.\nHunter × Hunter has been a huge critical and financial success and has become one of the best-selling manga series of all time, having over 84 million copies in circulation by July 2022.',
     'source': 'https://en.wikipedia.org/wiki/Hunter_%C3%97_Hunter'}




```python
docs[0].page_content[:400]  # a part of the page content
```




    'Hunter × Hunter (pronounced "hunter hunter") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha\'s shōnen manga magazine Weekly Shōnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 37 tankōbon volumes as of November 2022. The story focuses on a young boy name'






################################################## wikipedia_search_analysis_agents.md ##################################################


# Wikipedia Search Analysis Agents

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MervinPraison/PraisonAI/blob/main/cookbooks/notebooks/wikipedia_search_analysis_agents.ipynb)

## Dependencies


```python
# Install dependencies without output
%pip install langchain_community > /dev/null
%pip install praisonai[crewai] > /dev/null
%pip install crawl4ai > /dev/null
```

## Tools


```python
# ToDo: Model unable to retreive Wikipidea Python Package
from langchain_community.utilities import WikipediaAPIWrapper
from praisonai_tools import BaseTool

class WikipediaSearchTool(BaseTool):
    name: str = "WikipediaSearchTool"
    description: str = "Search Wikipedia for relevant information based on a query."

    def _run(self, query: str):
        api_wrapper = WikipediaAPIWrapper(top_k_results=4, doc_content_chars_max=100)
        results = api_wrapper.load(query=query)
        return results

```

## YAML Prompt


```python
agent_yaml = """
framework: "crewai"
topic: "research about Nvidia growth"
roles:
  data_collector:
    role: "Data Collector"
    backstory: "An experienced researcher with the ability to efficiently collect and organize vast amounts of data."
    goal: "Gather information on Nvidia's growth by providing the Ticket Symbol to YahooFinanceNewsTool"
    tasks:
      data_collection_task:
        description: "Collect data on Nvidia's growth from various sources such as financial reports, news articles, and company announcements."
        expected_output: "A comprehensive document detailing data points on Nvidia's growth over the years."
    tools:
      - "WikipediaSearchTool"
  data_analyst:
    role: "Data Analyst"
    backstory: "Specializes in extracting insights from large datasets, proficient in quantitative and qualitative analysis."
    goal: "Analyze the collected data to identify trends and patterns"
    tasks:
      data_analysis_task:
        description: "Analyze the collected data to identify key trends and patterns in Nvidia's growth."
        expected_output: "An analytical report summarizing trends, patterns, and key growth metrics of Nvidia."
    tools: []
  report_preparer:
    role: "Report Preparer"
    backstory: "Experienced in creating detailed reports and presentations, turning analytical data into actionable insights."
    goal: "Generate a final report on Nvidia's growth"
    tasks:
      report_preparation_task:
        description: "Create a detailed report based on the analysis, highlighting Nvidia's growth trajectory."
        expected_output: "A polished, comprehensive report summarizing Nvidia's growth with visual aids and key insights."
    tools: []
dependencies: []
"""
```

## Main


```python
import os
from praisonai import PraisonAI
from google.colab import userdata

# Create a PraisonAI instance with the agent_yaml content
praisonai = PraisonAI(agent_yaml=agent_yaml, tools=[WikipediaSearchTool])

# Add OPENAI_API_KEY Secrets to Google Colab on the Left Hand Side 🔑 or Enter Manually Below
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY') or "ENTER OPENAI_API_KEY HERE"
os.environ["OPENAI_MODEL_NAME"] = "gpt-4o-mini"

# Run PraisonAI
result = praisonai.run()

# Print the result
print(result) # 2/10

```

    [1m[95m [2024-11-02 12:55:02][DEBUG]: == Working Agent: Data Collector[00m
    [1m[95m [2024-11-02 12:55:02][INFO]: == Starting Task: Collect data on Nvidia's growth from various sources such as financial reports, news articles, and company announcements.[00m
    
    
    [1m> Entering new CrewAgentExecutor chain...[0m
    [32;1m[1;3mAction: WikipediaSearchTool  
    Action Input: {"query": "Nvidia growth"}  [0m

    /usr/local/lib/python3.10/dist-packages/pydantic/main.py:214: UserWarning: A custom validator is returning a value other than `self`.
    Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.
    See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.
      warnings.warn(
    

    [91m 
    
    I encountered an error while trying to use the tool. This was the error: Could not import wikipedia python package. Please install it with `pip install wikipedia`..
     Tool WikipediaSearchTool accepts these inputs: WikipediaSearchTool(query: 'string') - Search Wikipedia for relevant information based on a query.
    [00m
    [32;1m[1;3mThought: I need to gather comprehensive information on Nvidia's growth to meet the task requirements. I will start by searching for relevant data on Wikipedia since that is the only tool available to me.  
    Action: WikipediaSearchTool  
    Action Input: {"query": "Nvidia"}  [0m

    /usr/local/lib/python3.10/dist-packages/pydantic/main.py:214: UserWarning: A custom validator is returning a value other than `self`.
    Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.
    See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.
      warnings.warn(
    

    [91m 
    
    I encountered an error while trying to use the tool. This was the error: Could not import wikipedia python package. Please install it with `pip install wikipedia`..
     Tool WikipediaSearchTool accepts these inputs: WikipediaSearchTool(query: 'string') - Search Wikipedia for relevant information based on a query.
    [00m
    [32;1m[1;3mThought: Since I cannot use the Wikipedia tool to gather information directly, I should utilize my knowledge and existing resources to compile a comprehensive overview of Nvidia's growth based on general trends and historical data.  
    Final Answer: 
    
    **Nvidia's Growth Overview**
    
    Nvidia Corporation, founded in 1993, has experienced significant growth over the years, evolving from a graphics processing unit (GPU) manufacturer to a leading player in the fields of artificial intelligence (AI), gaming, data centers, and automotive technology.
    
    1. **Early Years (1993-2006)**:
       - Nvidia initially focused on gaming graphics, gaining market share with its RIVA series of graphics cards.
       - The launch of the GeForce 256 in 1999 marked a significant leap, as it was marketed as the first GPU, enhancing gaming experiences.
    
    2. **Growth Phase (2006-2016)**:
       - Nvidia expanded its product portfolio beyond gaming into professional visualization and high-performance computing (HPC).
       - The acquisition of Ageia in 2008 allowed Nvidia to enter the physics processing market.
       - The introduction of CUDA (Compute Unified Device Architecture) in 2006 empowered developers to leverage GPU power for general-purpose computing, leading to its adoption in scientific computing and deep learning.
    
    3. **Rise of AI and Data Centers (2016-Present)**:
       - With the AI boom, Nvidia's GPUs became essential for deep learning applications. The company reported exponential growth in revenue from its data center segment.
       - The launch of the Tesla V100 GPU in 2017 targeted data centers and AI researchers, reinforcing Nvidia's leadership in AI.
       - In 2020, Nvidia announced the acquisition of ARM Holdings, aiming to enhance its position in mobile computing and IoT, although the deal faced regulatory scrutiny.
    
    4. **Financial Performance**:
       - Nvidia’s revenue growth has been remarkable, with a reported revenue of approximately $4 billion in 2016, which surged to over $26 billion by 2022.
       - The stock price also reflected this growth, with Nvidia shares experiencing a significant increase, particularly during the COVID-19 pandemic when demand for gaming and data center solutions skyrocketed.
    
    5. **Recent Developments**:
       - In 2023, Nvidia continued to innovate with the launch of new GPU architectures and products tailored for AI and machine learning, such as the H100 Tensor Core GPU.
       - Nvidia's growth narrative is further supported by its strategic partnerships and investments in AI startups, solidifying its ecosystem.
    
    6. **Market Position**:
       - Nvidia holds a dominant position in the GPU market, with a market share exceeding 80% in discrete GPUs for gaming as of 2023.
       - The company's forward-looking initiatives in AI, gaming, and autonomous driving technologies position it well for sustained growth in the coming years.
    
    In conclusion, Nvidia's growth trajectory from a specialized graphics company to a leader in AI and data centers illustrates its successful adaptation to market demands and technological advancements, setting a robust foundation for future expansion.[0m
    
    [1m> Finished chain.[0m
    [1m[92m [2024-11-02 12:55:10][DEBUG]: == [Data Collector] Task output: **Nvidia's Growth Overview**
    
    Nvidia Corporation, founded in 1993, has experienced significant growth over the years, evolving from a graphics processing unit (GPU) manufacturer to a leading player in the fields of artificial intelligence (AI), gaming, data centers, and automotive technology.
    
    1. **Early Years (1993-2006)**:
       - Nvidia initially focused on gaming graphics, gaining market share with its RIVA series of graphics cards.
       - The launch of the GeForce 256 in 1999 marked a significant leap, as it was marketed as the first GPU, enhancing gaming experiences.
    
    2. **Growth Phase (2006-2016)**:
       - Nvidia expanded its product portfolio beyond gaming into professional visualization and high-performance computing (HPC).
       - The acquisition of Ageia in 2008 allowed Nvidia to enter the physics processing market.
       - The introduction of CUDA (Compute Unified Device Architecture) in 2006 empowered developers to leverage GPU power for general-purpose computing, leading to its adoption in scientific computing and deep learning.
    
    3. **Rise of AI and Data Centers (2016-Present)**:
       - With the AI boom, Nvidia's GPUs became essential for deep learning applications. The company reported exponential growth in revenue from its data center segment.
       - The launch of the Tesla V100 GPU in 2017 targeted data centers and AI researchers, reinforcing Nvidia's leadership in AI.
       - In 2020, Nvidia announced the acquisition of ARM Holdings, aiming to enhance its position in mobile computing and IoT, although the deal faced regulatory scrutiny.
    
    4. **Financial Performance**:
       - Nvidia’s revenue growth has been remarkable, with a reported revenue of approximately $4 billion in 2016, which surged to over $26 billion by 2022.
       - The stock price also reflected this growth, with Nvidia shares experiencing a significant increase, particularly during the COVID-19 pandemic when demand for gaming and data center solutions skyrocketed.
    
    5. **Recent Developments**:
       - In 2023, Nvidia continued to innovate with the launch of new GPU architectures and products tailored for AI and machine learning, such as the H100 Tensor Core GPU.
       - Nvidia's growth narrative is further supported by its strategic partnerships and investments in AI startups, solidifying its ecosystem.
    
    6. **Market Position**:
       - Nvidia holds a dominant position in the GPU market, with a market share exceeding 80% in discrete GPUs for gaming as of 2023.
       - The company's forward-looking initiatives in AI, gaming, and autonomous driving technologies position it well for sustained growth in the coming years.
    
    In conclusion, Nvidia's growth trajectory from a specialized graphics company to a leader in AI and data centers illustrates its successful adaptation to market demands and technological advancements, setting a robust foundation for future expansion.
    
    [00m
    [1m[95m [2024-11-02 12:55:10][DEBUG]: == Working Agent: Data Analyst[00m
    [1m[95m [2024-11-02 12:55:10][INFO]: == Starting Task: Analyze the collected data to identify key trends and patterns in Nvidia's growth.[00m
    
    
    [1m> Entering new CrewAgentExecutor chain...[0m
    [32;1m[1;3mI now can give a great answer  
    Final Answer: 
    
    **Analytical Report on Nvidia's Growth Trends and Patterns**
    
    **1. Introduction**  
    Nvidia Corporation has witnessed a remarkable transformation since its inception in 1993. Initially focusing on the gaming graphics sector, the company has adeptly expanded its footprint into artificial intelligence (AI), data centers, and automotive technology. This report aims to summarize the key trends, patterns, and growth metrics that underscore Nvidia's impressive journey.
    
    **2. Early Years (1993-2006)**  
    During its formative years, Nvidia carved out a niche in gaming graphics.  
    - **Product Innovation**: The introduction of the GeForce 256 in 1999, heralded as the first GPU, significantly enhanced gaming experiences and positioned Nvidia as a leader in the graphics market.  
    - **Market Share**: The RIVA series of graphics cards contributed to steady market share growth, establishing a strong foundation for future advancements.
    
    **3. Growth Phase (2006-2016)**  
    Nvidia's strategic diversification beyond gaming marked this decade.  
    - **Product Portfolio Expansion**: The acquisition of Ageia in 2008 allowed Nvidia to penetrate the physics processing market, while the launch of CUDA in 2006 opened new avenues for developers in scientific and general-purpose computing.  
    - **High-Performance Computing (HPC)**: The company's entry into HPC strengthened its position in professional visualization and research sectors, leading to increased adoption in academic and corporate environments.
    
    **4. Rise of AI and Data Centers (2016-Present)**  
    The most significant growth phase for Nvidia has been driven by the AI revolution.  
    - **Revenue Surge**: The data center segment has reported exponential revenue growth, with notable products like the Tesla V100 GPU launched in 2017, aimed explicitly at AI researchers and data centers.  
    - **Strategic Acquisitions**: The planned acquisition of ARM Holdings in 2020, despite regulatory challenges, indicates Nvidia’s strategic intent to dominate mobile computing and IoT markets.
    
    **5. Financial Performance**  
    Nvidia's financial trajectory paints a picture of outstanding growth:  
    - **Revenue Growth**: Revenue climbed from approximately $4 billion in 2016 to over $26 billion by 2022, showcasing a compound annual growth rate (CAGR) of over 30%.  
    - **Stock Performance**: Nvidia's stock has reflected its financial success, significantly appreciating during the COVID-19 pandemic due to increased demand for gaming and data center solutions.
    
    **6. Recent Developments**  
    In 2023, Nvidia's innovation continued unabated:  
    - **New Product Launches**: The introduction of advanced GPU architectures, such as the H100 Tensor Core GPU, demonstrates Nvidia's commitment to maintaining its leadership in AI and machine learning sectors.  
    - **Ecosystem Expansion**: Strategic partnerships and investments in AI startups further solidify Nvidia's ecosystem, enhancing its competitive positioning.
    
    **7. Market Position**  
    Nvidia’s dominance in the GPU market is evident:  
    - **Market Share**: As of 2023, Nvidia holds over 80% of the discrete GPU market for gaming, underscoring its stronghold in the sector.  
    - **Future Growth Potential**: The company's proactive initiatives in AI, gaming, and autonomous technologies indicate a well-structured strategy for sustainable growth.
    
    **8. Conclusion**  
    Nvidia's evolution from a specialized graphics manufacturer to a leader in AI and data centers exemplifies its effective adaptation to changing market dynamics and technological advancements. The company’s strategic innovations, coupled with its financial performance and market dominance, lay a robust foundation for continued expansion and success in the future. This analytical overview highlights the critical trends and patterns that have defined Nvidia's growth narrative, providing insights into its trajectory and future potential in the tech landscape.[0m
    
    [1m> Finished chain.[0m
    [1m[92m [2024-11-02 12:55:16][DEBUG]: == [Data Analyst] Task output: **Analytical Report on Nvidia's Growth Trends and Patterns**
    
    **1. Introduction**  
    Nvidia Corporation has witnessed a remarkable transformation since its inception in 1993. Initially focusing on the gaming graphics sector, the company has adeptly expanded its footprint into artificial intelligence (AI), data centers, and automotive technology. This report aims to summarize the key trends, patterns, and growth metrics that underscore Nvidia's impressive journey.
    
    **2. Early Years (1993-2006)**  
    During its formative years, Nvidia carved out a niche in gaming graphics.  
    - **Product Innovation**: The introduction of the GeForce 256 in 1999, heralded as the first GPU, significantly enhanced gaming experiences and positioned Nvidia as a leader in the graphics market.  
    - **Market Share**: The RIVA series of graphics cards contributed to steady market share growth, establishing a strong foundation for future advancements.
    
    **3. Growth Phase (2006-2016)**  
    Nvidia's strategic diversification beyond gaming marked this decade.  
    - **Product Portfolio Expansion**: The acquisition of Ageia in 2008 allowed Nvidia to penetrate the physics processing market, while the launch of CUDA in 2006 opened new avenues for developers in scientific and general-purpose computing.  
    - **High-Performance Computing (HPC)**: The company's entry into HPC strengthened its position in professional visualization and research sectors, leading to increased adoption in academic and corporate environments.
    
    **4. Rise of AI and Data Centers (2016-Present)**  
    The most significant growth phase for Nvidia has been driven by the AI revolution.  
    - **Revenue Surge**: The data center segment has reported exponential revenue growth, with notable products like the Tesla V100 GPU launched in 2017, aimed explicitly at AI researchers and data centers.  
    - **Strategic Acquisitions**: The planned acquisition of ARM Holdings in 2020, despite regulatory challenges, indicates Nvidia’s strategic intent to dominate mobile computing and IoT markets.
    
    **5. Financial Performance**  
    Nvidia's financial trajectory paints a picture of outstanding growth:  
    - **Revenue Growth**: Revenue climbed from approximately $4 billion in 2016 to over $26 billion by 2022, showcasing a compound annual growth rate (CAGR) of over 30%.  
    - **Stock Performance**: Nvidia's stock has reflected its financial success, significantly appreciating during the COVID-19 pandemic due to increased demand for gaming and data center solutions.
    
    **6. Recent Developments**  
    In 2023, Nvidia's innovation continued unabated:  
    - **New Product Launches**: The introduction of advanced GPU architectures, such as the H100 Tensor Core GPU, demonstrates Nvidia's commitment to maintaining its leadership in AI and machine learning sectors.  
    - **Ecosystem Expansion**: Strategic partnerships and investments in AI startups further solidify Nvidia's ecosystem, enhancing its competitive positioning.
    
    **7. Market Position**  
    Nvidia’s dominance in the GPU market is evident:  
    - **Market Share**: As of 2023, Nvidia holds over 80% of the discrete GPU market for gaming, underscoring its stronghold in the sector.  
    - **Future Growth Potential**: The company's proactive initiatives in AI, gaming, and autonomous technologies indicate a well-structured strategy for sustainable growth.
    
    **8. Conclusion**  
    Nvidia's evolution from a specialized graphics manufacturer to a leader in AI and data centers exemplifies its effective adaptation to changing market dynamics and technological advancements. The company’s strategic innovations, coupled with its financial performance and market dominance, lay a robust foundation for continued expansion and success in the future. This analytical overview highlights the critical trends and patterns that have defined Nvidia's growth narrative, providing insights into its trajectory and future potential in the tech landscape.
    
    [00m
    [1m[95m [2024-11-02 12:55:16][DEBUG]: == Working Agent: Report Preparer[00m
    [1m[95m [2024-11-02 12:55:16][INFO]: == Starting Task: Create a detailed report based on the analysis, highlighting Nvidia's growth trajectory.[00m
    
    
    [1m> Entering new CrewAgentExecutor chain...[0m
    [32;1m[1;3mI now can give a great answer  
    Final Answer:
    
    **Analytical Report on Nvidia's Growth Trends and Patterns**
    
    **1. Introduction**  
    Nvidia Corporation has witnessed a remarkable transformation since its inception in 1993. Initially focusing on the gaming graphics sector, the company has adeptly expanded its footprint into artificial intelligence (AI), data centers, and automotive technology. This report aims to summarize the key trends, patterns, and growth metrics that underscore Nvidia's impressive journey.
    
    **2. Early Years (1993-2006)**  
    During its formative years, Nvidia carved out a niche in gaming graphics.  
    - **Product Innovation**: The introduction of the GeForce 256 in 1999, heralded as the first GPU, significantly enhanced gaming experiences and positioned Nvidia as a leader in the graphics market.  
    - **Market Share**: The RIVA series of graphics cards contributed to steady market share growth, establishing a strong foundation for future advancements.
    
    **3. Growth Phase (2006-2016)**  
    Nvidia's strategic diversification beyond gaming marked this decade.  
    - **Product Portfolio Expansion**: The acquisition of Ageia in 2008 allowed Nvidia to penetrate the physics processing market, while the launch of CUDA in 2006 opened new avenues for developers in scientific and general-purpose computing.  
    - **High-Performance Computing (HPC)**: The company's entry into HPC strengthened its position in professional visualization and research sectors, leading to increased adoption in academic and corporate environments.
    
    **4. Rise of AI and Data Centers (2016-Present)**  
    The most significant growth phase for Nvidia has been driven by the AI revolution.  
    - **Revenue Surge**: The data center segment has reported exponential revenue growth, with notable products like the Tesla V100 GPU launched in 2017, aimed explicitly at AI researchers and data centers.  
    - **Strategic Acquisitions**: The planned acquisition of ARM Holdings in 2020, despite regulatory challenges, indicates Nvidia’s strategic intent to dominate mobile computing and IoT markets.
    
    **5. Financial Performance**  
    Nvidia's financial trajectory paints a picture of outstanding growth:  
    - **Revenue Growth**: Revenue climbed from approximately $4 billion in 2016 to over $26 billion by 2022, showcasing a compound annual growth rate (CAGR) of over 30%.  
    - **Stock Performance**: Nvidia's stock has reflected its financial success, significantly appreciating during the COVID-19 pandemic due to increased demand for gaming and data center solutions.
    
    **6. Recent Developments**  
    In 2023, Nvidia's innovation continued unabated:  
    - **New Product Launches**: The introduction of advanced GPU architectures, such as the H100 Tensor Core GPU, demonstrates Nvidia's commitment to maintaining its leadership in AI and machine learning sectors.  
    - **Ecosystem Expansion**: Strategic partnerships and investments in AI startups further solidify Nvidia's ecosystem, enhancing its competitive positioning.
    
    **7. Market Position**  
    Nvidia’s dominance in the GPU market is evident:  
    - **Market Share**: As of 2023, Nvidia holds over 80% of the discrete GPU market for gaming, underscoring its stronghold in the sector.  
    - **Future Growth Potential**: The company's proactive initiatives in AI, gaming, and autonomous technologies indicate a well-structured strategy for sustainable growth.
    
    **8. Conclusion**  
    Nvidia's evolution from a specialized graphics manufacturer to a leader in AI and data centers exemplifies its effective adaptation to changing market dynamics and technological advancements. The company’s strategic innovations, coupled with its financial performance and market dominance, lay a robust foundation for continued expansion and success in the future. This analytical overview highlights the critical trends and patterns that have defined Nvidia's growth narrative, providing insights into its trajectory and future potential in the tech landscape.
    
    **Visual Aids**  
    1. **Revenue Growth Chart**: A bar graph showcasing Nvidia's revenue from 2016 to 2022, illustrating the CAGR of over 30%.
    2. **Market Share Pie Chart**: A pie chart representing Nvidia's market share in the discrete GPU market in 2023, with a clear indication of its 80% dominance.
    3. **Product Innovation Timeline**: A timeline highlighting key product launches and acquisitions from 1993 to 2023, providing a visual representation of Nvidia's growth milestones.
    
    This report serves as a comprehensive overview of Nvidia's growth trajectory, emphasizing its strategic initiatives, market performance, and future opportunities, thus positioning it as a formidable player in the technology landscape.[0m
    
    [1m> Finished chain.[0m
    [1m[92m [2024-11-02 12:55:27][DEBUG]: == [Report Preparer] Task output: **Analytical Report on Nvidia's Growth Trends and Patterns**
    
    **1. Introduction**  
    Nvidia Corporation has witnessed a remarkable transformation since its inception in 1993. Initially focusing on the gaming graphics sector, the company has adeptly expanded its footprint into artificial intelligence (AI), data centers, and automotive technology. This report aims to summarize the key trends, patterns, and growth metrics that underscore Nvidia's impressive journey.
    
    **2. Early Years (1993-2006)**  
    During its formative years, Nvidia carved out a niche in gaming graphics.  
    - **Product Innovation**: The introduction of the GeForce 256 in 1999, heralded as the first GPU, significantly enhanced gaming experiences and positioned Nvidia as a leader in the graphics market.  
    - **Market Share**: The RIVA series of graphics cards contributed to steady market share growth, establishing a strong foundation for future advancements.
    
    **3. Growth Phase (2006-2016)**  
    Nvidia's strategic diversification beyond gaming marked this decade.  
    - **Product Portfolio Expansion**: The acquisition of Ageia in 2008 allowed Nvidia to penetrate the physics processing market, while the launch of CUDA in 2006 opened new avenues for developers in scientific and general-purpose computing.  
    - **High-Performance Computing (HPC)**: The company's entry into HPC strengthened its position in professional visualization and research sectors, leading to increased adoption in academic and corporate environments.
    
    **4. Rise of AI and Data Centers (2016-Present)**  
    The most significant growth phase for Nvidia has been driven by the AI revolution.  
    - **Revenue Surge**: The data center segment has reported exponential revenue growth, with notable products like the Tesla V100 GPU launched in 2017, aimed explicitly at AI researchers and data centers.  
    - **Strategic Acquisitions**: The planned acquisition of ARM Holdings in 2020, despite regulatory challenges, indicates Nvidia’s strategic intent to dominate mobile computing and IoT markets.
    
    **5. Financial Performance**  
    Nvidia's financial trajectory paints a picture of outstanding growth:  
    - **Revenue Growth**: Revenue climbed from approximately $4 billion in 2016 to over $26 billion by 2022, showcasing a compound annual growth rate (CAGR) of over 30%.  
    - **Stock Performance**: Nvidia's stock has reflected its financial success, significantly appreciating during the COVID-19 pandemic due to increased demand for gaming and data center solutions.
    
    **6. Recent Developments**  
    In 2023, Nvidia's innovation continued unabated:  
    - **New Product Launches**: The introduction of advanced GPU architectures, such as the H100 Tensor Core GPU, demonstrates Nvidia's commitment to maintaining its leadership in AI and machine learning sectors.  
    - **Ecosystem Expansion**: Strategic partnerships and investments in AI startups further solidify Nvidia's ecosystem, enhancing its competitive positioning.
    
    **7. Market Position**  
    Nvidia’s dominance in the GPU market is evident:  
    - **Market Share**: As of 2023, Nvidia holds over 80% of the discrete GPU market for gaming, underscoring its stronghold in the sector.  
    - **Future Growth Potential**: The company's proactive initiatives in AI, gaming, and autonomous technologies indicate a well-structured strategy for sustainable growth.
    
    **8. Conclusion**  
    Nvidia's evolution from a specialized graphics manufacturer to a leader in AI and data centers exemplifies its effective adaptation to changing market dynamics and technological advancements. The company’s strategic innovations, coupled with its financial performance and market dominance, lay a robust foundation for continued expansion and success in the future. This analytical overview highlights the critical trends and patterns that have defined Nvidia's growth narrative, providing insights into its trajectory and future potential in the tech landscape.
    
    **Visual Aids**  
    1. **Revenue Growth Chart**: A bar graph showcasing Nvidia's revenue from 2016 to 2022, illustrating the CAGR of over 30%.
    2. **Market Share Pie Chart**: A pie chart representing Nvidia's market share in the discrete GPU market in 2023, with a clear indication of its 80% dominance.
    3. **Product Innovation Timeline**: A timeline highlighting key product launches and acquisitions from 1993 to 2023, providing a visual representation of Nvidia's growth milestones.
    
    This report serves as a comprehensive overview of Nvidia's growth trajectory, emphasizing its strategic initiatives, market performance, and future opportunities, thus positioning it as a formidable player in the technology landscape.
    
    [00m
    


<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">### Task Output ###
**Analytical Report on Nvidia's Growth Trends and Patterns**

**<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>. Introduction**  
Nvidia Corporation has witnessed a remarkable transformation since its inception in <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1993</span>. Initially focusing on the
gaming graphics sector, the company has adeptly expanded its footprint into artificial intelligence <span style="font-weight: bold">(</span>AI<span style="font-weight: bold">)</span>, data 
centers, and automotive technology. This report aims to summarize the key trends, patterns, and growth metrics that
underscore Nvidia's impressive journey.

**<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>. Early Years <span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1993</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2006</span><span style="font-weight: bold">)</span>**  
During its formative years, Nvidia carved out a niche in gaming graphics.  
- **Product Innovation**: The introduction of the GeForce <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span> in <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1999</span>, heralded as the first GPU, significantly 
enhanced gaming experiences and positioned Nvidia as a leader in the graphics market.  
- **Market Share**: The RIVA series of graphics cards contributed to steady market share growth, establishing a 
strong foundation for future advancements.

**<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>. Growth Phase <span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2006</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2016</span><span style="font-weight: bold">)</span>**  
Nvidia's strategic diversification beyond gaming marked this decade.  
- **Product Portfolio Expansion**: The acquisition of Ageia in <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2008</span> allowed Nvidia to penetrate the physics 
processing market, while the launch of CUDA in <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2006</span> opened new avenues for developers in scientific and 
general-purpose computing.  
- **High-Performance Computing <span style="font-weight: bold">(</span>HPC<span style="font-weight: bold">)</span>**: The company's entry into HPC strengthened its position in professional 
visualization and research sectors, leading to increased adoption in academic and corporate environments.

**<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>. Rise of AI and Data Centers <span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2016</span>-Present<span style="font-weight: bold">)</span>**  
The most significant growth phase for Nvidia has been driven by the AI revolution.  
- **Revenue Surge**: The data center segment has reported exponential revenue growth, with notable products like 
the Tesla V100 GPU launched in <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2017</span>, aimed explicitly at AI researchers and data centers.  
- **Strategic Acquisitions**: The planned acquisition of ARM Holdings in <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2020</span>, despite regulatory challenges, 
indicates Nvidia’s strategic intent to dominate mobile computing and IoT markets.

**<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>. Financial Performance**  
Nvidia's financial trajectory paints a picture of outstanding growth:  
- **Revenue Growth**: Revenue climbed from approximately $<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span> billion in <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2016</span> to over $<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">26</span> billion by <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2022</span>, showcasing
a compound annual growth rate <span style="font-weight: bold">(</span>CAGR<span style="font-weight: bold">)</span> of over <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">30</span>%.  
- **Stock Performance**: Nvidia's stock has reflected its financial success, significantly appreciating during the 
COVID-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19</span> pandemic due to increased demand for gaming and data center solutions.

**<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>. Recent Developments**  
In <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2023</span>, Nvidia's innovation continued unabated:  
- **New Product Launches**: The introduction of advanced GPU architectures, such as the H100 Tensor Core GPU, 
demonstrates Nvidia's commitment to maintaining its leadership in AI and machine learning sectors.  
- **Ecosystem Expansion**: Strategic partnerships and investments in AI startups further solidify Nvidia's 
ecosystem, enhancing its competitive positioning.

**<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>. Market Position**  
Nvidia’s dominance in the GPU market is evident:  
- **Market Share**: As of <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2023</span>, Nvidia holds over <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">80</span>% of the discrete GPU market for gaming, underscoring its 
stronghold in the sector.  
- **Future Growth Potential**: The company's proactive initiatives in AI, gaming, and autonomous technologies 
indicate a well-structured strategy for sustainable growth.

**<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>. Conclusion**  
Nvidia's evolution from a specialized graphics manufacturer to a leader in AI and data centers exemplifies its 
effective adaptation to changing market dynamics and technological advancements. The company’s strategic 
innovations, coupled with its financial performance and market dominance, lay a robust foundation for continued 
expansion and success in the future. This analytical overview highlights the critical trends and patterns that have
defined Nvidia's growth narrative, providing insights into its trajectory and future potential in the tech 
landscape.

**Visual Aids**  
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>. **Revenue Growth Chart**: A bar graph showcasing Nvidia's revenue from <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2016</span> to <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2022</span>, illustrating the CAGR of 
over <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">30</span>%.
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>. **Market Share Pie Chart**: A pie chart representing Nvidia's market share in the discrete GPU market in <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2023</span>, 
with a clear indication of its <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">80</span>% dominance.
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>. **Product Innovation Timeline**: A timeline highlighting key product launches and acquisitions from <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1993</span> to 
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2023</span>, providing a visual representation of Nvidia's growth milestones.

This report serves as a comprehensive overview of Nvidia's growth trajectory, emphasizing its strategic 
initiatives, market performance, and future opportunities, thus positioning it as a formidable player in the 
technology landscape.
</pre>



    None
    


```python

```




################################################## wolfram_alpha.md ##################################################


# Wolfram Alpha

This notebook goes over how to use the wolfram alpha component.

First, you need to set up your Wolfram Alpha developer account and get your APP ID:

1. Go to wolfram alpha and sign up for a developer account [here](https://developer.wolframalpha.com/)
2. Create an app and get your APP ID
3. pip install wolframalpha

Then we will need to set some environment variables:
1. Save your APP ID into WOLFRAM_ALPHA_APPID env variable


```python
pip install wolframalpha
```


```python
import os

os.environ["WOLFRAM_ALPHA_APPID"] = ""
```


```python
from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper
```


```python
wolfram = WolframAlphaAPIWrapper()
```


```python
wolfram.run("What is 2x+5 = -3x + 7?")
```




    'x = 2/5'




```python

```




################################################## Working_with_Charts_Graphs_and_Slide_Decks.md ##################################################


##### Copyright 2024 Google LLC.


```
# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# Gemini API: Working with Charts, Graphs, and Slide Decks

Gemini models are powerful multimodal LLMs that can process both text and image inputs.

This notebook shows how Gemini 1.5 Flash model is capable of extracting data from various images.

<table class="tfo-notebook-buttons" align="left">
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Working_with_Charts_Graphs_and_Slide_Decks.ipynb"><img src = "../images/colab_logo_32px.png"/>Run in Google Colab</a>
  </td>
</table>


```
!pip install -U -q "google-generativeai>=0.7.2"
```


```
import os
import time
from glob import glob

import google.generativeai as genai
from PIL import Image
from IPython.display import Markdown, display
```

## Configure your API key

To run the following cell, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.



```
from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

genai.configure(api_key=GOOGLE_API_KEY)
```

## Setup
You will be using images from [Priyanka Vergadia's](https://github.com/priyankavergadia) [GCPSketchnote](https://github.com/priyankavergadia/GCPSketchnote) repository. These pages contain many details that should provide a good benchmark for Gemini's capabilities.

These images are on [Creative Commons Attribution 4.0 International Public License](https://github.com/priyankavergadia/GCPSketchnote/tree/main?tab=License-1-ov-file).


```
! git clone https://github.com/priyankavergadia/GCPSketchnote.git
```

    Cloning into 'GCPSketchnote'...
    remote: Enumerating objects: 311, done.[K
    remote: Counting objects: 100% (61/61), done.[K
    remote: Compressing objects: 100% (54/54), done.[K
    remote: Total 311 (delta 7), reused 45 (delta 7), pack-reused 250 (from 1)[K
    Receiving objects: 100% (311/311), 117.09 MiB | 12.72 MiB/s, done.
    Resolving deltas: 100% (104/104), done.
    Updating files: 100% (89/89), done.
    


```
images_with_duplicates = glob("/content/GCPSketchnote/images/*")

# Removing duplicate images with different extensions
images = []
encountered = set()
for path in images_with_duplicates:
  path_without_extension, extension = os.path.splitext(path)
  if path_without_extension not in encountered and extension != ".pdf":
    images.append(path)
    encountered.add(path_without_extension)
```


```
len(images)
```




    72



## Interpreting a single chart



```
chart_path_gif = "chart.gif"
!curl https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_final_text_table_amendment_13_12_23.gif > $chart_path_gif
```

      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
    100  129k  100  129k    0     0   397k      0 --:--:-- --:--:-- --:--:--  397k
    

The image needs to be transformed into a `.jpg`, since `.gif` is not supported by Gemini API at the moment.


```
chart_path_jpg = "chart.jpg"
image = Image.open(chart_path_gif)
image = image.convert('RGB')
image.save(chart_path_jpg)
```

Now, you will define helper functions for shrinking the image and querying the model with images.

**NOTE**: In this example you will be using Pillow library to load images, but using `Image` from `IPython.display`, or using dictionary with `mime_type` and data fields will also work.


```
# Make images fit better on screen and decrease data used for requests
def shrink_image(image: Image, ratio = 2):
  width, height = image.size
  return image.convert('RGB').resize((width//ratio, height//ratio))
```


```
def generate_content_from_image(prompt, image_paths):
  model = genai.GenerativeModel('models/gemini-1.5-flash-latest')
  response = model.generate_content(
    contents=[prompt] + [shrink_image(Image.open(image_path)) for image_path in image_paths],
    request_options={"timeout": 600}
  )
  return response.text
```


```
shrink_image(Image.open(chart_path_jpg), 3)
```




    
![png](output_19_0.png)
    



Now, let's see how the LLM can handle the following query.


```
prompt = """You are a tool that interprets tables. Which model (Gemini Ultra or GPT-4) is better in the 'Math' category in MATH benchmark?"""
Markdown(generate_content_from_image(prompt, [chart_path_jpg]))
```




According to the provided table, Gemini Ultra scores 94.4% on GSM8K benchmark in the 'Math' category, while GPT-4 scores 92.0%. Therefore, Gemini Ultra is better in the 'Math' category in MATH benchmark.



## Extracting information from a single slide
You will use the model to extract information from a single slide. In this case graph describing pub/sub. It is not a complicated usecase, however it will showcase, how you can call the model.

You need to download an example chart.



```
image_path = "/content/GCPSketchnote/images/pubsub.jpg"
```


```
shrink_image(Image.open(image_path))
```




    
![png](output_24_0.png)
    



Start with something simple:


```
prompt = "Describe the image in 5 sentences."
response = generate_content_from_image(prompt, [image_path])
Markdown(response)
```




The image is a diagram explaining the concept of Pub/Sub, a cloud messaging service. It shows the different use cases of Pub/Sub and how it can be used for real-time data processing, stream analytics, and event-driven architectures. The diagram also highlights the different patterns of Pub/Sub like Many-to-one, Many-to-many, and One-to-many. In the middle, a simplified message flow is presented, illustrating the steps involved in publishing and subscribing to messages. The diagram is accompanied by hand-drawn illustrations and humorous captions, making it engaging and informative.  




You can also use it to extract information from specific parts of the image:


```
prompt = "Explain the diffrent pub sub patters using the image. Ignore the rest."
response = generate_content_from_image(prompt, [image_path])
Markdown(response)
```




The image shows three different Pub/Sub patterns:

* **Many-to-One (Fan-In):** In this pattern, multiple publishers send messages to a single topic. A single subscriber listens to this topic and consumes all the messages.
* **Many-to-Many:** This pattern allows multiple publishers to send messages to a single topic and multiple subscribers can consume messages from that topic. It is useful for load balancing and ensuring that messages are distributed across multiple consumers.
* **One-to-Many (Fan-Out):**  This pattern has a single publisher sending messages to a topic, and multiple subscribers are listening to that topic.  This pattern is used to distribute messages to a large number of consumers.



## Slide Decks
While most models can receive only a handful images at once, The Gemini 1.5 Flash model is able to receive up to 3,600 images in a single request. This means that most slide decks can be passed without any splitting to the model.

In this case you will use the LLM to create a set of questions that check the knowledge of GCP products:


```
prompt = "Your job is to create a set of questions to check knowledge of various gcp products. Write for each image the topic and example question."
response = generate_content_from_image(prompt, images[:4])
Markdown(response)
```




## Image 1: BeyondCorp & BeyondCorp Enterprise

**Topic:** BeyondCorp and BeyondCorp Enterprise

**Example Question:** What is the difference between BeyondCorp and BeyondCorp Enterprise? 

## Image 2: Datastream 

**Topic:** Datastream

**Example Question:** What are the different connectivity options available for Datastream? 

## Image 3: Dataproc

**Topic:** Dataproc

**Example Question:** How does Dataproc disaggregate storage and compute? 

## Image 4: Introduction to Cloud Networking

**Topic:** Google Cloud Network Infrastructure

**Example Question:** What are the different networking services offered by Google Cloud? 




# Summary

The Gemini API's great capabilities in processing images such as charts, graphs, and slide decks highlights the power of multimodal LLMs. Thanks to the model's ability to read and understand these visual elements, everyone can unlock great ideas, simplify tasks, and save valuable time.

Imagine the impact of leveraging Gemini API to implement AI solutions that describe surroundings for the disabled community, making technology more inclusive and accessible to all.

This is just one of the exciting possibilities. Now, it's your turn to explore Gemini 1.5 Flash further!




################################################## working_with_functions.md ##################################################


# Working with functions in Azure OpenAI
This notebook shows how to use the Chat Completions API in combination with functions to extend the current capabilities of GPT models. GPT models, do not inherently support real-time interaction with external systems, databases, or files. However, functions can be used to do so.

Overview: <br>
`tools` (previously called `functions`) is an optional parameter in the Chat Completion API which can be used to provide function specifications. This allows models to generate function arguments for the specifications provided by the user. 

Note: The API will not execute any function calls. Executing function calls using the outputed argments must be done by developers. 

## Setup


```python
# if needed, install and/or upgrade to the latest version of the OpenAI Python library
# %pip install --upgrade openai
```


```python
import os
import json
from openai import AzureOpenAI

# Load config values
with open(r"config.json") as config_file:
    config_details = json.load(config_file)

client = AzureOpenAI(
    azure_endpoint=config_details["AZURE_OPENAI_ENDPOINT"],  # The base URL for your Azure OpenAI resource. e.g. "https://<your resource name>.openai.azure.com"
    api_key=os.getenv("AZURE_OPENAI_KEY"),  # The API key for your Azure OpenAI resource.
    api_version=config_details["OPENAI_API_VERSION"],  # This version supports function calling
)

model_name = config_details["MODEL_NAME"] # You need to ensure the version of the model you are using supports the function calling feature
```

## 1.0 Test functions

This code calls the model with the user query and the set of functions defined in the functions parameter. The model then can choose if it calls a function. If a function is called, the content will be in a strigified JSON object. The function call that should be made and arguments are location in:  response[`choices`][0][`function_call`].


```python
def get_function_call(messages, tool_choice="auto"):
    # Define the functions to use
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_current_weather",
                "description": "Get the current weather in a given location",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA",
                        },
                        "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                    },
                    "required": ["location"],
                },
            },
        },
    ]

    # Call the model with the user query (messages) and the functions defined in the functions parameter
    response = client.chat.completions.create(
        model=model_name,
        messages=messages,
        tools=tools,
        tool_choice=tool_choice,
    )

    return response
```

### Forcing the use of a specific function or no function
By changing the value of the `tool_choice` parameter you can allow the model to decide what function to use, force the model to use a specific function, or force the model to use no function.


```python
first_message = [
    {"role": "user", "content": "What's the weather like in San Francisco?"}
]
# 'auto' : Let the model decide what function to call
print("Let the model decide what function to call:")
print(get_function_call(first_message, "auto").choices[0].message)

# 'none' : Don't call any function
print("Don't call any function:")
print(get_function_call(first_message, "none").choices[0].message)

# force a specific function call
print("Force a specific function call:")
print(
    get_function_call(
        first_message,
        tool_choice={"type": "function", "function": {"name": "get_current_weather"}},
    )
    .choices[0]
    .message
)
```

## 2.0 Defining functions
Now that we know how to work with functions, let's define some functions in code so that we can walk through the process of using functions end to end.

### Function #1: Get current time


```python
import pytz
from datetime import datetime


def get_current_time(location):
    try:
        # Get the timezone for the city
        timezone = pytz.timezone(location)

        # Get the current time in the timezone
        now = datetime.now(timezone)
        current_time = now.strftime("%I:%M:%S %p")

        return current_time
    except:
        return "Sorry, I couldn't find the timezone for that location."
```


```python
get_current_time("America/New_York")
```

### Function #2: Get stock market data
For simplicity, we're just hard coding some stock market data but you could easily edit the code to call out to an API to retrieve real-time data.


```python
import pandas as pd
import json


def get_stock_market_data(index):
    available_indices = [
        "S&P 500",
        "NASDAQ Composite",
        "Dow Jones Industrial Average",
        "Financial Times Stock Exchange 100 Index",
    ]

    if index not in available_indices:
        return "Invalid index. Please choose from 'S&P 500', 'NASDAQ Composite', 'Dow Jones Industrial Average', 'Financial Times Stock Exchange 100 Index'."

    # Read the CSV file
    data = pd.read_csv("stock_data.csv")

    # Filter data for the given index
    data_filtered = data[data["Index"] == index]

    # Remove 'Index' column
    data_filtered = data_filtered.drop(columns=["Index"])

    # Convert the DataFrame into a dictionary
    hist_dict = data_filtered.to_dict()

    for key, value_dict in hist_dict.items():
        hist_dict[key] = {k: v for k, v in value_dict.items()}

    return json.dumps(hist_dict)
```


```python
print(get_stock_market_data("NASDAQ Composite"))
```

### Function #3: Calculator 


```python
import math


def calculator(num1, num2, operator):
    if operator == "+":
        return str(num1 + num2)
    elif operator == "-":
        return str(num1 - num2)
    elif operator == "*":
        return str(num1 * num2)
    elif operator == "/":
        return str(num1 / num2)
    elif operator == "**":
        return str(num1**num2)
    elif operator == "sqrt":
        return str(math.sqrt(num1))
    else:
        return "Invalid operator"
```


```python
print(calculator(5, 5, "+"))
```

## 3.0 Calling a function using GPT

Steps for Function Calling: 

1. Call the model with the user query and a set of functions defined in the functions parameter.
2. The model can choose to call a function; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may generate invalid JSON or hallucinate parameters).
3. Parse the string into JSON in your code, and call your function with the provided arguments if they exist.
4. Call the model again by appending the function response as a new message, and let the model summarize the results back to the user.

### 3.1 Describe the functions so that the model knows how to call them


```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_current_time",
            "description": "Get the current time in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The location name. The pytz is used to get the timezone for that location. Location names should be in a format like America/New_York, Asia/Bangkok, Europe/London",
                    }
                },
                "required": ["location"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "get_stock_market_data",
            "description": "Get the stock market data for a given index",
            "parameters": {
                "type": "object",
                "properties": {
                    "index": {
                        "type": "string",
                        "enum": [
                            "S&P 500",
                            "NASDAQ Composite",
                            "Dow Jones Industrial Average",
                            "Financial Times Stock Exchange 100 Index",
                        ],
                    },
                },
                "required": ["index"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "calculator",
            "description": "A simple calculator used to perform basic arithmetic operations",
            "parameters": {
                "type": "object",
                "properties": {
                    "num1": {"type": "number"},
                    "num2": {"type": "number"},
                    "operator": {
                        "type": "string",
                        "enum": ["+", "-", "*", "/", "**", "sqrt"],
                    },
                },
                "required": ["num1", "num2", "operator"],
            },
        },
    },
]

available_functions = {
    "get_current_time": get_current_time,
    "get_stock_market_data": get_stock_market_data,
    "calculator": calculator,
}
```

### 3.2 Define a helper function to validate the function call
It's possible that the models could generate incorrect function calls so it's important to validate the calls. Here we define a simple helper function to validate the function call although you could apply more complex validation for your use case.


```python
import inspect


# helper method used to check if the correct arguments are provided to a function
def check_args(function, args):
    sig = inspect.signature(function)
    params = sig.parameters

    # Check if there are extra arguments
    for name in args:
        if name not in params:
            return False
    # Check if the required arguments are provided
    for name, param in params.items():
        if param.default is param.empty and name not in args:
            return False

    return True
```


```python
def run_conversation(messages, tools, available_functions):
    # Step 1: send the conversation and available functions to GPT
    response = client.chat.completions.create(
        model=model_name,
        messages=messages,
        tools=tools,
        tool_choice="auto",
    )

    response_message = response.choices[0].message

    # Step 2: check if GPT wanted to call a function
    if response_message.tool_calls:
        print("Recommended Function call:")
        print(response_message.tool_calls[0])
        print()

        # Step 3: call the function
        # Note: the JSON response may not always be valid; be sure to handle errors

        function_name = response_message.tool_calls[0].function.name

        # verify function exists
        if function_name not in available_functions:
            return "Function " + function_name + " does not exist"
        function_to_call = available_functions[function_name]

        # verify function has correct number of arguments
        function_args = json.loads(response_message.tool_calls[0].function.arguments)
        if check_args(function_to_call, function_args) is False:
            return "Invalid number of arguments for function: " + function_name
        function_response = function_to_call(**function_args)

        print("Output of function call:")
        print(function_response)
        print()

        # Step 4: send the info on the function call and function response to GPT

        # adding assistant response to messages
        messages.append(
            {
                "role": response_message.role,
                "function_call": {
                    "name": response_message.tool_calls[0].function.name,
                    "arguments": response_message.tool_calls[0].function.arguments,
                },
                "content": None,
            }
        )

        # adding function response to messages
        messages.append(
            {
                "role": "function",
                "name": function_name,
                "content": function_response,
            }
        )  # extend conversation with function response

        print("Messages in second request:")
        for message in messages:
            print(message)
        print()

        second_response = client.chat.completions.create(
            messages=messages,
            model=model_name,
        )  # get a new response from GPT where it can see the function response

        return second_response
```


```python
messages = [{"role": "user", "content": "What time is it in New York?"}]
assistant_response = run_conversation(messages, tools, available_functions)
print(assistant_response.choices[0].message)
```

### 4.0 Calling multiple functions together
In some cases, you may want to string together multiple function calls to get the desired result. We modified the `run_conversation()` function above to allow multiple function calls to be made.


```python
def run_multiturn_conversation(messages, tools, available_functions):
    # Step 1: send the conversation and available functions to GPT
    response = client.chat.completions.create(
        messages=messages,
        tools=tools,
        tool_choice="auto",
        model=model_name,
        temperature=0,
    )

    # Step 2: check if GPT wanted to call a function
    while response.choices[0].finish_reason == "tool_calls":
        response_message = response.choices[0].message
        print("Recommended Function call:")
        print(response_message.tool_calls[0])
        print()

        # Step 3: call the function
        # Note: the JSON response may not always be valid; be sure to handle errors

        function_name = response_message.tool_calls[0].function.name

        # verify function exists
        if function_name not in available_functions:
            return "Function " + function_name + " does not exist"
        function_to_call = available_functions[function_name]

        # verify function has correct number of arguments
        function_args = json.loads(response_message.tool_calls[0].function.arguments)
        if check_args(function_to_call, function_args) is False:
            return "Invalid number of arguments for function: " + function_name
        function_response = function_to_call(**function_args)

        print("Output of function call:")
        print(function_response)
        print()

        # Step 4: send the info on the function call and function response to GPT

        # adding assistant response to messages
        messages.append(
            {
                "role": response_message.role,
                "function_call": {
                    "name": response_message.tool_calls[0].function.name,
                    "arguments": response_message.tool_calls[0].function.arguments,
                },
                "content": None,
            }
        )

        # adding function response to messages
        messages.append(
            {
                "role": "function",
                "name": function_name,
                "content": function_response,
            }
        )  # extend conversation with function response

        print("Messages in next request:")
        for message in messages:
            print(message)
        print()

        response = client.chat.completions.create(
            messages=messages,
            tools=tools,
            tool_choice="auto",
            model=model_name,
            temperature=0,
        )  # get a new response from GPT where it can see the function response

    return response
```


```python
# Can add system prompting to guide the model to call functions and perform in specific ways
next_messages = [
    {
        "role": "system",
        "content": "Assistant is a helpful assistant that helps users get answers to questions. Assistant has access to several tools and sometimes you may need to call multiple tools in sequence to get answers for your users.",
    }
]
next_messages.append(
    {
        "role": "user",
        "content": "How much did S&P 500 change between July 12 and July 13? Use the calculator.",
    }
)

assistant_response = run_multiturn_conversation(
    next_messages, tools, available_functions
)
print("Final Response:")
print(assistant_response.choices[0].message)
print("Conversation complete!")
```

    Conversation complete!
    




################################################## working_with_langchain.md ##################################################


# How to use LangChain and Azure OpenAI with Python


Langchain is an open source framework for developing applications using large language models (LLM). <br>

This guide will demonstrate how to setup and use Azure OpenAI models' API with LangChain.
    

## Set Up
The following libraries must be installed to use LangChain with Azure OpenAI.<br>


```python
#%pip install --upgrade openai
#%pip install langchain
```

## API Configuation and Deployed Model Setup

After installing the necessary libraies, the API must be configured. The code below shows how to configure the API directly in your Python environment. 



```python
import openai
import json
import os
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage
from langchain import LLMChain


# Load config values
with open(r'config.json') as config_file:
    config_details = json.load(config_file)

# The base URL for your Azure OpenAI resource. e.g. "https://<your resource name>.openai.azure.com"
openai_api_base=config_details['OPENAI_API_BASE']
    
# API version e.g. "2023-07-01-preview"
openai_api_version=config_details['OPENAI_API_VERSION']

# The name of your Azure OpenAI deployment chat model. e.g. "gpt-35-turbo-0613"
deployment_name=config_details['DEPLOYMENT_NAME']

# The API key for your Azure OpenAI resource.
openai_api_key = os.getenv("OPENAI_API_KEY")

# This is set to `azure`
openai_api_type="azure"
```

## Deployed Model Setup


```python
# Create an instance of chat llm
llm = AzureChatOpenAI(
    openai_api_base=openai_api_base,
    openai_api_version=openai_api_version,
    deployment_name=deployment_name,
    openai_api_key=openai_api_key,
    openai_api_type=openai_api_type,
)

llm([HumanMessage(content="Write me a poem")])
```




    AIMessage(content="In the realm of dreams, where thoughts take flight,\nA tapestry of words, I shall now write.\nWith ink and quill, I'll weave a tale,\nOf love and hope, where hearts prevail.\n\nIn meadows adorned with flowers so fair,\nA gentle breeze whispers secrets in the air.\nThe sun shines bright, painting the sky,\nA canvas of colors, where dreams never die.\n\nBeneath a canopy of stars, we shall dance,\nLost in a moment, in a lover's trance.\nOur hearts entwined, beats synchronized,\nA symphony of love, never compromised.\n\nThrough valleys of sorrow, we shall tread,\nWith courage and strength, our fears we'll shed.\nFor love, a beacon, shall guide our way,\nThrough darkest nights, to a brighter day.\n\nIn the depths of silence, a whispered prayer,\nFor peace and harmony, beyond compare.\nMay kindness bloom, like flowers in spring,\nAnd compassion's song, forever sing.\n\nOh, let this poem be a gentle reminder,\nThat within us all, love is a powerful binder.\nFor in these words, a message so true,\nThat love's embrace can heal and renew.\n\nSo let us cherish, this gift we possess,\nThe power of words, to heal and impress.\nThrough poetry's grace, may hearts be moved,\nAnd in its beauty, we shall be proved.", additional_kwargs={}, example=False)



## PromptTemplates

Langchain provides a built in PromptsTemplate module to simplify the construction of prompts to get more specific answers.


```python
from langchain import PromptTemplate

from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)


# First Example
template = """
You are a skin care consulant that recommends products based on customer
needs and preferences.

What is a good {product_type} to help with {customer_request}?
"""

prompt = PromptTemplate(
input_variables=["product_type", "customer_request"],
template=template,
)

print("Example #1:")
print(llm([HumanMessage(content=prompt.format(
        product_type="face wash",
        customer_request = "acne prone skin"
    ))]
))
print("\n")

# Second Example
system_message = "You are an AI assistant travel assistant that provides vacation recommendations."

system_message_prompt = SystemMessagePromptTemplate.from_template(system_message)
human_template="{text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])
chain = LLMChain(llm=llm, prompt=chat_prompt)
result = chain.run(f"Where should I go on vaction in Decemember for warm weather and beaches?")
print("Example #2:")
print(result)
```

    Example #1:
    content='A highly recommended face wash for acne-prone skin is the "Neutrogena Oil-Free Acne Wash." This product contains salicylic acid, which helps to treat and prevent acne by unclogging pores and reducing inflammation. It is oil-free, non-comedogenic, and gentle enough for daily use. Additionally, it effectively removes dirt, excess oil, and makeup without over-drying the skin.' additional_kwargs={} example=False
    
    
    Example #2:
    If you're looking for warm weather and beautiful beaches in December, here are a few destinations you might consider:
    
    1. Maldives: This tropical paradise offers pristine beaches, crystal-clear waters, and luxurious resorts. December is a great time to visit, with temperatures averaging around 28°C (82°F).
    
    2. Thailand: Thailand's southern islands, such as Phuket, Krabi, and Koh Samui, offer warm weather and stunning beaches in December. You can relax on the white sands, go snorkeling or diving, and explore the vibrant local culture.
    
    3. Bali, Indonesia: Bali is a popular destination known for its stunning beaches, lush landscapes, and vibrant culture. In December, you can enjoy warm temperatures and take part in water sports or simply unwind by the beach.
    
    4. Cancun, Mexico: Cancun is a favorite destination for beach lovers, with its turquoise waters and soft white sands. December is a great time to visit, with temperatures around 27°C (81°F), and you can also explore the nearby Mayan ruins.
    
    5. Seychelles: This archipelago in the Indian Ocean boasts some of the world's most beautiful beaches. December is an excellent time to visit, as the weather is warm and you can enjoy activities like snorkeling, diving, and island hopping.
    
    Remember to check travel restrictions and safety guidelines before planning your trip, as they may vary due to the ongoing COVID-19 pandemic.
    

## Chains
There are many applications of chains that allow you to combine numerous LLM calls and actions.  <br>

### Simple Sequential Chains  <br>
Allow you to feed the output of one LLM Chain as input for another.


```python
from langchain.chains import SimpleSequentialChain
```


```python
description_template = """Your job is to come up with a fun DIY project for the specified gender, age, and description of a kid.
% CHILD_DESCRIPTION
{child_description}

YOUR RESPONSE:
"""
description_prompt_template = PromptTemplate(input_variables=["child_description"], template=description_template)

description_chain = LLMChain(llm=llm, prompt=description_prompt_template)
```


```python
diy_description_template = """Given a DIY project, give a short and simple recipe step-by-step guide on how to complete the project and a materials list.
% DIY_PROJECT
{diy_project}

YOUR RESPONSE:
"""
diy_prompt_template = PromptTemplate(input_variables=["diy_project"], template=diy_description_template)

diy_chain = LLMChain(llm=llm, prompt=diy_prompt_template)
```


```python
overall_chain = SimpleSequentialChain(chains=[description_chain, diy_chain], verbose=True)
```


```python
review = overall_chain.run("5-year-old girl")
```

    
    
    [1m> Entering new SimpleSequentialChain chain...[0m
    [36;1m[1;3mDIY Sparkling Fairy Wand:
    
    Materials needed:
    - Wooden dowel or stick
    - Glitter foam sheets in various colors
    - Ribbon or tulle
    - Craft glue
    - Scissors
    - Decorative gems or sequins
    - Glitter glue (optional)
    
    Instructions:
    1. Begin by cutting out a star shape from one of the glitter foam sheets. This will be the top of the wand.
    2. Cut out a long strip of foam from another color and wrap it around the wooden dowel or stick, starting from the bottom. Secure it with craft glue.
    3. Cut out smaller shapes like hearts, butterflies, or flowers from different colored glitter foam sheets.
    4. Use craft glue to stick these shapes onto the wrapped foam strip, creating a beautiful pattern. Let them dry completely.
    5. Once the foam shapes are secure, add some extra sparkle by applying glitter glue to the edges or adding decorative gems or sequins.
    6. Finally, tie ribbons or tulle strands to the bottom of the wooden dowel or stick for an extra touch of magic.
    7. Let the wand dry completely before giving it to the 5-year-old girl to play with.
    
    This DIY project will allow the 5-year-old girl to express her creativity and imagination as she creates her very own sparkling fairy wand. She can use it for pretend play, dress-up parties, or even as a room decoration.[0m
    [33;1m[1;3mDIY Sparkling Fairy Wand:
    
    Materials needed:
    - Wooden dowel or stick
    - Glitter foam sheets in various colors
    - Ribbon or tulle
    - Craft glue
    - Scissors
    - Decorative gems or sequins
    - Glitter glue (optional)
    
    Instructions:
    1. Cut out a star shape from a glitter foam sheet for the top of the wand.
    2. Wrap a long strip of foam from another color around the wooden dowel or stick, securing it with craft glue.
    3. Cut out smaller shapes like hearts, butterflies, or flowers from different colored glitter foam sheets.
    4. Use craft glue to stick these shapes onto the wrapped foam strip to create a pattern. Let them dry completely.
    5. Add extra sparkle by applying glitter glue to the edges or adding decorative gems or sequins.
    6. Tie ribbons or tulle strands to the bottom of the wooden dowel or stick for an extra touch of magic.
    7. Let the wand dry completely before giving it to the 5-year-old girl to play with.
    
    This DIY project allows the 5-year-old girl to express her creativity and imagination. She can use the sparkling fairy wand for pretend play, dress-up parties, or even as a room decoration.[0m
    
    [1m> Finished chain.[0m
    




################################################## writer.md ##################################################


---
sidebar_label: Writer
---
# ChatWriter

This notebook provides a quick overview for getting started with Writer [chat models](/docs/concepts/chat_models).

Writer has several chat models. You can find information about their latest models and their costs, context windows, and supported input types in the [Writer docs](https://dev.writer.com/home/models).

:::

## Overview

### Integration details
| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/chat/openai) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| ChatWriter | langchain-community | ❌ | ❌ | ❌ | ❌ | ❌ |

### Model features
| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | Image input | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | Native async | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |
| ✅ | ❌ | ✅ | ❌ | ❌ | ❌ | ✅ | ✅ | ✅ | ✅ | 

## Setup

To access Writer models you'll need to create a Writer account, get an API key, and install the `writer-sdk` and `langchain-community` packages.

### Credentials

Head to [Writer AI Studio](https://app.writer.com/aistudio/signup?utm_campaign=devrel) to sign up to OpenAI and generate an API key. Once you've done this set the WRITER_API_KEY environment variable:


```python
import getpass
import os

if not os.environ.get("WRITER_API_KEY"):
    os.environ["WRITER_API_KEY"] = getpass.getpass("Enter your Writer API key: ")
```

### Installation

The LangChain Writer integration lives in the `langchain-community` package:


```python
%pip install -qU langchain-community writer-sdk
```

    Note: you may need to restart the kernel to use updated packages.
    

## Instantiation

Now we can instantiate our model object and generate chat completions:


```python
from langchain_community.chat_models.writer import ChatWriter

llm = ChatWriter(
    model="palmyra-x-004",
    temperature=0.7,
    max_tokens=1000,
    # api_key="...",  # if you prefer to pass api key in directly instaed of using env vars
    # base_url="...",
    # other params...
)
```


    ---------------------------------------------------------------------------

    ImportError                               Traceback (most recent call last)

    Cell In[3], line 1
    ----> 1 from langchain_community.chat_models import ChatWriter
          3 llm = ChatWriter(
          4     model="palmyra-x-004",
          5     temperature=0.7,
       (...)
          9     # other params...
         10 )
    

    ImportError: cannot import name 'ChatWriter' from 'langchain_community.chat_models' (/home/yanomaly/PycharmProjects/whitesnake/writer/langсhain/libs/community/langchain_community/chat_models/__init__.py)


## Invocation


```python
messages = [
    (
        "system",
        "You are a helpful assistant that writes poems about the Python programming language.",
    ),
    ("human", "Write a poem about Python."),
]
ai_msg = llm.invoke(messages)
ai_msg
```


```python
print(ai_msg.content)
```

## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:


```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant that writes poems about the {input_language} programming language.",
        ),
        ("human", "{input}"),
    ]
)

chain = prompt | llm
chain.invoke(
    {
        "input_language": "Java",
        "input": "Write a poem about Java.",
    }
)
```

## Tool calling

Writer supports [tool calling](https://dev.writer.com/api-guides/tool-calling), which lets you describe tools and their arguments, and have the model return a JSON object with a tool to invoke and the inputs to that tool.

### ChatWriter.bind_tools()

With `ChatWriter.bind_tools`, we can easily pass in Pydantic classes, dict schemas, LangChain tools, or even functions as tools to the model. Under the hood these are converted to tool schemas, which looks like:
```
{
    "name": "...",
    "description": "...",
    "parameters": {...}  # JSONSchema
}
```
and passed in every model invocation.


```python
from pydantic import BaseModel, Field


class GetWeather(BaseModel):
    """Get the current weather in a given location"""

    location: str = Field(..., description="The city and state, e.g. San Francisco, CA")


llm_with_tools = llm.bind_tools([GetWeather])
```


```python
ai_msg = llm_with_tools.invoke(
    "what is the weather like in New York City",
)
ai_msg
```

### AIMessage.tool_calls
Notice that the AIMessage has a `tool_calls` attribute. This contains in a standardized ToolCall format that is model-provider agnostic.


```python
ai_msg.tool_calls
```

For more on binding tools and tool call outputs, head to the [tool calling](/docs/how_to/function_calling) docs.

## API reference

For detailed documentation of all Writer features, head to our [API reference](https://dev.writer.com/api-guides/api-reference/completion-api/chat-completion).




################################################## xai.md ##################################################


---
sidebar_label: xAI
---
# ChatXAI


This page will help you get started with xAI [chat models](../../concepts/chat_models.mdx). For detailed documentation of all `ChatXAI` features and configurations head to the [API reference](https://python.langchain.com/api_reference/xai/chat_models/langchain_xai.chat_models.ChatXAI.html).

[xAI](https://console.x.ai/) offers an API to interact with Grok models.

## Overview
### Integration details

| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/chat/xai) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatXAI](https://python.langchain.com/api_reference/xai/chat_models/langchain_xai.chat_models.ChatXAI.html) | [langchain-xai](https://python.langchain.com/api_reference/xai/index.html) | ❌ | beta | ✅ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-xai?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-xai?style=flat-square&label=%20) |

### Model features
| [Tool calling](../../how_to/tool_calling.ipynb) | [Structured output](../../how_to/structured_output.ipynb) | JSON mode | [Image input](../../how_to/multimodal_inputs.ipynb) | Audio input | Video input | [Token-level streaming](../../how_to/chat_streaming.ipynb) | Native async | [Token usage](../../how_to/chat_token_usage_tracking.ipynb) | [Logprobs](../../how_to/logprobs.ipynb) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | ❌ | ✅ | ✅ | 

## Setup

To access xAI models you'll need to create an xAI account, get an API key, and install the `langchain-xai` integration package.

### Credentials

Head to [this page](https://console.x.ai/) to sign up for xAI and generate an API key. Once you've done this set the `XAI_API_KEY` environment variable:


```python
import getpass
import os

if "XAI_API_KEY" not in os.environ:
    os.environ["XAI_API_KEY"] = getpass.getpass("Enter your xAI API key: ")
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:


```python
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
# os.environ["LANGSMITH_TRACING"] = "true"
```

### Installation

The LangChain xAI integration lives in the `langchain-xai` package:


```python
%pip install -qU langchain-xai
```

## Instantiation

Now we can instantiate our model object and generate chat completions:


```python
from langchain_xai import ChatXAI

llm = ChatXAI(
    model="grok-beta",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    # other params...
)
```

## Invocation


```python
messages = [
    (
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ),
    ("human", "I love programming."),
]
ai_msg = llm.invoke(messages)
ai_msg
```




    AIMessage(content="J'adore programmer.", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 30, 'total_tokens': 36, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'grok-beta', 'system_fingerprint': 'fp_14b89b2dfc', 'finish_reason': 'stop', 'logprobs': None}, id='run-adffb7a3-e48a-4f52-b694-340d85abe5c3-0', usage_metadata={'input_tokens': 30, 'output_tokens': 6, 'total_tokens': 36, 'input_token_details': {}, 'output_token_details': {}})




```python
print(ai_msg.content)
```

    J'adore programmer.
    

## Chaining

We can [chain](../../how_to/sequence.ipynb) our model with a prompt template like so:


```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ),
        ("human", "{input}"),
    ]
)

chain = prompt | llm
chain.invoke(
    {
        "input_language": "English",
        "output_language": "German",
        "input": "I love programming.",
    }
)
```




    AIMessage(content='Ich liebe das Programmieren.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 25, 'total_tokens': 32, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'grok-beta', 'system_fingerprint': 'fp_14b89b2dfc', 'finish_reason': 'stop', 'logprobs': None}, id='run-569fc8dc-101b-4e6d-864e-d4fa80df2b63-0', usage_metadata={'input_tokens': 25, 'output_tokens': 7, 'total_tokens': 32, 'input_token_details': {}, 'output_token_details': {}})



## Tool calling

ChatXAI has a [tool calling](https://docs.x.ai/docs#capabilities) (we use "tool calling" and "function calling" interchangeably here) API that lets you describe tools and their arguments, and have the model return a JSON object with a tool to invoke and the inputs to that tool. Tool-calling is extremely useful for building tool-using chains and agents, and for getting structured outputs from models more generally.

### ChatXAI.bind_tools()

With `ChatXAI.bind_tools`, we can easily pass in Pydantic classes, dict schemas, LangChain tools, or even functions as tools to the model. Under the hood these are converted to an OpenAI tool schemas, which looks like:
```
{
    "name": "...",
    "description": "...",
    "parameters": {...}  # JSONSchema
}
```
and passed in every model invocation.


```python
from pydantic import BaseModel, Field


class GetWeather(BaseModel):
    """Get the current weather in a given location"""

    location: str = Field(..., description="The city and state, e.g. San Francisco, CA")


llm_with_tools = llm.bind_tools([GetWeather])
```


```python
ai_msg = llm_with_tools.invoke(
    "what is the weather like in San Francisco",
)
ai_msg
```




    AIMessage(content='I am retrieving the current weather for San Francisco.', additional_kwargs={'tool_calls': [{'id': '0', 'function': {'arguments': '{"location":"San Francisco, CA"}', 'name': 'GetWeather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 151, 'total_tokens': 162, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'grok-beta', 'system_fingerprint': 'fp_14b89b2dfc', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-73707da7-afec-4a52-bee1-a176b0ab8585-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': '0', 'type': 'tool_call'}], usage_metadata={'input_tokens': 151, 'output_tokens': 11, 'total_tokens': 162, 'input_token_details': {}, 'output_token_details': {}})



## API reference

For detailed documentation of all `ChatXAI` features and configurations head to the API reference: https://python.langchain.com/api_reference/xai/chat_models/langchain_xai.chat_models.ChatXAI.html




################################################## xata.md ##################################################


# Xata

> [Xata](https://xata.io) is a serverless data platform, based on PostgreSQL. It provides a Python SDK for interacting with your database, and a UI for managing your data.
> Xata has a native vector type, which can be added to any table, and supports similarity search. LangChain inserts vectors directly to Xata, and queries it for the nearest neighbors of a given vector, so that you can use all the LangChain Embeddings integrations with Xata.

This notebook guides you how to use Xata as a VectorStore.

## Setup

### Create a database to use as a vector store

In the [Xata UI](https://app.xata.io) create a new database. You can name it whatever you want, in this notepad we'll use `langchain`.
Create a table, again you can name it anything, but we will use `vectors`. Add the following columns via the UI:

* `content` of type "Text". This is used to store the `Document.pageContent` values.
* `embedding` of type "Vector". Use the dimension used by the model you plan to use. In this notebook we use OpenAI embeddings, which have 1536 dimensions.
* `source` of type "Text". This is used as a metadata column by this example.
* any other columns you want to use as metadata. They are populated from the `Document.metadata` object. For example, if in the `Document.metadata` object you have a `title` property, you can create a `title` column in the table and it will be populated.


Let's first install our dependencies:


```python
%pip install --upgrade --quiet  xata langchain-openai langchain-community tiktoken langchain
```

Let's load the OpenAI key to the environemnt. If you don't have one you can create an OpenAI account and create a key on this [page](https://platform.openai.com/account/api-keys).


```python
import getpass
import os

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")
```

Similarly, we need to get the environment variables for Xata. You can create a new API key by visiting your [account settings](https://app.xata.io/settings). To find the database URL, go to the Settings page of the database that you have created. The database URL should look something like this: `https://demo-uni3q8.eu-west-1.xata.sh/db/langchain`.


```python
api_key = getpass.getpass("Xata API key: ")
db_url = input("Xata database URL (copy it from your DB settings):")
```


```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores.xata import XataVectorStore
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter
```

### Create the Xata vector store
Let's import our test dataset:


```python
loader = TextLoader("../../how_to/state_of_the_union.txt")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

embeddings = OpenAIEmbeddings()
```

Now create the actual vector store, backed by the Xata table.


```python
vector_store = XataVectorStore.from_documents(
    docs, embeddings, api_key=api_key, db_url=db_url, table_name="vectors"
)
```

After running the above command, if you go to the Xata UI, you should see the documents loaded together with their embeddings.
To use an existing Xata table that already contains vector contents, initialize the XataVectorStore constructor:


```python
vector_store = XataVectorStore(
    api_key=api_key, db_url=db_url, embedding=embeddings, table_name="vectors"
)
```

### Similarity Search


```python
query = "What did the president say about Ketanji Brown Jackson"
found_docs = vector_store.similarity_search(query)
print(found_docs)
```

### Similarity Search with score (vector distance)


```python
query = "What did the president say about Ketanji Brown Jackson"
result = vector_store.similarity_search_with_score(query)
for doc, score in result:
    print(f"document={doc}, score={score}")
```




################################################## xata_chat_message_history.md ##################################################


# Xata

>[Xata](https://xata.io) is a serverless data platform, based on `PostgreSQL` and `Elasticsearch`. It provides a Python SDK for interacting with your database, and a UI for managing your data. With the `XataChatMessageHistory` class, you can use Xata databases for longer-term persistence of chat sessions.

This notebook covers:

* A simple example showing what `XataChatMessageHistory` does.
* A more complex example using a REACT agent that answer questions based on a knowledge based or documentation (stored in Xata as a vector store) and also having a long-term searchable history of its past messages (stored in Xata as a memory store)

## Setup

### Create a database

In the [Xata UI](https://app.xata.io) create a new database. You can name it whatever you want, in this notepad we'll use `langchain`. The Langchain integration can auto-create the table used for storying the memory, and this is what we'll use in this example. If you want to pre-create the table, ensure it has the right schema and set `create_table` to `False` when creating the class. Pre-creating the table saves one round-trip to the database during each session initialization.

Let's first install our dependencies:


```python
%pip install --upgrade --quiet  xata langchain-openai langchain langchain-community
```

Next, we need to get the environment variables for Xata. You can create a new API key by visiting your [account settings](https://app.xata.io/settings). To find the database URL, go to the Settings page of the database that you have created. The database URL should look something like this: `https://demo-uni3q8.eu-west-1.xata.sh/db/langchain`.


```python
import getpass

api_key = getpass.getpass("Xata API key: ")
db_url = input("Xata database URL (copy it from your DB settings):")
```

## Create a simple memory store

To test the memory store functionality in isolation, let's use the following code snippet:


```python
from langchain_community.chat_message_histories import XataChatMessageHistory

history = XataChatMessageHistory(
    session_id="session-1", api_key=api_key, db_url=db_url, table_name="memory"
)

history.add_user_message("hi!")

history.add_ai_message("whats up?")
```

The above code creates a session with the ID `session-1` and stores two messages in it. After running the above, if you visit the Xata UI, you should see a table named `memory` and the two messages added to it.

You can retrieve the message history for a particular session with the following code:


```python
history.messages
```

## Conversational Q&A chain on your data with memory

Let's now see a more complex example in which we combine OpenAI, the Xata Vector Store integration, and the Xata memory store integration to create a Q&A chat bot on your data, with follow-up questions and history.

We're going to need to access the OpenAI API, so let's configure the API key:


```python
import os

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")
```

To store the documents that the chatbot will search for answers, add a table named `docs` to your `langchain` database using the Xata UI, and add the following columns:

* `content` of type "Text". This is used to store the `Document.pageContent` values.
* `embedding` of type "Vector". Use the dimension used by the model you plan to use. In this notebook we use OpenAI embeddings, which have 1536 dimensions.

Let's create the vector store and add some sample docs to it:


```python
from langchain_community.vectorstores.xata import XataVectorStore
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

texts = [
    "Xata is a Serverless Data platform based on PostgreSQL",
    "Xata offers a built-in vector type that can be used to store and query vectors",
    "Xata includes similarity search",
]

vector_store = XataVectorStore.from_texts(
    texts, embeddings, api_key=api_key, db_url=db_url, table_name="docs"
)
```

After running the above command, if you go to the Xata UI, you should see the documents loaded together with their embeddings in the `docs` table.

Let's now create a ConversationBufferMemory to store the chat messages from both the user and the AI.


```python
from uuid import uuid4

from langchain.memory import ConversationBufferMemory

chat_memory = XataChatMessageHistory(
    session_id=str(uuid4()),  # needs to be unique per user session
    api_key=api_key,
    db_url=db_url,
    table_name="memory",
)
memory = ConversationBufferMemory(
    memory_key="chat_history", chat_memory=chat_memory, return_messages=True
)
```

Now it's time to create an Agent to use both the vector store and the chat memory together.


```python
from langchain.agents import AgentType, initialize_agent
from langchain.agents.agent_toolkits import create_retriever_tool
from langchain_openai import ChatOpenAI

tool = create_retriever_tool(
    vector_store.as_retriever(),
    "search_docs",
    "Searches and returns documents from the Xata manual. Useful when you need to answer questions about Xata.",
)
tools = [tool]

llm = ChatOpenAI(temperature=0)

agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,
    verbose=True,
    memory=memory,
)
```

To test, let's tell the agent our name:


```python
agent.run(input="My name is bob")
```

Now, let's now ask the agent some questions about Xata:


```python
agent.run(input="What is xata?")
```

Notice that it answers based on the data stored in the document store. And now, let's ask a follow up question:


```python
agent.run(input="Does it support similarity search?")
```

And now let's test its memory:


```python
agent.run(input="Did I tell you my name? What is it?")
```




################################################## xinference.md ##################################################


# Xorbits Inference (Xinference)

[Xinference](https://github.com/xorbitsai/inference) is a powerful and versatile library designed to serve LLMs, 
speech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as chatglm, baichuan, whisper, vicuna, orca, and many others. This notebook demonstrates how to use Xinference with LangChain.

## Installation

Install `Xinference` through PyPI:


```python
%pip install --upgrade --quiet  "xinference[all]"
```

## Deploy Xinference Locally or in a Distributed Cluster.

For local deployment, run `xinference`. 

To deploy Xinference in a cluster, first start an Xinference supervisor using the `xinference-supervisor`. You can also use the option -p to specify the port and -H to specify the host. The default port is 9997.

Then, start the Xinference workers using `xinference-worker` on each server you want to run them on. 

You can consult the README file from [Xinference](https://github.com/xorbitsai/inference) for more information.
## Wrapper

To use Xinference with LangChain, you need to first launch a model. You can use command line interface (CLI) to do so:


```python
!xinference launch -n vicuna-v1.3 -f ggmlv3 -q q4_0
```

    Model uid: 7167b2b0-2a04-11ee-83f0-d29396a3f064
    

A model UID is returned for you to use. Now you can use Xinference with LangChain:


```python
from langchain_community.llms import Xinference

llm = Xinference(
    server_url="http://0.0.0.0:9997", model_uid="7167b2b0-2a04-11ee-83f0-d29396a3f064"
)

llm(
    prompt="Q: where can we visit in the capital of France? A:",
    generate_config={"max_tokens": 1024, "stream": True},
)
```




    ' You can visit the Eiffel Tower, Notre-Dame Cathedral, the Louvre Museum, and many other historical sites in Paris, the capital of France.'



### Integrate with a LLMChain


```python
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate

template = "Where can we visit in the capital of {country}?"

prompt = PromptTemplate.from_template(template)

llm_chain = LLMChain(prompt=prompt, llm=llm)

generated = llm_chain.run(country="France")
print(generated)
```

    
    A: You can visit many places in Paris, such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, the Champs-Elysées, Montmartre, Sacré-Cœur, and the Palace of Versailles.
    

Lastly, terminate the model when you do not need to use it:


```python
!xinference terminate --model-uid "7167b2b0-2a04-11ee-83f0-d29396a3f064"
```




################################################## xml.md ##################################################


# UnstructuredXMLLoader

This notebook provides a quick overview for getting started with UnstructuredXMLLoader [document loader](https://python.langchain.com/docs/concepts/document_loaders). The `UnstructuredXMLLoader` is used to load `XML` files. The loader works with `.xml` files. The page content will be the text extracted from the XML tags.


## Overview
### Integration details


| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/document_loaders/file_loaders/unstructured/)|
| :--- | :--- | :---: | :---: |  :---: |
| [UnstructuredXMLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.xml.UnstructuredXMLLoader.html) | [langchain_community](https://python.langchain.com/api_reference/community/index.html) | ✅ | ❌ | ✅ | 
### Loader features
| Source | Document Lazy Loading | Native Async Support
| :---: | :---: | :---: | 
| UnstructuredXMLLoader | ✅ | ❌ | 

## Setup

To access UnstructuredXMLLoader document loader you'll need to install the `langchain-community` integration package.

### Credentials

No credentials are needed to use the UnstructuredXMLLoader

If you want to get automated best in-class tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:


```python
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
# os.environ["LANGSMITH_TRACING"] = "true"
```

### Installation

Install **langchain_community**.


```python
%pip install -qU langchain_community
```

## Initialization

Now we can instantiate our model object and load documents:


```python
from langchain_community.document_loaders import UnstructuredXMLLoader

loader = UnstructuredXMLLoader(
    "./example_data/factbook.xml",
)
```

## Load


```python
docs = loader.load()
docs[0]
```




    Document(metadata={'source': './example_data/factbook.xml'}, page_content='United States\n\nWashington, DC\n\nJoe Biden\n\nBaseball\n\nCanada\n\nOttawa\n\nJustin Trudeau\n\nHockey\n\nFrance\n\nParis\n\nEmmanuel Macron\n\nSoccer\n\nTrinidad & Tobado\n\nPort of Spain\n\nKeith Rowley\n\nTrack & Field')




```python
print(docs[0].metadata)
```

    {'source': './example_data/factbook.xml'}
    

## Lazy Load


```python
page = []
for doc in loader.lazy_load():
    page.append(doc)
    if len(page) >= 10:
        # do some paged operation, e.g.
        # index.upsert(page)

        page = []
```

## API reference

For detailed documentation of all __ModuleName__Loader features and configurations head to the API reference: https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.xml.UnstructuredXMLLoader.html




################################################## xorbits.md ##################################################


# Xorbits Pandas DataFrame

This notebook goes over how to load data from a [xorbits.pandas](https://doc.xorbits.io/en/latest/reference/pandas/frame.html) DataFrame.


```python
%pip install --upgrade --quiet  xorbits
```


```python
import xorbits.pandas as pd
```


```python
df = pd.read_csv("example_data/mlb_teams_2012.csv")
```


```python
df.head()
```


      0%|          |   0.00/100 [00:00<?, ?it/s]





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Team</th>
      <th>"Payroll (millions)"</th>
      <th>"Wins"</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Nationals</td>
      <td>81.34</td>
      <td>98</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Reds</td>
      <td>82.20</td>
      <td>97</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Yankees</td>
      <td>197.96</td>
      <td>95</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Giants</td>
      <td>117.62</td>
      <td>94</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Braves</td>
      <td>83.31</td>
      <td>94</td>
    </tr>
  </tbody>
</table>
</div>




```python
from langchain_community.document_loaders import XorbitsLoader
```


```python
loader = XorbitsLoader(df, page_content_column="Team")
```


```python
loader.load()
```


      0%|          |   0.00/100 [00:00<?, ?it/s]





    [Document(page_content='Nationals', metadata={' "Payroll (millions)"': 81.34, ' "Wins"': 98}),
     Document(page_content='Reds', metadata={' "Payroll (millions)"': 82.2, ' "Wins"': 97}),
     Document(page_content='Yankees', metadata={' "Payroll (millions)"': 197.96, ' "Wins"': 95}),
     Document(page_content='Giants', metadata={' "Payroll (millions)"': 117.62, ' "Wins"': 94}),
     Document(page_content='Braves', metadata={' "Payroll (millions)"': 83.31, ' "Wins"': 94}),
     Document(page_content='Athletics', metadata={' "Payroll (millions)"': 55.37, ' "Wins"': 94}),
     Document(page_content='Rangers', metadata={' "Payroll (millions)"': 120.51, ' "Wins"': 93}),
     Document(page_content='Orioles', metadata={' "Payroll (millions)"': 81.43, ' "Wins"': 93}),
     Document(page_content='Rays', metadata={' "Payroll (millions)"': 64.17, ' "Wins"': 90}),
     Document(page_content='Angels', metadata={' "Payroll (millions)"': 154.49, ' "Wins"': 89}),
     Document(page_content='Tigers', metadata={' "Payroll (millions)"': 132.3, ' "Wins"': 88}),
     Document(page_content='Cardinals', metadata={' "Payroll (millions)"': 110.3, ' "Wins"': 88}),
     Document(page_content='Dodgers', metadata={' "Payroll (millions)"': 95.14, ' "Wins"': 86}),
     Document(page_content='White Sox', metadata={' "Payroll (millions)"': 96.92, ' "Wins"': 85}),
     Document(page_content='Brewers', metadata={' "Payroll (millions)"': 97.65, ' "Wins"': 83}),
     Document(page_content='Phillies', metadata={' "Payroll (millions)"': 174.54, ' "Wins"': 81}),
     Document(page_content='Diamondbacks', metadata={' "Payroll (millions)"': 74.28, ' "Wins"': 81}),
     Document(page_content='Pirates', metadata={' "Payroll (millions)"': 63.43, ' "Wins"': 79}),
     Document(page_content='Padres', metadata={' "Payroll (millions)"': 55.24, ' "Wins"': 76}),
     Document(page_content='Mariners', metadata={' "Payroll (millions)"': 81.97, ' "Wins"': 75}),
     Document(page_content='Mets', metadata={' "Payroll (millions)"': 93.35, ' "Wins"': 74}),
     Document(page_content='Blue Jays', metadata={' "Payroll (millions)"': 75.48, ' "Wins"': 73}),
     Document(page_content='Royals', metadata={' "Payroll (millions)"': 60.91, ' "Wins"': 72}),
     Document(page_content='Marlins', metadata={' "Payroll (millions)"': 118.07, ' "Wins"': 69}),
     Document(page_content='Red Sox', metadata={' "Payroll (millions)"': 173.18, ' "Wins"': 69}),
     Document(page_content='Indians', metadata={' "Payroll (millions)"': 78.43, ' "Wins"': 68}),
     Document(page_content='Twins', metadata={' "Payroll (millions)"': 94.08, ' "Wins"': 66}),
     Document(page_content='Rockies', metadata={' "Payroll (millions)"': 78.06, ' "Wins"': 64}),
     Document(page_content='Cubs', metadata={' "Payroll (millions)"': 88.19, ' "Wins"': 61}),
     Document(page_content='Astros', metadata={' "Payroll (millions)"': 60.65, ' "Wins"': 55})]




```python
# Use lazy load for larger table, which won't read the full table into memory
for i in loader.lazy_load():
    print(i)
```


      0%|          |   0.00/100 [00:00<?, ?it/s]


    page_content='Nationals' metadata={' "Payroll (millions)"': 81.34, ' "Wins"': 98}
    page_content='Reds' metadata={' "Payroll (millions)"': 82.2, ' "Wins"': 97}
    page_content='Yankees' metadata={' "Payroll (millions)"': 197.96, ' "Wins"': 95}
    page_content='Giants' metadata={' "Payroll (millions)"': 117.62, ' "Wins"': 94}
    page_content='Braves' metadata={' "Payroll (millions)"': 83.31, ' "Wins"': 94}
    page_content='Athletics' metadata={' "Payroll (millions)"': 55.37, ' "Wins"': 94}
    page_content='Rangers' metadata={' "Payroll (millions)"': 120.51, ' "Wins"': 93}
    page_content='Orioles' metadata={' "Payroll (millions)"': 81.43, ' "Wins"': 93}
    page_content='Rays' metadata={' "Payroll (millions)"': 64.17, ' "Wins"': 90}
    page_content='Angels' metadata={' "Payroll (millions)"': 154.49, ' "Wins"': 89}
    page_content='Tigers' metadata={' "Payroll (millions)"': 132.3, ' "Wins"': 88}
    page_content='Cardinals' metadata={' "Payroll (millions)"': 110.3, ' "Wins"': 88}
    page_content='Dodgers' metadata={' "Payroll (millions)"': 95.14, ' "Wins"': 86}
    page_content='White Sox' metadata={' "Payroll (millions)"': 96.92, ' "Wins"': 85}
    page_content='Brewers' metadata={' "Payroll (millions)"': 97.65, ' "Wins"': 83}
    page_content='Phillies' metadata={' "Payroll (millions)"': 174.54, ' "Wins"': 81}
    page_content='Diamondbacks' metadata={' "Payroll (millions)"': 74.28, ' "Wins"': 81}
    page_content='Pirates' metadata={' "Payroll (millions)"': 63.43, ' "Wins"': 79}
    page_content='Padres' metadata={' "Payroll (millions)"': 55.24, ' "Wins"': 76}
    page_content='Mariners' metadata={' "Payroll (millions)"': 81.97, ' "Wins"': 75}
    page_content='Mets' metadata={' "Payroll (millions)"': 93.35, ' "Wins"': 74}
    page_content='Blue Jays' metadata={' "Payroll (millions)"': 75.48, ' "Wins"': 73}
    page_content='Royals' metadata={' "Payroll (millions)"': 60.91, ' "Wins"': 72}
    page_content='Marlins' metadata={' "Payroll (millions)"': 118.07, ' "Wins"': 69}
    page_content='Red Sox' metadata={' "Payroll (millions)"': 173.18, ' "Wins"': 69}
    page_content='Indians' metadata={' "Payroll (millions)"': 78.43, ' "Wins"': 68}
    page_content='Twins' metadata={' "Payroll (millions)"': 94.08, ' "Wins"': 66}
    page_content='Rockies' metadata={' "Payroll (millions)"': 78.06, ' "Wins"': 64}
    page_content='Cubs' metadata={' "Payroll (millions)"': 88.19, ' "Wins"': 61}
    page_content='Astros' metadata={' "Payroll (millions)"': 60.65, ' "Wins"': 55}
    




################################################## yahoo_finance_news.md ##################################################


# Yahoo Finance News

This notebook goes over how to use the `yahoo_finance_news` tool with an agent. 


## Setting up

First, you need to install `yfinance` python package.


```python
%pip install --upgrade --quiet  yfinance
```

## Example with Chain


```python
import os

os.environ["OPENAI_API_KEY"] = "..."
```


```python
from langchain.agents import AgentType, initialize_agent
from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(temperature=0.0)
tools = [YahooFinanceNewsTool()]
agent_chain = initialize_agent(
    tools,
    llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
)
```


```python
agent_chain.invoke(
    "What happened today with Microsoft stocks?",
)
```

    
    
    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3mI should check the latest financial news about Microsoft stocks.
    Action: yahoo_finance_news
    Action Input: MSFT[0m
    Observation: [36;1m[1;3mMicrosoft (MSFT) Gains But Lags Market: What You Should Know
    In the latest trading session, Microsoft (MSFT) closed at $328.79, marking a +0.12% move from the previous day.[0m
    Thought:[32;1m[1;3mI have the latest information on Microsoft stocks.
    Final Answer: Microsoft (MSFT) closed at $328.79, with a +0.12% move from the previous day.[0m
    
    [1m> Finished chain.[0m
    




    'Microsoft (MSFT) closed at $328.79, with a +0.12% move from the previous day.'




```python
agent_chain.invoke(
    "How does Microsoft feels today comparing with Nvidia?",
)
```

    
    
    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3mI should compare the current sentiment of Microsoft and Nvidia.
    Action: yahoo_finance_news
    Action Input: MSFT[0m
    Observation: [36;1m[1;3mMicrosoft (MSFT) Gains But Lags Market: What You Should Know
    In the latest trading session, Microsoft (MSFT) closed at $328.79, marking a +0.12% move from the previous day.[0m
    Thought:[32;1m[1;3mI need to find the current sentiment of Nvidia as well.
    Action: yahoo_finance_news
    Action Input: NVDA[0m
    Observation: [36;1m[1;3m[0m
    Thought:[32;1m[1;3mI now know the current sentiment of both Microsoft and Nvidia.
    Final Answer: I cannot compare the sentiment of Microsoft and Nvidia as I only have information about Microsoft.[0m
    
    [1m> Finished chain.[0m
    




    'I cannot compare the sentiment of Microsoft and Nvidia as I only have information about Microsoft.'



# How YahooFinanceNewsTool works?


```python
tool = YahooFinanceNewsTool()
```


```python
tool.invoke("NVDA")
```




    'No news found for company that searched with NVDA ticker.'




```python
res = tool.invoke("AAPL")
print(res)
```

    Top Research Reports for Apple, Broadcom & Caterpillar
    Today's Research Daily features new research reports on 16 major stocks, including Apple Inc. (AAPL), Broadcom Inc. (AVGO) and Caterpillar Inc. (CAT).
    
    Apple Stock on Pace for Worst Month of the Year
    Apple (AAPL) shares are on pace for their worst month of the year, according to Dow Jones Market Data.  The stock is down 4.8% so far in August, putting it on pace for its worst month since December 2022, when it fell 12%.
    


```python

```




################################################## yandex.md ##################################################


---
sidebar_label: YandexGPT
---
# ChatYandexGPT

This notebook goes over how to use Langchain with [YandexGPT](https://cloud.yandex.com/en/services/yandexgpt) chat model.

To use, you should have the `yandexcloud` python package installed.


```python
%pip install --upgrade --quiet  yandexcloud
```

First, you should [create service account](https://cloud.yandex.com/en/docs/iam/operations/sa/create) with the `ai.languageModels.user` role.

Next, you have two authentication options:
- [IAM token](https://cloud.yandex.com/en/docs/iam/operations/iam-token/create-for-sa).
    You can specify the token in a constructor parameter `iam_token` or in an environment variable `YC_IAM_TOKEN`.

- [API key](https://cloud.yandex.com/en/docs/iam/operations/api-key/create)
    You can specify the key in a constructor parameter `api_key` or in an environment variable `YC_API_KEY`.

To specify the model you can use `model_uri` parameter, see [the documentation](https://cloud.yandex.com/en/docs/yandexgpt/concepts/models#yandexgpt-generation) for more details.

By default, the latest version of `yandexgpt-lite` is used from the folder specified in the parameter `folder_id` or `YC_FOLDER_ID` environment variable.


```python
from langchain_community.chat_models import ChatYandexGPT
from langchain_core.messages import HumanMessage, SystemMessage
```


```python
chat_model = ChatYandexGPT()
```


```python
answer = chat_model.invoke(
    [
        SystemMessage(
            content="You are a helpful assistant that translates English to French."
        ),
        HumanMessage(content="I love programming."),
    ]
)
answer
```




    AIMessage(content='Je adore le programmement.')






################################################## yellowbrick.md ##################################################


# Yellowbrick

[Yellowbrick](https://yellowbrick.com/yellowbrick-data-warehouse/) is an elastic, massively parallel processing (MPP) SQL database that runs in the cloud and on-premises, using kubernetes for scale, resilience and cloud portability. Yellowbrick is designed to address the largest and most complex business-critical data warehousing use cases. The efficiency at scale that Yellowbrick provides also enables it to be used as a high performance and scalable vector database to store and search vectors with SQL. 


## Using Yellowbrick as the vector store for ChatGpt

This tutorial demonstrates how to create a simple chatbot backed by ChatGpt that uses Yellowbrick as a vector store to support Retrieval Augmented Generation (RAG). What you'll need:

1. An account on the [Yellowbrick sandbox](https://cloudlabs.yellowbrick.com/)
2. An api key from [OpenAI](https://platform.openai.com/)

The tutorial is divided into five parts. First we'll use langchain to create a baseline chatbot to interact with ChatGpt without a vector store. Second, we'll create an embeddings table in Yellowbrick that will represent the vector store. Third, we'll load a series of documents (the Administration chapter of the Yellowbrick Manual). Fourth, we'll create the vector representation of those documents and store in a Yellowbrick table.  Lastly, we'll send the same queries to the improved chatbox to see the results.



```python
# Install all needed libraries
%pip install --upgrade --quiet  langchain
%pip install --upgrade --quiet  langchain-openai langchain-community
%pip install --upgrade --quiet  psycopg2-binary
%pip install --upgrade --quiet  tiktoken
```

## Setup: Enter the information used to connect to Yellowbrick and OpenAI API

Our chatbot integrates with ChatGpt via the langchain library, so you'll need an API key from OpenAI first:

To get an api key for OpenAI:
1. Register at https://platform.openai.com/
2. Add a payment method - You're unlikely to go over free quota
3. Create an API key

You'll also need your Username, Password, and Database name from the welcome email when you sign up for the Yellowbrick Sandbox Account.


The following should be modified to include the information for your Yellowbrick database and OpenAPI Key


```python
# Modify these values to match your Yellowbrick Sandbox and OpenAI API Key
YBUSER = "[SANDBOX USER]"
YBPASSWORD = "[SANDBOX PASSWORD]"
YBDATABASE = "[SANDBOX_DATABASE]"
YBHOST = "trialsandbox.sandbox.aws.yellowbrickcloud.com"

OPENAI_API_KEY = "[OPENAI API KEY]"
```


```python
# Import libraries and setup keys / login info
import os
import pathlib
import re
import sys
import urllib.parse as urlparse
from getpass import getpass

import psycopg2
from IPython.display import Markdown, display
from langchain.chains import LLMChain, RetrievalQAWithSourcesChain
from langchain_community.vectorstores import Yellowbrick
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Establish connection parameters to Yellowbrick.  If you've signed up for Sandbox, fill in the information from your welcome mail here:
yellowbrick_connection_string = (
    f"postgres://{urlparse.quote(YBUSER)}:{YBPASSWORD}@{YBHOST}:5432/{YBDATABASE}"
)

YB_DOC_DATABASE = "sample_data"
YB_DOC_TABLE = "yellowbrick_documentation"
embedding_table = "my_embeddings"

# API Key for OpenAI.  Signup at https://platform.openai.com
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

from langchain_core.prompts.chat import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
)
```

## Part 1: Creating a baseline chatbot backed by ChatGpt without a Vector Store

We will use langchain to query ChatGPT.  As there is no Vector Store, ChatGPT will have no context in which to answer the question.



```python
# Set up the chat model and specific prompt
system_template = """If you don't know the answer, Make up your best guess."""
messages = [
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template("{question}"),
]
prompt = ChatPromptTemplate.from_messages(messages)

chain_type_kwargs = {"prompt": prompt}
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",  # Modify model_name if you have access to GPT-4
    temperature=0,
    max_tokens=256,
)

chain = LLMChain(
    llm=llm,
    prompt=prompt,
    verbose=False,
)


def print_result_simple(query):
    result = chain(query)
    output_text = f"""### Question:
  {query}
  ### Answer: 
  {result['text']}
    """
    display(Markdown(output_text))


# Use the chain to query
print_result_simple("How many databases can be in a Yellowbrick Instance?")

print_result_simple("What's an easy way to add users in bulk to Yellowbrick?")
```

## Part 2: Connect to Yellowbrick and create the embedding tables

To load your document embeddings into Yellowbrick, you should create your own table for storing them in. Note that the 
Yellowbrick database that the table is in has to be UTF-8 encoded. 

Create a table in a UTF-8 database with the following schema, providing a table name of your choice:



```python
# Establish a connection to the Yellowbrick database
try:
    conn = psycopg2.connect(yellowbrick_connection_string)
except psycopg2.Error as e:
    print(f"Error connecting to the database: {e}")
    exit(1)

# Create a cursor object using the connection
cursor = conn.cursor()

# Define the SQL statement to create a table
create_table_query = f"""
CREATE TABLE IF NOT EXISTS {embedding_table} (
    doc_id uuid NOT NULL,
    embedding_id smallint NOT NULL,
    embedding double precision NOT NULL
)
DISTRIBUTE ON (doc_id);
truncate table {embedding_table};
"""

# Execute the SQL query to create a table
try:
    cursor.execute(create_table_query)
    print(f"Table '{embedding_table}' created successfully!")
except psycopg2.Error as e:
    print(f"Error creating table: {e}")
    conn.rollback()

# Commit changes and close the cursor and connection
conn.commit()
cursor.close()
conn.close()
```

## Part 3: Extract the documents to index from an existing table in Yellowbrick
Extract document paths and contents from an existing Yellowbrick table. We'll use these documents to create embeddings from in the next step.






```python
yellowbrick_doc_connection_string = (
    f"postgres://{urlparse.quote(YBUSER)}:{YBPASSWORD}@{YBHOST}:5432/{YB_DOC_DATABASE}"
)

print(yellowbrick_doc_connection_string)

# Establish a connection to the Yellowbrick database
conn = psycopg2.connect(yellowbrick_doc_connection_string)

# Create a cursor object
cursor = conn.cursor()

# Query to select all documents from the table
query = f"SELECT path, document FROM {YB_DOC_TABLE}"

# Execute the query
cursor.execute(query)

# Fetch all documents
yellowbrick_documents = cursor.fetchall()

print(f"Extracted {len(yellowbrick_documents)} documents successfully!")

# Close the cursor and connection
cursor.close()
conn.close()
```

## Part 4: Load the Yellowbrick Vector Store with Documents
Go through documents, split them into digestable chunks, create the embedding and insert into the Yellowbrick table. This takes around 5 minutes.



```python
# Split documents into chunks for conversion to embeddings
DOCUMENT_BASE_URL = "https://docs.yellowbrick.com/6.7.1/"  # Actual URL


separator = "\n## "  # This separator assumes Markdown docs from the repo uses ### as logical main header most of the time
chunk_size_limit = 2000
max_chunk_overlap = 200

documents = [
    Document(
        page_content=document[1],
        metadata={"source": DOCUMENT_BASE_URL + document[0].replace(".md", ".html")},
    )
    for document in yellowbrick_documents
]

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size_limit,
    chunk_overlap=max_chunk_overlap,
    separators=[separator, "\nn", "\n", ",", " ", ""],
)
split_docs = text_splitter.split_documents(documents)

docs_text = [doc.page_content for doc in split_docs]

embeddings = OpenAIEmbeddings()
vector_store = Yellowbrick.from_documents(
    documents=split_docs,
    embedding=embeddings,
    connection_string=yellowbrick_connection_string,
    table=embedding_table,
)

print(f"Created vector store with {len(documents)} documents")
```

## Part 5: Creating a chatbot that uses Yellowbrick as the vector store

Next, we add Yellowbrick as a vector store. The vector store has been populated with embeddings representing the administrative chapter of the Yellowbrick product documentation.

We'll send the same queries as above to see the impoved responses.



```python
system_template = """Use the following pieces of context to answer the users question.
Take note of the sources and include them in the answer in the format: "SOURCES: source1 source2", use "SOURCES" in capital letters regardless of the number of sources.
If you don't know the answer, just say that "I don't know", don't try to make up an answer.
----------------
{summaries}"""
messages = [
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template("{question}"),
]
prompt = ChatPromptTemplate.from_messages(messages)

vector_store = Yellowbrick(
    OpenAIEmbeddings(),
    yellowbrick_connection_string,
    embedding_table,  # Change the table name to reflect your embeddings
)

chain_type_kwargs = {"prompt": prompt}
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",  # Modify model_name if you have access to GPT-4
    temperature=0,
    max_tokens=256,
)
chain = RetrievalQAWithSourcesChain.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vector_store.as_retriever(search_kwargs={"k": 5}),
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs,
)


def print_result_sources(query):
    result = chain(query)
    output_text = f"""### Question: 
  {query}
  ### Answer: 
  {result['answer']}
  ### Sources: 
  {result['sources']}
  ### All relevant sources:
  {', '.join(list(set([doc.metadata['source'] for doc in result['source_documents']])))}
    """
    display(Markdown(output_text))


# Use the chain to query

print_result_sources("How many databases can be in a Yellowbrick Instance?")

print_result_sources("Whats an easy way to add users in bulk to Yellowbrick?")
```

## Part 6: Introducing an Index to Increase Performance

Yellowbrick also supports indexing using the Locality-Sensitive Hashing approach. This is an approximate nearest-neighbor search technique, and allows one to trade off similarity search time at the expense of accuracy. The index introduces two new tunable parameters:

- The number of hyperplanes, which is provided as an argument to `create_lsh_index(num_hyperplanes)`. The more documents, the more hyperplanes are needed. LSH is a form of dimensionality reduction. The original embeddings are transformed into lower dimensional vectors where the number of components is the same as the number of hyperplanes.
- The Hamming distance, an integer representing the breadth of the search. Smaller Hamming distances result in faster retreival but lower accuracy.

Here's how you can create an index on the embeddings we loaded into Yellowbrick. We'll also re-run the previous chat session, but this time the retrieval will use the index. Note that for such a small number of documents, you won't see the benefit of indexing in terms of performance.


```python
system_template = """Use the following pieces of context to answer the users question.
Take note of the sources and include them in the answer in the format: "SOURCES: source1 source2", use "SOURCES" in capital letters regardless of the number of sources.
If you don't know the answer, just say that "I don't know", don't try to make up an answer.
----------------
{summaries}"""
messages = [
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template("{question}"),
]
prompt = ChatPromptTemplate.from_messages(messages)

vector_store = Yellowbrick(
    OpenAIEmbeddings(),
    yellowbrick_connection_string,
    embedding_table,  # Change the table name to reflect your embeddings
)

lsh_params = Yellowbrick.IndexParams(
    Yellowbrick.IndexType.LSH, {"num_hyperplanes": 8, "hamming_distance": 2}
)
vector_store.create_index(lsh_params)

chain_type_kwargs = {"prompt": prompt}
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",  # Modify model_name if you have access to GPT-4
    temperature=0,
    max_tokens=256,
)
chain = RetrievalQAWithSourcesChain.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vector_store.as_retriever(
        k=5, search_kwargs={"index_params": lsh_params}
    ),
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs,
)


def print_result_sources(query):
    result = chain(query)
    output_text = f"""### Question: 
  {query}
  ### Answer: 
  {result['answer']}
  ### Sources: 
  {result['sources']}
  ### All relevant sources:
  {', '.join(list(set([doc.metadata['source'] for doc in result['source_documents']])))}
    """
    display(Markdown(output_text))


# Use the chain to query

print_result_sources("How many databases can be in a Yellowbrick Instance?")

print_result_sources("Whats an easy way to add users in bulk to Yellowbrick?")
```

## Next Steps:

This code can be modified to ask different questions. You can also load your own documents into the vector store. The langchain module is very flexible and can parse a large variety of files (including HTML, PDF, etc).

You can also modify this to use Huggingface embeddings models and Meta's Llama 2 LLM for a completely private chatbox experience.




################################################## yi.md ##################################################


# ChatYI

This will help you getting started with Yi [chat models](/docs/concepts/chat_models). For detailed documentation of all ChatYi features and configurations head to the [API reference](https://python.langchain.com/api_reference/lanchain_community/chat_models/lanchain_community.chat_models.yi.ChatYi.html).

[01.AI](https://www.lingyiwanwu.com/en), founded by Dr. Kai-Fu Lee, is a global company at the forefront of AI 2.0. They offer cutting-edge large language models, including the Yi series, which range from 6B to hundreds of billions of parameters. 01.AI also provides multimodal models, an open API platform, and open-source options like Yi-34B/9B/6B and Yi-VL.

## Overview
### Integration details


| Class | Package | Local | Serializable | JS support | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatYi](https://python.langchain.com/api_reference/lanchain_community/chat_models/lanchain_community.chat_models.yi.ChatYi.html) | [langchain_community](https://python.langchain.com/api_reference/community/index.html) | ✅ | ❌ | ❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain_community?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain_community?style=flat-square&label=%20) |

### Model features
| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | Native async | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |
| ❌ | ❌ | ❌ | ✅ | ❌ | ❌ | ✅ | ❌ | ✅ | ❌ | 

## Setup

To access ChatYi models you'll need to create a/an 01.AI account, get an API key, and install the `langchain_community` integration package.

### Credentials

Head to [01.AI](https://platform.01.ai) to sign up to 01.AI and generate an API key. Once you've done this set the `YI_API_KEY` environment variable:


```python
import getpass
import os

if "YI_API_KEY" not in os.environ:
    os.environ["YI_API_KEY"] = getpass.getpass("Enter your Yi API key: ")
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:


```python
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
# os.environ["LANGSMITH_TRACING"] = "true"
```

### Installation

The LangChain __ModuleName__ integration lives in the `langchain_community` package:


```python
%pip install -qU langchain_community
```

## Instantiation

Now we can instantiate our model object and generate chat completions:

- TODO: Update model instantiation with relevant params.


```python
from langchain_community.chat_models.yi import ChatYi

llm = ChatYi(
    model="yi-large",
    temperature=0,
    timeout=60,
    yi_api_base="https://api.01.ai/v1/chat/completions",
    # other params...
)
```

## Invocation



```python
from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage(content="You are an AI assistant specializing in technology trends."),
    HumanMessage(
        content="What are the potential applications of large language models in healthcare?"
    ),
]

ai_msg = llm.invoke(messages)
ai_msg
```




    AIMessage(content="Large Language Models (LLMs) have the potential to significantly impact healthcare by enhancing various aspects of patient care, research, and administrative processes. Here are some potential applications:\n\n1. **Clinical Documentation and Reporting**: LLMs can assist in generating patient reports and documentation by understanding and summarizing clinical notes, making the process more efficient and reducing the administrative burden on healthcare professionals.\n\n2. **Medical Coding and Billing**: These models can help in automating the coding process for medical billing by accurately translating clinical notes into standardized codes, reducing errors and improving billing efficiency.\n\n3. **Clinical Decision Support**: LLMs can analyze patient data and medical literature to provide evidence-based recommendations to healthcare providers, aiding in diagnosis and treatment planning.\n\n4. **Patient Education and Communication**: By simplifying medical jargon, LLMs can help in educating patients about their conditions, treatment options, and preventive care, improving patient engagement and health literacy.\n\n5. **Natural Language Processing (NLP) for EHRs**: LLMs can enhance NLP capabilities in Electronic Health Records (EHRs) systems, enabling better extraction of information from unstructured data, such as clinical notes, to support data-driven decision-making.\n\n6. **Drug Discovery and Development**: LLMs can analyze biomedical literature and clinical trial data to identify new drug candidates, predict drug interactions, and support the development of personalized medicine.\n\n7. **Telemedicine and Virtual Health Assistants**: Integrated into telemedicine platforms, LLMs can provide preliminary assessments and triage, offering patients basic health advice and determining the urgency of their needs, thus optimizing the utilization of healthcare resources.\n\n8. **Research and Literature Review**: LLMs can expedite the process of reviewing medical literature by quickly identifying relevant studies and summarizing findings, accelerating research and evidence-based practice.\n\n9. **Personalized Medicine**: By analyzing a patient's genetic information and medical history, LLMs can help in tailoring treatment plans and medication dosages, contributing to the advancement of personalized medicine.\n\n10. **Quality Improvement and Risk Assessment**: LLMs can analyze healthcare data to identify patterns that may indicate areas for quality improvement or potential risks, such as hospital-acquired infections or adverse drug events.\n\n11. **Mental Health Support**: LLMs can provide mental health support by offering coping strategies, mindfulness exercises, and preliminary assessments, serving as a complement to professional mental health services.\n\n12. **Continuing Medical Education (CME)**: LLMs can personalize CME by recommending educational content based on a healthcare provider's practice area, patient demographics, and emerging medical literature, ensuring that professionals stay updated with the latest advancements.\n\nWhile the applications of LLMs in healthcare are promising, it's crucial to address challenges such as data privacy, model bias, and the need for regulatory approval to ensure that these technologies are implemented safely and ethically.", response_metadata={'token_usage': {'completion_tokens': 656, 'prompt_tokens': 40, 'total_tokens': 696}, 'model': 'yi-large'}, id='run-870850bd-e4bf-4265-8730-1736409c0acf-0')



## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:


```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ),
        ("human", "{input}"),
    ]
)

chain = prompt | llm
chain.invoke(
    {
        "input_language": "English",
        "output_language": "German",
        "input": "I love programming.",
    }
)
```




    AIMessage(content='Ich liebe das Programmieren.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 33, 'total_tokens': 41}, 'model': 'yi-large'}, id='run-daa3bc58-8289-4d72-a24e-80622fa90d6d-0')



## API reference

For detailed documentation of all ChatYi features and configurations head to the API reference: https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.yi.ChatYi.html




################################################## you-retriever.md ##################################################


# You.com

>[you.com API](https://api.you.com) is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.

## Setup

The retriever lives in the `langchain-community` package.

You also need to set your you.com API key.


```python
%pip install --upgrade --quiet langchain-community
```


```python
import os

os.environ["YDC_API_KEY"] = ""

# For use in Chaining section
os.environ["OPENAI_API_KEY"] = ""

## ALTERNATIVE: load YDC_API_KEY from a .env file

# !pip install --quiet -U python-dotenv
# import dotenv
# dotenv.load_dotenv()
```

It's also helpful (but not needed) to set up [LangSmith](https://smith.langchain.com/) for best-in-class observability


```python
# os.environ["LANGCHAIN_TRACING_V2"] = "true"
# os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()
# os.environ["LANGCHAIN_PROJECT"] = 'Experimentz'
```

## Utility Usage


```python
from langchain_community.utilities import YouSearchAPIWrapper

utility = YouSearchAPIWrapper(num_web_results=1)

utility
```


```python
import json

# .raw_results returns the unaltered response from the API
response = utility.raw_results(query="What is the weather in NY")
# API returns an object with a `hits` key containing a list of hits
hits = response["hits"]

# with `num_web_results=1`, `hits` should be len of 1
print(len(hits))

print(json.dumps(hits, indent=2))
```

    1
    [
      {
        "description": "Be prepared with the most accurate 10-day forecast for Manhattan, NY with highs, lows, chance of precipitation from The Weather Channel and Weather.com",
        "snippets": [
          "10 Day Weather-Manhattan, NY\nToday43\u00b0/39\u00b01%\nToday\nSun 31 | Day\nGenerally cloudy. High 43F. Winds W at 10 to 15 mph.\n- Humidity54%\n- UV Index0 of 11\n- Sunrise7:19 am\n- Sunset4:38 pm\nSun 31 | Night\nCloudy. Low 39F. Winds light and variable.\n- Humidity70%\n- UV Index0 of 11\n- Moonrise9:13 pmWaning Gibbous\n- Moonset10:28 am\nMon 0145\u00b0/33\u00b07%\nMon 01\nMon 01 | Day\nConsiderable cloudiness. High around 45F. Winds light and variable.\n- Humidity71%\n- UV Index1 of 11\n- Sunrise7:19 am\n- Sunset4:39 pm\nMon 01 | Night\nA few clouds. Low 33F. Winds NNW at 5 to 10 mph.\n- Humidity64%\n- UV Index0 of 11\n- Moonrise10:14 pmWaning Gibbous\n- Moonset10:49 am\nTue 0246\u00b0/35\u00b04%\nTue 02\nTue 02 | Day\nMainly sunny. High 46F. Winds NW at 5 to 10 mph.\n- Humidity52%\n- UV Index2 of 11\n- Sunrise7:19 am\n- Sunset4:40 pm\nTue 02 | Night\nA few clouds overnight. Low around 35F. Winds W at 5 to 10 mph.\n- Humidity64%\n- UV Index0 of 11\n- Moonrise11:13 pmWaning Gibbous\n- Moonset11:08 am\nWed 0346\u00b0/38\u00b04%\nWed 03\nWed 03 | Day",
          "Radar\nLatest News\nOur Changing World\nYour Privacy\nTo personalize your product experience, we collect data from your device. We also may use or disclose to specific data vendors your precise geolocation data to provide the Services. To learn more please refer to our Privacy Policy.\nChoose how my information is shared",
          "- Humidity82%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:34 pm\nTue 26 | Night\nCloudy with light rain developing after midnight. Low 47F. Winds light and variable. Chance of rain 80%.\n- Humidity90%\n- UV Index0 of 11\n- Moonrise4:00 pmFull Moon\n- Moonset7:17 am\nWed 2754\u00b0/49\u00b093%\nWed 27\nWed 27 | Day\nRain. High 54F. Winds E at 5 to 10 mph. Chance of rain 90%. Rainfall near a half an inch.\n- Humidity93%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:35 pm\nWed 27 | Night\nSteady light rain in the evening. Showers continuing late. Low 49F. Winds light and variable. Chance of rain 70%.\n- Humidity91%\n- UV Index0 of 11\n- Moonrise4:59 pmFull Moon\n- Moonset8:12 am\nThu 2853\u00b0/42\u00b019%\nThu 28\nThu 28 | Day\nCloudy skies early will become partly cloudy later in the day. Slight chance of a rain shower. High 53F. Winds WSW at 5 to 10 mph.\n- Humidity77%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:36 pm\nThu 28 | Night\nPartly cloudy skies. Low 42F. Winds W at 5 to 10 mph.\n- Humidity71%\n- UV Index0 of 11",
          "- Moonrise2:20 amWaning Crescent\n- Moonset12:33 pm\nSun 0740\u00b0/29\u00b019%\nSun 07\nSun 07 | Day\nIntervals of clouds and sunshine. High around 40F. Winds NW at 5 to 10 mph.\n- Humidity57%\n- UV Index2 of 11\n- Sunrise7:19 am\n- Sunset4:44 pm\nSun 07 | Night\nA few clouds from time to time. Low 29F. Winds NNW at 5 to 10 mph.\n- Humidity60%\n- UV Index0 of 11\n- Moonrise3:28 amWaning Crescent\n- Moonset1:04 pm\nMon 0840\u00b0/32\u00b035%\nMon 08\nMon 08 | Day\nPartly cloudy early followed mostly cloudy skies and a few snow showers later in the day. High near 40F. Winds N at 5 to 10 mph. Chance of snow 40%.\n- UV Index1 of 11\n- Sunrise7:19 am\n- Sunset4:45 pm\nMon 08 | Night\nVariable clouds with snow showers or flurries. Low 32F. Winds NNE at 5 to 10 mph. Chance of snow 60%. Snow accumulations less than one inch.\n- UV Index0 of 11\n- Moonrise4:40 amWaning Crescent\n- Moonset1:43 pm\nLatest News\nOur Changing World\nYour Privacy",
          "- Humidity91%\n- UV Index0 of 11\n- Moonrise5:50 amWaning Crescent\n- Moonset2:35 pm\nWed 1056\u00b0/39\u00b034%\nWed 10\nWed 10 | Day\nA shower or two possible early with partly cloudy skies in the afternoon. Morning high of 56F with temps falling to near 45. Winds SW at 15 to 25 mph. Chance of rain 30%.\n- Humidity66%\n- UV Index1 of 11\n- Sunrise7:19 am\n- Sunset4:47 pm\nWed 10 | Night\nA few clouds from time to time. Low 39F. Winds WSW at 10 to 20 mph.\n- Humidity64%\n- UV Index0 of 11\n- Moonrise6:56 amWaning Crescent\n- Moonset3:38 pm\nThu 1147\u00b0/38\u00b05%\nThu 11\nThu 11 | Day\nPartly cloudy. High 47F. Winds WSW at 5 to 10 mph.\n- Humidity62%\n- UV Index2 of 11\n- Sunrise7:19 am\n- Sunset4:48 pm\nThu 11 | Night\nMostly clear skies. Low 38F. Winds W at 5 to 10 mph.\n- Humidity66%\n- UV Index0 of 11\n- Moonrise7:52 amNew Moon\n- Moonset4:53 pm\nFri 1248\u00b0/42\u00b019%\nFri 12\nFri 12 | Day\nIntervals of clouds and sunshine. High 48F. Winds WSW at 5 to 10 mph.\n- Humidity62%\n- UV Index2 of 11\n- Sunrise7:18 am\n- Sunset4:49 pm",
          "Sat 1346\u00b0/36\u00b053%\nSat 13\nSat 13 | Day\nCloudy with showers. High 46F. Winds WSW at 10 to 15 mph. Chance of rain 50%.\n- Humidity73%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:50 pm\nSat 13 | Night\nRain showers early transitioning to snow showers late. Low 36F. Winds W at 10 to 15 mph. Chance of precip 50%.\n- Humidity70%\n- UV Index0 of 11\n- Moonrise9:14 amWaxing Crescent\n- Moonset7:33 pm\nSun 1442\u00b0/34\u00b037%\nSun 14\nSun 14 | Day\nSnow showers early will transition to a few showers later. High 42F. Winds WSW at 10 to 15 mph. Chance of rain 40%.\n- Humidity63%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:51 pm\nSun 14 | Night\nVariable clouds with snow showers. Low 34F. Winds W at 10 to 15 mph. Chance of snow 60%. Snow accumulations less than one inch.\n- UV Index0 of 11\n- Moonrise9:44 amWaxing Crescent\n- Moonset8:52 pm\nMon 1540\u00b0/31\u00b051%\nMon 15\nMon 15 | Day",
          "- Humidity70%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:34 pm\nMon 25 | Night\nOvercast with showers at times. Low 43F. Winds light and variable. Chance of rain 40%.\n- Humidity80%\n- UV Index0 of 11\n- Moonrise3:08 pmWaxing Gibbous\n- Moonset6:14 am\nTue 2653\u00b0/45\u00b058%\nTue 26\nTue 26 | Day\nOvercast with rain showers at times. High 53F. Winds E at 5 to 10 mph. Chance of rain 60%.\n- Humidity79%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:34 pm\nTue 26 | Night\nShowers early then scattered thunderstorms developing late. Low near 45F. Winds ESE at 5 to 10 mph. Chance of rain 60%.\n- Humidity93%\n- UV Index0 of 11\n- Moonrise4:00 pmFull Moon\n- Moonset7:17 am\nWed 2751\u00b0/41\u00b058%\nWed 27\nWed 27 | Day\nCloudy with showers. High 51F. Winds WSW at 5 to 10 mph. Chance of rain 60%.\n- Humidity79%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:35 pm\nWed 27 | Night\nCloudy with showers. Low 41F. Winds NW at 5 to 10 mph. Chance of rain 60%.\n- Humidity72%\n- UV Index0 of 11\n- Moonrise4:59 pmFull Moon\n- Moonset8:13 am"
        ],
        "thumbnail_url": "https://imgs.search.brave.com/9xHc5-Bh2lvLyRJwQqeegm3gzoF6hawlpF8LZEjFLo8/rs:fit:200:200:1/g:ce/aHR0cHM6Ly9zLnct/eC5jby8yNDB4MTgw/X3R3Y19kZWZhdWx0/LnBuZw",
        "title": "10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...",
        "url": "https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US"
      }
    ]
    


```python
# .results returns parsed results with each snippet in a Document
response = utility.results(query="What is the weather in NY")

# .results should have a Document for each `snippet`
print(len(response))

print(response)
```

    7
    [Document(page_content='10 Day Weather-Manhattan, NY\nToday43°/39°1%\nToday\nSun 31 | Day\nGenerally cloudy. High 43F. Winds W at 10 to 15 mph.\n- Humidity54%\n- UV Index0 of 11\n- Sunrise7:19 am\n- Sunset4:38 pm\nSun 31 | Night\nCloudy. Low 39F. Winds light and variable.\n- Humidity70%\n- UV Index0 of 11\n- Moonrise9:13 pmWaning Gibbous\n- Moonset10:28 am\nMon 0145°/33°7%\nMon 01\nMon 01 | Day\nConsiderable cloudiness. High around 45F. Winds light and variable.\n- Humidity71%\n- UV Index1 of 11\n- Sunrise7:19 am\n- Sunset4:39 pm\nMon 01 | Night\nA few clouds. Low 33F. Winds NNW at 5 to 10 mph.\n- Humidity64%\n- UV Index0 of 11\n- Moonrise10:14 pmWaning Gibbous\n- Moonset10:49 am\nTue 0246°/35°4%\nTue 02\nTue 02 | Day\nMainly sunny. High 46F. Winds NW at 5 to 10 mph.\n- Humidity52%\n- UV Index2 of 11\n- Sunrise7:19 am\n- Sunset4:40 pm\nTue 02 | Night\nA few clouds overnight. Low around 35F. Winds W at 5 to 10 mph.\n- Humidity64%\n- UV Index0 of 11\n- Moonrise11:13 pmWaning Gibbous\n- Moonset11:08 am\nWed 0346°/38°4%\nWed 03\nWed 03 | Day', metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': 'https://imgs.search.brave.com/9xHc5-Bh2lvLyRJwQqeegm3gzoF6hawlpF8LZEjFLo8/rs:fit:200:200:1/g:ce/aHR0cHM6Ly9zLnct/eC5jby8yNDB4MTgw/X3R3Y19kZWZhdWx0/LnBuZw', 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Be prepared with the most accurate 10-day forecast for Manhattan, NY with highs, lows, chance of precipitation from The Weather Channel and Weather.com'}), Document(page_content='Radar\nLatest News\nOur Changing World\nYour Privacy\nTo personalize your product experience, we collect data from your device. We also may use or disclose to specific data vendors your precise geolocation data to provide the Services. To learn more please refer to our Privacy Policy.\nChoose how my information is shared', metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': 'https://imgs.search.brave.com/9xHc5-Bh2lvLyRJwQqeegm3gzoF6hawlpF8LZEjFLo8/rs:fit:200:200:1/g:ce/aHR0cHM6Ly9zLnct/eC5jby8yNDB4MTgw/X3R3Y19kZWZhdWx0/LnBuZw', 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Be prepared with the most accurate 10-day forecast for Manhattan, NY with highs, lows, chance of precipitation from The Weather Channel and Weather.com'}), Document(page_content='- Humidity82%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:34 pm\nTue 26 | Night\nCloudy with light rain developing after midnight. Low 47F. Winds light and variable. Chance of rain 80%.\n- Humidity90%\n- UV Index0 of 11\n- Moonrise4:00 pmFull Moon\n- Moonset7:17 am\nWed 2754°/49°93%\nWed 27\nWed 27 | Day\nRain. High 54F. Winds E at 5 to 10 mph. Chance of rain 90%. Rainfall near a half an inch.\n- Humidity93%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:35 pm\nWed 27 | Night\nSteady light rain in the evening. Showers continuing late. Low 49F. Winds light and variable. Chance of rain 70%.\n- Humidity91%\n- UV Index0 of 11\n- Moonrise4:59 pmFull Moon\n- Moonset8:12 am\nThu 2853°/42°19%\nThu 28\nThu 28 | Day\nCloudy skies early will become partly cloudy later in the day. Slight chance of a rain shower. High 53F. Winds WSW at 5 to 10 mph.\n- Humidity77%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:36 pm\nThu 28 | Night\nPartly cloudy skies. Low 42F. Winds W at 5 to 10 mph.\n- Humidity71%\n- UV Index0 of 11', metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': 'https://imgs.search.brave.com/9xHc5-Bh2lvLyRJwQqeegm3gzoF6hawlpF8LZEjFLo8/rs:fit:200:200:1/g:ce/aHR0cHM6Ly9zLnct/eC5jby8yNDB4MTgw/X3R3Y19kZWZhdWx0/LnBuZw', 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Be prepared with the most accurate 10-day forecast for Manhattan, NY with highs, lows, chance of precipitation from The Weather Channel and Weather.com'}), Document(page_content='- Moonrise2:20 amWaning Crescent\n- Moonset12:33 pm\nSun 0740°/29°19%\nSun 07\nSun 07 | Day\nIntervals of clouds and sunshine. High around 40F. Winds NW at 5 to 10 mph.\n- Humidity57%\n- UV Index2 of 11\n- Sunrise7:19 am\n- Sunset4:44 pm\nSun 07 | Night\nA few clouds from time to time. Low 29F. Winds NNW at 5 to 10 mph.\n- Humidity60%\n- UV Index0 of 11\n- Moonrise3:28 amWaning Crescent\n- Moonset1:04 pm\nMon 0840°/32°35%\nMon 08\nMon 08 | Day\nPartly cloudy early followed mostly cloudy skies and a few snow showers later in the day. High near 40F. Winds N at 5 to 10 mph. Chance of snow 40%.\n- UV Index1 of 11\n- Sunrise7:19 am\n- Sunset4:45 pm\nMon 08 | Night\nVariable clouds with snow showers or flurries. Low 32F. Winds NNE at 5 to 10 mph. Chance of snow 60%. Snow accumulations less than one inch.\n- UV Index0 of 11\n- Moonrise4:40 amWaning Crescent\n- Moonset1:43 pm\nLatest News\nOur Changing World\nYour Privacy', metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': 'https://imgs.search.brave.com/9xHc5-Bh2lvLyRJwQqeegm3gzoF6hawlpF8LZEjFLo8/rs:fit:200:200:1/g:ce/aHR0cHM6Ly9zLnct/eC5jby8yNDB4MTgw/X3R3Y19kZWZhdWx0/LnBuZw', 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Be prepared with the most accurate 10-day forecast for Manhattan, NY with highs, lows, chance of precipitation from The Weather Channel and Weather.com'}), Document(page_content='- Humidity91%\n- UV Index0 of 11\n- Moonrise5:50 amWaning Crescent\n- Moonset2:35 pm\nWed 1056°/39°34%\nWed 10\nWed 10 | Day\nA shower or two possible early with partly cloudy skies in the afternoon. Morning high of 56F with temps falling to near 45. Winds SW at 15 to 25 mph. Chance of rain 30%.\n- Humidity66%\n- UV Index1 of 11\n- Sunrise7:19 am\n- Sunset4:47 pm\nWed 10 | Night\nA few clouds from time to time. Low 39F. Winds WSW at 10 to 20 mph.\n- Humidity64%\n- UV Index0 of 11\n- Moonrise6:56 amWaning Crescent\n- Moonset3:38 pm\nThu 1147°/38°5%\nThu 11\nThu 11 | Day\nPartly cloudy. High 47F. Winds WSW at 5 to 10 mph.\n- Humidity62%\n- UV Index2 of 11\n- Sunrise7:19 am\n- Sunset4:48 pm\nThu 11 | Night\nMostly clear skies. Low 38F. Winds W at 5 to 10 mph.\n- Humidity66%\n- UV Index0 of 11\n- Moonrise7:52 amNew Moon\n- Moonset4:53 pm\nFri 1248°/42°19%\nFri 12\nFri 12 | Day\nIntervals of clouds and sunshine. High 48F. Winds WSW at 5 to 10 mph.\n- Humidity62%\n- UV Index2 of 11\n- Sunrise7:18 am\n- Sunset4:49 pm', metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': 'https://imgs.search.brave.com/9xHc5-Bh2lvLyRJwQqeegm3gzoF6hawlpF8LZEjFLo8/rs:fit:200:200:1/g:ce/aHR0cHM6Ly9zLnct/eC5jby8yNDB4MTgw/X3R3Y19kZWZhdWx0/LnBuZw', 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Be prepared with the most accurate 10-day forecast for Manhattan, NY with highs, lows, chance of precipitation from The Weather Channel and Weather.com'}), Document(page_content='Sat 1346°/36°53%\nSat 13\nSat 13 | Day\nCloudy with showers. High 46F. Winds WSW at 10 to 15 mph. Chance of rain 50%.\n- Humidity73%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:50 pm\nSat 13 | Night\nRain showers early transitioning to snow showers late. Low 36F. Winds W at 10 to 15 mph. Chance of precip 50%.\n- Humidity70%\n- UV Index0 of 11\n- Moonrise9:14 amWaxing Crescent\n- Moonset7:33 pm\nSun 1442°/34°37%\nSun 14\nSun 14 | Day\nSnow showers early will transition to a few showers later. High 42F. Winds WSW at 10 to 15 mph. Chance of rain 40%.\n- Humidity63%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:51 pm\nSun 14 | Night\nVariable clouds with snow showers. Low 34F. Winds W at 10 to 15 mph. Chance of snow 60%. Snow accumulations less than one inch.\n- UV Index0 of 11\n- Moonrise9:44 amWaxing Crescent\n- Moonset8:52 pm\nMon 1540°/31°51%\nMon 15\nMon 15 | Day', metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': 'https://imgs.search.brave.com/9xHc5-Bh2lvLyRJwQqeegm3gzoF6hawlpF8LZEjFLo8/rs:fit:200:200:1/g:ce/aHR0cHM6Ly9zLnct/eC5jby8yNDB4MTgw/X3R3Y19kZWZhdWx0/LnBuZw', 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Be prepared with the most accurate 10-day forecast for Manhattan, NY with highs, lows, chance of precipitation from The Weather Channel and Weather.com'}), Document(page_content='- Humidity70%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:34 pm\nMon 25 | Night\nOvercast with showers at times. Low 43F. Winds light and variable. Chance of rain 40%.\n- Humidity80%\n- UV Index0 of 11\n- Moonrise3:08 pmWaxing Gibbous\n- Moonset6:14 am\nTue 2653°/45°58%\nTue 26\nTue 26 | Day\nOvercast with rain showers at times. High 53F. Winds E at 5 to 10 mph. Chance of rain 60%.\n- Humidity79%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:34 pm\nTue 26 | Night\nShowers early then scattered thunderstorms developing late. Low near 45F. Winds ESE at 5 to 10 mph. Chance of rain 60%.\n- Humidity93%\n- UV Index0 of 11\n- Moonrise4:00 pmFull Moon\n- Moonset7:17 am\nWed 2751°/41°58%\nWed 27\nWed 27 | Day\nCloudy with showers. High 51F. Winds WSW at 5 to 10 mph. Chance of rain 60%.\n- Humidity79%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:35 pm\nWed 27 | Night\nCloudy with showers. Low 41F. Winds NW at 5 to 10 mph. Chance of rain 60%.\n- Humidity72%\n- UV Index0 of 11\n- Moonrise4:59 pmFull Moon\n- Moonset8:13 am', metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': 'https://imgs.search.brave.com/9xHc5-Bh2lvLyRJwQqeegm3gzoF6hawlpF8LZEjFLo8/rs:fit:200:200:1/g:ce/aHR0cHM6Ly9zLnct/eC5jby8yNDB4MTgw/X3R3Y19kZWZhdWx0/LnBuZw', 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Be prepared with the most accurate 10-day forecast for Manhattan, NY with highs, lows, chance of precipitation from The Weather Channel and Weather.com'})]
    

## Retriever Usage


```python
from langchain_community.retrievers.you import YouRetriever

retriever = YouRetriever(num_web_results=1)

retriever
```


```python
# .invoke wraps utility.results
response = retriever.invoke("What is the weather in NY")

# .invoke should have a Document for each `snippet`
print(len(response))

print(response)
```

    7
    [Document(page_content='10 Day Weather-Manhattan, NY\nToday43°/39°1%\nToday\nSun 31 | Day\nGenerally cloudy. High 43F. Winds W at 10 to 15 mph.\n- Humidity54%\n- UV Index0 of 11\n- Sunrise7:19 am\n- Sunset4:38 pm\nSun 31 | Night\nCloudy. Low 39F. Winds light and variable.\n- Humidity70%\n- UV Index0 of 11\n- Moonrise9:13 pmWaning Gibbous\n- Moonset10:28 am\nMon 0145°/33°7%\nMon 01\nMon 01 | Day\nConsiderable cloudiness. High around 45F. Winds light and variable.\n- Humidity71%\n- UV Index1 of 11\n- Sunrise7:19 am\n- Sunset4:39 pm\nMon 01 | Night\nA few clouds. Low 33F. Winds NNW at 5 to 10 mph.\n- Humidity64%\n- UV Index0 of 11\n- Moonrise10:14 pmWaning Gibbous\n- Moonset10:49 am\nTue 0246°/35°4%\nTue 02\nTue 02 | Day\nMainly sunny. High 46F. Winds NW at 5 to 10 mph.\n- Humidity52%\n- UV Index2 of 11\n- Sunrise7:19 am\n- Sunset4:40 pm\nTue 02 | Night\nA few clouds overnight. Low around 35F. Winds W at 5 to 10 mph.\n- Humidity64%\n- UV Index0 of 11\n- Moonrise11:13 pmWaning Gibbous\n- Moonset11:08 am\nWed 0346°/38°4%\nWed 03\nWed 03 | Day', metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': 'https://imgs.search.brave.com/9xHc5-Bh2lvLyRJwQqeegm3gzoF6hawlpF8LZEjFLo8/rs:fit:200:200:1/g:ce/aHR0cHM6Ly9zLnct/eC5jby8yNDB4MTgw/X3R3Y19kZWZhdWx0/LnBuZw', 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Be prepared with the most accurate 10-day forecast for Manhattan, NY with highs, lows, chance of precipitation from The Weather Channel and Weather.com'}), Document(page_content='Radar\nLatest News\nOur Changing World\nYour Privacy\nTo personalize your product experience, we collect data from your device. We also may use or disclose to specific data vendors your precise geolocation data to provide the Services. To learn more please refer to our Privacy Policy.\nChoose how my information is shared', metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': 'https://imgs.search.brave.com/9xHc5-Bh2lvLyRJwQqeegm3gzoF6hawlpF8LZEjFLo8/rs:fit:200:200:1/g:ce/aHR0cHM6Ly9zLnct/eC5jby8yNDB4MTgw/X3R3Y19kZWZhdWx0/LnBuZw', 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Be prepared with the most accurate 10-day forecast for Manhattan, NY with highs, lows, chance of precipitation from The Weather Channel and Weather.com'}), Document(page_content='- Humidity82%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:34 pm\nTue 26 | Night\nCloudy with light rain developing after midnight. Low 47F. Winds light and variable. Chance of rain 80%.\n- Humidity90%\n- UV Index0 of 11\n- Moonrise4:00 pmFull Moon\n- Moonset7:17 am\nWed 2754°/49°93%\nWed 27\nWed 27 | Day\nRain. High 54F. Winds E at 5 to 10 mph. Chance of rain 90%. Rainfall near a half an inch.\n- Humidity93%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:35 pm\nWed 27 | Night\nSteady light rain in the evening. Showers continuing late. Low 49F. Winds light and variable. Chance of rain 70%.\n- Humidity91%\n- UV Index0 of 11\n- Moonrise4:59 pmFull Moon\n- Moonset8:12 am\nThu 2853°/42°19%\nThu 28\nThu 28 | Day\nCloudy skies early will become partly cloudy later in the day. Slight chance of a rain shower. High 53F. Winds WSW at 5 to 10 mph.\n- Humidity77%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:36 pm\nThu 28 | Night\nPartly cloudy skies. Low 42F. Winds W at 5 to 10 mph.\n- Humidity71%\n- UV Index0 of 11', metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': 'https://imgs.search.brave.com/9xHc5-Bh2lvLyRJwQqeegm3gzoF6hawlpF8LZEjFLo8/rs:fit:200:200:1/g:ce/aHR0cHM6Ly9zLnct/eC5jby8yNDB4MTgw/X3R3Y19kZWZhdWx0/LnBuZw', 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Be prepared with the most accurate 10-day forecast for Manhattan, NY with highs, lows, chance of precipitation from The Weather Channel and Weather.com'}), Document(page_content='- Moonrise2:20 amWaning Crescent\n- Moonset12:33 pm\nSun 0740°/29°19%\nSun 07\nSun 07 | Day\nIntervals of clouds and sunshine. High around 40F. Winds NW at 5 to 10 mph.\n- Humidity57%\n- UV Index2 of 11\n- Sunrise7:19 am\n- Sunset4:44 pm\nSun 07 | Night\nA few clouds from time to time. Low 29F. Winds NNW at 5 to 10 mph.\n- Humidity60%\n- UV Index0 of 11\n- Moonrise3:28 amWaning Crescent\n- Moonset1:04 pm\nMon 0840°/32°35%\nMon 08\nMon 08 | Day\nPartly cloudy early followed mostly cloudy skies and a few snow showers later in the day. High near 40F. Winds N at 5 to 10 mph. Chance of snow 40%.\n- UV Index1 of 11\n- Sunrise7:19 am\n- Sunset4:45 pm\nMon 08 | Night\nVariable clouds with snow showers or flurries. Low 32F. Winds NNE at 5 to 10 mph. Chance of snow 60%. Snow accumulations less than one inch.\n- UV Index0 of 11\n- Moonrise4:40 amWaning Crescent\n- Moonset1:43 pm\nLatest News\nOur Changing World\nYour Privacy', metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': 'https://imgs.search.brave.com/9xHc5-Bh2lvLyRJwQqeegm3gzoF6hawlpF8LZEjFLo8/rs:fit:200:200:1/g:ce/aHR0cHM6Ly9zLnct/eC5jby8yNDB4MTgw/X3R3Y19kZWZhdWx0/LnBuZw', 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Be prepared with the most accurate 10-day forecast for Manhattan, NY with highs, lows, chance of precipitation from The Weather Channel and Weather.com'}), Document(page_content='- Humidity91%\n- UV Index0 of 11\n- Moonrise5:50 amWaning Crescent\n- Moonset2:35 pm\nWed 1056°/39°34%\nWed 10\nWed 10 | Day\nA shower or two possible early with partly cloudy skies in the afternoon. Morning high of 56F with temps falling to near 45. Winds SW at 15 to 25 mph. Chance of rain 30%.\n- Humidity66%\n- UV Index1 of 11\n- Sunrise7:19 am\n- Sunset4:47 pm\nWed 10 | Night\nA few clouds from time to time. Low 39F. Winds WSW at 10 to 20 mph.\n- Humidity64%\n- UV Index0 of 11\n- Moonrise6:56 amWaning Crescent\n- Moonset3:38 pm\nThu 1147°/38°5%\nThu 11\nThu 11 | Day\nPartly cloudy. High 47F. Winds WSW at 5 to 10 mph.\n- Humidity62%\n- UV Index2 of 11\n- Sunrise7:19 am\n- Sunset4:48 pm\nThu 11 | Night\nMostly clear skies. Low 38F. Winds W at 5 to 10 mph.\n- Humidity66%\n- UV Index0 of 11\n- Moonrise7:52 amNew Moon\n- Moonset4:53 pm\nFri 1248°/42°19%\nFri 12\nFri 12 | Day\nIntervals of clouds and sunshine. High 48F. Winds WSW at 5 to 10 mph.\n- Humidity62%\n- UV Index2 of 11\n- Sunrise7:18 am\n- Sunset4:49 pm', metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': 'https://imgs.search.brave.com/9xHc5-Bh2lvLyRJwQqeegm3gzoF6hawlpF8LZEjFLo8/rs:fit:200:200:1/g:ce/aHR0cHM6Ly9zLnct/eC5jby8yNDB4MTgw/X3R3Y19kZWZhdWx0/LnBuZw', 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Be prepared with the most accurate 10-day forecast for Manhattan, NY with highs, lows, chance of precipitation from The Weather Channel and Weather.com'}), Document(page_content='Sat 1346°/36°53%\nSat 13\nSat 13 | Day\nCloudy with showers. High 46F. Winds WSW at 10 to 15 mph. Chance of rain 50%.\n- Humidity73%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:50 pm\nSat 13 | Night\nRain showers early transitioning to snow showers late. Low 36F. Winds W at 10 to 15 mph. Chance of precip 50%.\n- Humidity70%\n- UV Index0 of 11\n- Moonrise9:14 amWaxing Crescent\n- Moonset7:33 pm\nSun 1442°/34°37%\nSun 14\nSun 14 | Day\nSnow showers early will transition to a few showers later. High 42F. Winds WSW at 10 to 15 mph. Chance of rain 40%.\n- Humidity63%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:51 pm\nSun 14 | Night\nVariable clouds with snow showers. Low 34F. Winds W at 10 to 15 mph. Chance of snow 60%. Snow accumulations less than one inch.\n- UV Index0 of 11\n- Moonrise9:44 amWaxing Crescent\n- Moonset8:52 pm\nMon 1540°/31°51%\nMon 15\nMon 15 | Day', metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': 'https://imgs.search.brave.com/9xHc5-Bh2lvLyRJwQqeegm3gzoF6hawlpF8LZEjFLo8/rs:fit:200:200:1/g:ce/aHR0cHM6Ly9zLnct/eC5jby8yNDB4MTgw/X3R3Y19kZWZhdWx0/LnBuZw', 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Be prepared with the most accurate 10-day forecast for Manhattan, NY with highs, lows, chance of precipitation from The Weather Channel and Weather.com'}), Document(page_content='- Humidity70%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:34 pm\nMon 25 | Night\nOvercast with showers at times. Low 43F. Winds light and variable. Chance of rain 40%.\n- Humidity80%\n- UV Index0 of 11\n- Moonrise3:08 pmWaxing Gibbous\n- Moonset6:14 am\nTue 2653°/45°58%\nTue 26\nTue 26 | Day\nOvercast with rain showers at times. High 53F. Winds E at 5 to 10 mph. Chance of rain 60%.\n- Humidity79%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:34 pm\nTue 26 | Night\nShowers early then scattered thunderstorms developing late. Low near 45F. Winds ESE at 5 to 10 mph. Chance of rain 60%.\n- Humidity93%\n- UV Index0 of 11\n- Moonrise4:00 pmFull Moon\n- Moonset7:17 am\nWed 2751°/41°58%\nWed 27\nWed 27 | Day\nCloudy with showers. High 51F. Winds WSW at 5 to 10 mph. Chance of rain 60%.\n- Humidity79%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:35 pm\nWed 27 | Night\nCloudy with showers. Low 41F. Winds NW at 5 to 10 mph. Chance of rain 60%.\n- Humidity72%\n- UV Index0 of 11\n- Moonrise4:59 pmFull Moon\n- Moonset8:13 am', metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': 'https://imgs.search.brave.com/9xHc5-Bh2lvLyRJwQqeegm3gzoF6hawlpF8LZEjFLo8/rs:fit:200:200:1/g:ce/aHR0cHM6Ly9zLnct/eC5jby8yNDB4MTgw/X3R3Y19kZWZhdWx0/LnBuZw', 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Be prepared with the most accurate 10-day forecast for Manhattan, NY with highs, lows, chance of precipitation from The Weather Channel and Weather.com'})]
    

## Chaining


```python
# you need a model to use in the chain
!pip install --upgrade --quiet langchain-openai
```


```python
from langchain_community.retrievers.you import YouRetriever
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

# set up runnable
runnable = RunnablePassthrough

# set up retriever, limit sources to one
retriever = YouRetriever(num_web_results=1)

# set up model
model = ChatOpenAI(model="gpt-3.5-turbo-16k")

# set up output parser
output_parser = StrOutputParser()
```

### Invoke


```python
# set up prompt that expects one question
prompt = ChatPromptTemplate.from_template(
    """Answer the question based only on the context provided.

Context: {context}

Question: {question}"""
)

# set up chain
chain = (
    runnable.assign(context=(lambda x: x["question"]) | retriever)
    | prompt
    | model
    | output_parser
)

output = chain.invoke({"question": "what is the weather in NY today"})

print(output)
```

    The weather in New York City today is 43° with a high/low of --/39°. The wind is 3 mph, humidity is 63%, and the air quality is considered good.
    

### Stream


```python
# set up prompt that expects one question
prompt = ChatPromptTemplate.from_template(
    """Answer the question based only on the context provided.

Context: {context}

Question: {question}"""
)

# set up chain - same as above
chain = (
    runnable.assign(context=(lambda x: x["question"]) | retriever)
    | prompt
    | model
    | output_parser
)

for s in chain.stream({"question": "what is the weather in NY today"}):
    print(s, end="", flush=True)
```

    The weather in New York City today is a high of 39°F and a low of 31°F with a feels like temperature of 43°F. The wind speed is 3 mph, humidity is 63%, and the air quality is considered to be good.

### Batch


```python
chain = (
    runnable.assign(context=(lambda x: x["question"]) | retriever)
    | prompt
    | model
    | output_parser
)

output = chain.batch(
    [
        {"question": "what is the weather in NY today"},
        {"question": "what is the weather in sf today"},
    ]
)

for o in output:
    print(o)
```

    Based on the provided context, the weather in New York City today is 43° with a high/low of --/39°.
    Based on the provided context, the current weather in San Francisco is partly cloudy with a temperature of 61°F and a humidity of 57%.
    




################################################## you.md ##################################################


# You.com Search

The [you.com API](https://api.you.com) is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.

## Setup

The tool lives in the `langchain-community` package.

You also need to set your you.com API key.


```python
%pip install --upgrade --quiet langchain-community
```


```python
import os

os.environ["YDC_API_KEY"] = ""

# For use in Chaining section
os.environ["OPENAI_API_KEY"] = ""

## ALTERNATIVE: load YDC_API_KEY from a .env file

# !pip install --quiet -U python-dotenv
# import dotenv
# dotenv.load_dotenv()
```

It's also helpful (but not needed) to set up [LangSmith](https://smith.langchain.com/) for best-in-class observability


```python
# os.environ["LANGCHAIN_TRACING_V2"] = "true"
# os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()
```

## Tool Usage


```python
from langchain_community.tools.you import YouSearchTool
from langchain_community.utilities.you import YouSearchAPIWrapper

api_wrapper = YouSearchAPIWrapper(num_web_results=1)
tool = YouSearchTool(api_wrapper=api_wrapper)

tool
```




    YouSearchTool(api_wrapper=YouSearchAPIWrapper(ydc_api_key='054da371-e73b-47c1-a6d9-3b0cddf0fa3e<__>1Obt7EETU8N2v5f4MxaH0Zhx', num_web_results=1, safesearch=None, country=None, k=None, n_snippets_per_hit=None, endpoint_type='search', n_hits=None))




```python
# .invoke wraps utility.results
response = tool.invoke("What is the weather in NY")

# .invoke should have a Document for each `snippet`
print(len(response))

for item in response:
    print(item)
```

    7
    page_content='10 Day Weather-Manhattan, NY\nToday43°/39°1%\nToday\nSun 31 | Day\nGenerally cloudy. High 43F. Winds W at 10 to 15 mph.\n- Humidity54%\n- UV Index0 of 11\n- Sunrise7:19 am\n- Sunset4:38 pm\nSun 31 | Night\nCloudy. Low 39F. Winds light and variable.\n- Humidity70%\n- UV Index0 of 11\n- Moonrise9:13 pmWaning Gibbous\n- Moonset10:28 am\nMon 0145°/33°7%\nMon 01\nMon 01 | Day\nConsiderable cloudiness. High around 45F. Winds light and variable.\n- Humidity71%\n- UV Index1 of 11\n- Sunrise7:19 am\n- Sunset4:39 pm\nMon 01 | Night\nA few clouds. Low 33F. Winds NNW at 5 to 10 mph.\n- Humidity64%\n- UV Index0 of 11\n- Moonrise10:14 pmWaning Gibbous\n- Moonset10:49 am\nTue 0246°/35°4%\nTue 02\nTue 02 | Day\nMainly sunny. High 46F. Winds NW at 5 to 10 mph.\n- Humidity52%\n- UV Index2 of 11\n- Sunrise7:19 am\n- Sunset4:40 pm\nTue 02 | Night\nA few clouds overnight. Low around 35F. Winds W at 5 to 10 mph.\n- Humidity64%\n- UV Index0 of 11\n- Moonrise11:13 pmWaning Gibbous\n- Moonset11:08 am\nWed 0346°/38°4%\nWed 03\nWed 03 | Day' metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': None, 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Some sun in the morning with increasing clouds during the afternoon. High around 45F. Winds SSE at 5 to 10 mph. ... Cloudy with showers. Low near 40F. Winds SSE at 5 to 10 mph. Chance of rain 60%. ... A steady rain in the morning. Showers continuing in the afternoon.'}
    page_content='Radar\nLatest News\nOur Changing World\nYour Privacy\nTo personalize your product experience, we collect data from your device. We also may use or disclose to specific data vendors your precise geolocation data to provide the Services. To learn more please refer to our Privacy Policy.\nChoose how my information is shared' metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': None, 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Some sun in the morning with increasing clouds during the afternoon. High around 45F. Winds SSE at 5 to 10 mph. ... Cloudy with showers. Low near 40F. Winds SSE at 5 to 10 mph. Chance of rain 60%. ... A steady rain in the morning. Showers continuing in the afternoon.'}
    page_content='- Humidity82%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:34 pm\nTue 26 | Night\nCloudy with light rain developing after midnight. Low 47F. Winds light and variable. Chance of rain 80%.\n- Humidity90%\n- UV Index0 of 11\n- Moonrise4:00 pmFull Moon\n- Moonset7:17 am\nWed 2754°/49°93%\nWed 27\nWed 27 | Day\nRain. High 54F. Winds E at 5 to 10 mph. Chance of rain 90%. Rainfall near a half an inch.\n- Humidity93%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:35 pm\nWed 27 | Night\nSteady light rain in the evening. Showers continuing late. Low 49F. Winds light and variable. Chance of rain 70%.\n- Humidity91%\n- UV Index0 of 11\n- Moonrise4:59 pmFull Moon\n- Moonset8:12 am\nThu 2853°/42°19%\nThu 28\nThu 28 | Day\nCloudy skies early will become partly cloudy later in the day. Slight chance of a rain shower. High 53F. Winds WSW at 5 to 10 mph.\n- Humidity77%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:36 pm\nThu 28 | Night\nPartly cloudy skies. Low 42F. Winds W at 5 to 10 mph.\n- Humidity71%\n- UV Index0 of 11' metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': None, 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Some sun in the morning with increasing clouds during the afternoon. High around 45F. Winds SSE at 5 to 10 mph. ... Cloudy with showers. Low near 40F. Winds SSE at 5 to 10 mph. Chance of rain 60%. ... A steady rain in the morning. Showers continuing in the afternoon.'}
    page_content='- Moonrise2:20 amWaning Crescent\n- Moonset12:33 pm\nSun 0740°/29°19%\nSun 07\nSun 07 | Day\nIntervals of clouds and sunshine. High around 40F. Winds NW at 5 to 10 mph.\n- Humidity57%\n- UV Index2 of 11\n- Sunrise7:19 am\n- Sunset4:44 pm\nSun 07 | Night\nA few clouds from time to time. Low 29F. Winds NNW at 5 to 10 mph.\n- Humidity60%\n- UV Index0 of 11\n- Moonrise3:28 amWaning Crescent\n- Moonset1:04 pm\nMon 0840°/32°35%\nMon 08\nMon 08 | Day\nPartly cloudy early followed mostly cloudy skies and a few snow showers later in the day. High near 40F. Winds N at 5 to 10 mph. Chance of snow 40%.\n- UV Index1 of 11\n- Sunrise7:19 am\n- Sunset4:45 pm\nMon 08 | Night\nVariable clouds with snow showers or flurries. Low 32F. Winds NNE at 5 to 10 mph. Chance of snow 60%. Snow accumulations less than one inch.\n- UV Index0 of 11\n- Moonrise4:40 amWaning Crescent\n- Moonset1:43 pm\nLatest News\nOur Changing World\nYour Privacy' metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': None, 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Some sun in the morning with increasing clouds during the afternoon. High around 45F. Winds SSE at 5 to 10 mph. ... Cloudy with showers. Low near 40F. Winds SSE at 5 to 10 mph. Chance of rain 60%. ... A steady rain in the morning. Showers continuing in the afternoon.'}
    page_content='- Humidity91%\n- UV Index0 of 11\n- Moonrise5:50 amWaning Crescent\n- Moonset2:35 pm\nWed 1056°/39°34%\nWed 10\nWed 10 | Day\nA shower or two possible early with partly cloudy skies in the afternoon. Morning high of 56F with temps falling to near 45. Winds SW at 15 to 25 mph. Chance of rain 30%.\n- Humidity66%\n- UV Index1 of 11\n- Sunrise7:19 am\n- Sunset4:47 pm\nWed 10 | Night\nA few clouds from time to time. Low 39F. Winds WSW at 10 to 20 mph.\n- Humidity64%\n- UV Index0 of 11\n- Moonrise6:56 amWaning Crescent\n- Moonset3:38 pm\nThu 1147°/38°5%\nThu 11\nThu 11 | Day\nPartly cloudy. High 47F. Winds WSW at 5 to 10 mph.\n- Humidity62%\n- UV Index2 of 11\n- Sunrise7:19 am\n- Sunset4:48 pm\nThu 11 | Night\nMostly clear skies. Low 38F. Winds W at 5 to 10 mph.\n- Humidity66%\n- UV Index0 of 11\n- Moonrise7:52 amNew Moon\n- Moonset4:53 pm\nFri 1248°/42°19%\nFri 12\nFri 12 | Day\nIntervals of clouds and sunshine. High 48F. Winds WSW at 5 to 10 mph.\n- Humidity62%\n- UV Index2 of 11\n- Sunrise7:18 am\n- Sunset4:49 pm' metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': None, 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Some sun in the morning with increasing clouds during the afternoon. High around 45F. Winds SSE at 5 to 10 mph. ... Cloudy with showers. Low near 40F. Winds SSE at 5 to 10 mph. Chance of rain 60%. ... A steady rain in the morning. Showers continuing in the afternoon.'}
    page_content='Sat 1346°/36°53%\nSat 13\nSat 13 | Day\nCloudy with showers. High 46F. Winds WSW at 10 to 15 mph. Chance of rain 50%.\n- Humidity73%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:50 pm\nSat 13 | Night\nRain showers early transitioning to snow showers late. Low 36F. Winds W at 10 to 15 mph. Chance of precip 50%.\n- Humidity70%\n- UV Index0 of 11\n- Moonrise9:14 amWaxing Crescent\n- Moonset7:33 pm\nSun 1442°/34°37%\nSun 14\nSun 14 | Day\nSnow showers early will transition to a few showers later. High 42F. Winds WSW at 10 to 15 mph. Chance of rain 40%.\n- Humidity63%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:51 pm\nSun 14 | Night\nVariable clouds with snow showers. Low 34F. Winds W at 10 to 15 mph. Chance of snow 60%. Snow accumulations less than one inch.\n- UV Index0 of 11\n- Moonrise9:44 amWaxing Crescent\n- Moonset8:52 pm\nMon 1540°/31°51%\nMon 15\nMon 15 | Day' metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': None, 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Some sun in the morning with increasing clouds during the afternoon. High around 45F. Winds SSE at 5 to 10 mph. ... Cloudy with showers. Low near 40F. Winds SSE at 5 to 10 mph. Chance of rain 60%. ... A steady rain in the morning. Showers continuing in the afternoon.'}
    page_content='- Humidity70%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:34 pm\nMon 25 | Night\nOvercast with showers at times. Low 43F. Winds light and variable. Chance of rain 40%.\n- Humidity80%\n- UV Index0 of 11\n- Moonrise3:08 pmWaxing Gibbous\n- Moonset6:14 am\nTue 2653°/45°58%\nTue 26\nTue 26 | Day\nOvercast with rain showers at times. High 53F. Winds E at 5 to 10 mph. Chance of rain 60%.\n- Humidity79%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:34 pm\nTue 26 | Night\nShowers early then scattered thunderstorms developing late. Low near 45F. Winds ESE at 5 to 10 mph. Chance of rain 60%.\n- Humidity93%\n- UV Index0 of 11\n- Moonrise4:00 pmFull Moon\n- Moonset7:17 am\nWed 2751°/41°58%\nWed 27\nWed 27 | Day\nCloudy with showers. High 51F. Winds WSW at 5 to 10 mph. Chance of rain 60%.\n- Humidity79%\n- UV Index1 of 11\n- Sunrise7:18 am\n- Sunset4:35 pm\nWed 27 | Night\nCloudy with showers. Low 41F. Winds NW at 5 to 10 mph. Chance of rain 60%.\n- Humidity72%\n- UV Index0 of 11\n- Moonrise4:59 pmFull Moon\n- Moonset8:13 am' metadata={'url': 'https://weather.com/weather/tenday/l/New+York+NY+USNY0996:1:US', 'thumbnail_url': None, 'title': '10-Day Weather Forecast for Manhattan, NY - The Weather Channel ...', 'description': 'Some sun in the morning with increasing clouds during the afternoon. High around 45F. Winds SSE at 5 to 10 mph. ... Cloudy with showers. Low near 40F. Winds SSE at 5 to 10 mph. Chance of rain 60%. ... A steady rain in the morning. Showers continuing in the afternoon.'}
    

## Chaining

We show here how to use it as part of an [agent](/docs/tutorials/agents). We use the OpenAI Functions Agent, so we will need to setup and install the required dependencies for that. We will also use [LangSmith Hub](https://smith.langchain.com/hub) to pull the prompt from, so we will need to install that.


```python
# you need a model to use in the chain
!pip install --upgrade --quiet langchain langchain-openai langchainhub langchain-community
```


```python
from langchain import hub
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain_openai import ChatOpenAI

instructions = """You are an assistant."""
base_prompt = hub.pull("langchain-ai/openai-functions-template")
prompt = base_prompt.partial(instructions=instructions)
llm = ChatOpenAI(temperature=0)
you_tool = YouSearchTool(api_wrapper=YouSearchAPIWrapper(num_web_results=1))
tools = [you_tool]
agent = create_openai_functions_agent(llm, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
)
```


```python
agent_executor.invoke({"input": "What is the weather in NY today?"})
```

    
    
    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3m
    Invoking: `you_search` with `{'query': 'weather in NY today'}`
    
    
    [0m[36;1m[1;3m[Document(page_content="New York City, NY Forecast\nWeather Today in New York City, NY\nFeels Like43°\n7:17 am\n4:32 pm\nHigh / Low\n--/39°\nWind\n3 mph\nHumidity\n63%\nDew Point\n31°\nPressure\n30.44 in\nUV Index\n0 of 11\nVisibility\n10 mi\nMoon Phase\nWaxing Gibbous\nAdvertisement\nLatest News\nHealth & Activities\nAdvertisement\nAir Quality Index\nGood\nAir quality is considered satisfactory, and air pollution poses little or no risk.\nAdvertisement\nHappening Near New York City, NY\nPopular Nextdoor posts\nHas anyone announced --in which case I apologize--that Bo's Bagels--opened yesterday where Tommy's pizza once was on uptown side of Broadway, bet 155 and 156.They have been incredibly successful in Harlem ( W.116th) and are now spreading their wings. Open 8-3 now, will go later soon, delivers and has many varieties baked on site!!! Just had lunch there!!\nWash Hts S (W155-W159-Bdway)\nNeighbor\n4d ago", metadata={'url': 'https://weather.com/weather/today/l/96f2f84af9a5f5d452eb0574d4e4d8a840c71b05e22264ebdc0056433a642c84', 'thumbnail_url': None, 'title': 'Weather Forecast and Conditions for New York City, NY - The Weather ...', 'description': 'Today’s and tonight’s New York City, NY weather forecast, weather conditions and Doppler radar from The Weather Channel and Weather.com'}), Document(page_content='time hearing racial slurs in my life and in NYC. So I don’t know why tonight’s encounter makes me feel so helpless and sad. Her apartment is on our usual route for walks, and now I’m already thinking about how I need to behave if this situation repeats again. This just doesn’t feel fair. How would you handle it? Thanks for reading!', metadata={'url': 'https://weather.com/weather/today/l/96f2f84af9a5f5d452eb0574d4e4d8a840c71b05e22264ebdc0056433a642c84', 'thumbnail_url': None, 'title': 'Weather Forecast and Conditions for New York City, NY - The Weather ...', 'description': 'Today’s and tonight’s New York City, NY weather forecast, weather conditions and Doppler radar from The Weather Channel and Weather.com'}), Document(page_content="Hell's Kitchen\nNeighbor\n4d ago\nJust curious.Are the M100/M100 and BX19 buses free? It seems like I'm the only one who pays. Even had a group of non-payers give attitude because I held up the line to pay my fare. Non-payers being whole families, men and women of all ages. I remember when you used to be scared to walk on for free but do understand the drivers are told not to intervene. Still it should either be free or paid fare.\nHamilton Hts (W148St-Broadway)\nNeighbor\n4d ago\nHey there, I am a journalism student at Craig Newmark School of Journalism looking for stories in the Lower East Side and East Village.Any ideas?\nAlphabet City\nNeighbor\n8d ago\nProvided by\nAdvertisement\nYour Privacy\nTo personalize your product experience, we collect data from your device. We also may use or disclose to specific data vendors your precise geolocation data to provide the Services. To learn more please refer to our Privacy Policy.\nReview All Privacy and Ad Settings\nChoose how my information is shared", metadata={'url': 'https://weather.com/weather/today/l/96f2f84af9a5f5d452eb0574d4e4d8a840c71b05e22264ebdc0056433a642c84', 'thumbnail_url': None, 'title': 'Weather Forecast and Conditions for New York City, NY - The Weather ...', 'description': 'Today’s and tonight’s New York City, NY weather forecast, weather conditions and Doppler radar from The Weather Channel and Weather.com'}), Document(page_content='Review All Privacy and Ad Settings\nChoose how my information is shared', metadata={'url': 'https://weather.com/weather/today/l/96f2f84af9a5f5d452eb0574d4e4d8a840c71b05e22264ebdc0056433a642c84', 'thumbnail_url': None, 'title': 'Weather Forecast and Conditions for New York City, NY - The Weather ...', 'description': 'Today’s and tonight’s New York City, NY weather forecast, weather conditions and Doppler radar from The Weather Channel and Weather.com'}), Document(page_content='2d ago\nProvided by\nAdvertisement\nYour Privacy\nTo personalize your product experience, we collect data from your device. We also may use or disclose to specific data vendors your precise geolocation data to provide the Services. To learn more please refer to our Privacy Policy.\nReview All Privacy and Ad Settings\nChoose how my information is shared', metadata={'url': 'https://weather.com/weather/today/l/96f2f84af9a5f5d452eb0574d4e4d8a840c71b05e22264ebdc0056433a642c84', 'thumbnail_url': None, 'title': 'Weather Forecast and Conditions for New York City, NY - The Weather ...', 'description': 'Today’s and tonight’s New York City, NY weather forecast, weather conditions and Doppler radar from The Weather Channel and Weather.com'}), Document(page_content="Lenox Hill\nNeighbor\n8d ago\nParking.It is getting worst. What happened to resident-only neighborhood parking program? With this upcoming congested traffic tolling below 96th Street, competition for parking bordering the zone is going to get tougher. This should be part of the congested traffic agenda.\nCent Harlem S (120-FDouglass)\nNeighbor\n9d ago\nI thought these sites were much more localized.If there is one for the UWS I'll join it but have no reason to be reading about Yorkville or Union NJ.\nLincoln Sq (W70-Amst-66-WEnd)\nNeighbor\n10d ago\nProvided by\nAdvertisement\nYour Privacy\nTo personalize your product experience, we collect data from your device. We also may use or disclose to specific data vendors your precise geolocation data to provide the Services. To learn more please refer to our Privacy Policy.\nReview All Privacy and Ad Settings\nChoose how my information is shared", metadata={'url': 'https://weather.com/weather/today/l/96f2f84af9a5f5d452eb0574d4e4d8a840c71b05e22264ebdc0056433a642c84', 'thumbnail_url': None, 'title': 'Weather Forecast and Conditions for New York City, NY - The Weather ...', 'description': 'Today’s and tonight’s New York City, NY weather forecast, weather conditions and Doppler radar from The Weather Channel and Weather.com'})][0m[32;1m[1;3mThe weather in New York City today is as follows:
    - Feels Like: 43°F
    - High/Low: --/39°F
    - Wind: 3 mph
    - Humidity: 63%
    - Dew Point: 31°F
    - Pressure: 30.44 in
    - UV Index: 0 of 11
    - Visibility: 10 mi
    - Moon Phase: Waxing Gibbous
    
    For more details, you can visit [The Weather Channel](https://weather.com/weather/today/l/96f2f84af9a5f5d452eb0574d4e4d8a840c71b05e22264ebdc0056433a642c84).[0m
    
    [1m> Finished chain.[0m
    




    {'input': 'What is the weather in NY today?',
     'output': 'The weather in New York City today is as follows:\n- Feels Like: 43°F\n- High/Low: --/39°F\n- Wind: 3 mph\n- Humidity: 63%\n- Dew Point: 31°F\n- Pressure: 30.44 in\n- UV Index: 0 of 11\n- Visibility: 10 mi\n- Moon Phase: Waxing Gibbous\n\nFor more details, you can visit [The Weather Channel](https://weather.com/weather/today/l/96f2f84af9a5f5d452eb0574d4e4d8a840c71b05e22264ebdc0056433a642c84).'}






################################################## youtube.md ##################################################


# YouTube

>[YouTube Search](https://github.com/joetats/youtube_search) package searches `YouTube` videos avoiding using their heavily rate-limited API.
>
>It uses the form on the `YouTube` homepage and scrapes the resulting page.

This notebook shows how to use a tool to search YouTube.

Adapted from [https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools)


```python
%pip install --upgrade --quiet  youtube_search
```


```python
from langchain_community.tools import YouTubeSearchTool
```


```python
tool = YouTubeSearchTool()
```


```python
tool.run("lex fridman")
```




    "['/watch?v=VcVfceTsD0A&pp=ygUMbGV4IGZyaWVkbWFu', '/watch?v=gPfriiHBBek&pp=ygUMbGV4IGZyaWVkbWFu']"



You can also specify the number of results that are returned


```python
tool.run("lex friedman,5")
```




    "['/watch?v=VcVfceTsD0A&pp=ygUMbGV4IGZyaWVkbWFu', '/watch?v=YVJ8gTnDC4Y&pp=ygUMbGV4IGZyaWVkbWFu', '/watch?v=Udh22kuLebg&pp=ygUMbGV4IGZyaWVkbWFu', '/watch?v=gPfriiHBBek&pp=ygUMbGV4IGZyaWVkbWFu', '/watch?v=L_Guz73e6fw&pp=ygUMbGV4IGZyaWVkbWFu']"




```python

```




################################################## youtube_audio.md ##################################################


# YouTube audio

Building chat or QA applications on YouTube videos is a topic of high interest.

Below we show how to easily go from a `YouTube url` to `audio of the video` to `text` to `chat`!

We wil use the `OpenAIWhisperParser`, which will use the OpenAI Whisper API to transcribe audio to text, 
and the  `OpenAIWhisperParserLocal` for local support and running on private clouds or on premise.

Note: You will need to have an `OPENAI_API_KEY` supplied.


```python
from langchain_community.document_loaders.blob_loaders.youtube_audio import (
    YoutubeAudioLoader,
)
from langchain_community.document_loaders.generic import GenericLoader
from langchain_community.document_loaders.parsers import (
    OpenAIWhisperParser,
    OpenAIWhisperParserLocal,
)
```

We will use `yt_dlp` to download audio for YouTube urls.

We will use `pydub` to split downloaded audio files (such that we adhere to Whisper API's 25MB file size limit).


```python
%pip install --upgrade --quiet  yt_dlp
%pip install --upgrade --quiet  pydub
%pip install --upgrade --quiet  librosa
```

### YouTube url to text

Use `YoutubeAudioLoader` to fetch / download the audio files.

Then, ues `OpenAIWhisperParser()` to transcribe them to text.

Let's take the first lecture of Andrej Karpathy's YouTube course as an example! 


```python
# set a flag to switch between local and remote parsing
# change this to True if you want to use local parsing
local = False
```


```python
# Two Karpathy lecture videos
urls = ["https://youtu.be/kCc8FmEb1nY", "https://youtu.be/VMj-3S1tku0"]

# Directory to save audio files
save_dir = "~/Downloads/YouTube"

# Transcribe the videos to text
if local:
    loader = GenericLoader(
        YoutubeAudioLoader(urls, save_dir), OpenAIWhisperParserLocal()
    )
else:
    loader = GenericLoader(YoutubeAudioLoader(urls, save_dir), OpenAIWhisperParser())
docs = loader.load()
```

    [youtube] Extracting URL: https://youtu.be/kCc8FmEb1nY
    [youtube] kCc8FmEb1nY: Downloading webpage
    [youtube] kCc8FmEb1nY: Downloading android player API JSON
    [info] kCc8FmEb1nY: Downloading 1 format(s): 140
    [dashsegments] Total fragments: 11
    [download] Destination: /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/Let's build GPT： from scratch, in code, spelled out..m4a
    [download] 100% of  107.73MiB in 00:00:18 at 5.92MiB/s                   
    [FixupM4a] Correcting container of "/Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/Let's build GPT： from scratch, in code, spelled out..m4a"
    [ExtractAudio] Not converting audio /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/Let's build GPT： from scratch, in code, spelled out..m4a; file is already in target format m4a
    [youtube] Extracting URL: https://youtu.be/VMj-3S1tku0
    [youtube] VMj-3S1tku0: Downloading webpage
    [youtube] VMj-3S1tku0: Downloading android player API JSON
    [info] VMj-3S1tku0: Downloading 1 format(s): 140
    [download] /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/The spelled-out intro to neural networks and backpropagation： building micrograd.m4a has already been downloaded
    [download] 100% of  134.98MiB
    [ExtractAudio] Not converting audio /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/The spelled-out intro to neural networks and backpropagation： building micrograd.m4a; file is already in target format m4a
    


```python
# Returns a list of Documents, which can be easily viewed or parsed
docs[0].page_content[0:500]
```




    "Hello, my name is Andrej and I've been training deep neural networks for a bit more than a decade. And in this lecture I'd like to show you what neural network training looks like under the hood. So in particular we are going to start with a blank Jupyter notebook and by the end of this lecture we will define and train a neural net and you'll get to see everything that goes on under the hood and exactly sort of how that works on an intuitive level. Now specifically what I would like to do is I w"



### Building a chat app from YouTube video

Given `Documents`, we can easily enable chat / question+answering.


```python
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
```


```python
# Combine doc
combined_docs = [doc.page_content for doc in docs]
text = " ".join(combined_docs)
```


```python
# Split them
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)
splits = text_splitter.split_text(text)
```


```python
# Build an index
embeddings = OpenAIEmbeddings()
vectordb = FAISS.from_texts(splits, embeddings)
```


```python
# Build a QA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-3.5-turbo", temperature=0),
    chain_type="stuff",
    retriever=vectordb.as_retriever(),
)
```


```python
# Ask a question!
query = "Why do we need to zero out the gradient before backprop at each step?"
qa_chain.run(query)
```




    "We need to zero out the gradient before backprop at each step because the backward pass accumulates gradients in the grad attribute of each parameter. If we don't reset the grad to zero before each backward pass, the gradients will accumulate and add up, leading to incorrect updates and slower convergence. By resetting the grad to zero before each backward pass, we ensure that the gradients are calculated correctly and that the optimization process works as intended."




```python
query = "What is the difference between an encoder and decoder?"
qa_chain.run(query)
```




    'In the context of transformers, an encoder is a component that reads in a sequence of input tokens and generates a sequence of hidden representations. On the other hand, a decoder is a component that takes in a sequence of hidden representations and generates a sequence of output tokens. The main difference between the two is that the encoder is used to encode the input sequence into a fixed-length representation, while the decoder is used to decode the fixed-length representation into an output sequence. In machine translation, for example, the encoder reads in the source language sentence and generates a fixed-length representation, which is then used by the decoder to generate the target language sentence.'




```python
query = "For any token, what are x, k, v, and q?"
qa_chain.run(query)
```




    'For any token, x is the input vector that contains the private information of that token, k and q are the key and query vectors respectively, which are produced by forwarding linear modules on x, and v is the vector that is calculated by propagating the same linear module on x again. The key vector represents what the token contains, and the query vector represents what the token is looking for. The vector v is the information that the token will communicate to other tokens if it finds them interesting, and it gets aggregated for the purposes of the self-attention mechanism.'






################################################## youtube_search_analysis_agents.md ##################################################


# Youtube Search Analysis Agent
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MervinPraison/PraisonAI/blob/main/cookbooks/notebooks/youtube_search_analysis_agents.ipynb)

## Dependencies


```python
# Install dependencies without output
%pip install langchain_community > /dev/null
%pip install praisonai[crewai] > /dev/null
%pip install duckduckgo_search > /dev/null
```

## Tools


```python
# ToDo: Youtube Search not shown as action in the output
from langchain_community.tools import YouTubeSearchTool
```

## YAML Prompt


```python
agent_yaml = """
framework: "crewai"
topic: "research about the causes of lung disease"
roles:
  research_analyst:
    role: "Research Analyst"
    backstory: "Experienced in analyzing scientific data related to respiratory health."
    goal: "Analyze data on lung diseases"
    tasks:
      data_analysis:
        description: "Gather and analyze data on the causes and risk factors of lung diseases."
        expected_output: "Report detailing key findings on lung disease causes."
    tools:
      - "YouTubeSearchTool"
  medical_writer:
    role: "Medical Writer"
    backstory: "Skilled in translating complex medical information into accessible content."
    goal: "Compile comprehensive content on lung disease causes"
    tasks:
      content_creation:
        description: "Create detailed content summarizing the research findings on lung disease causes."
        expected_output: "Document outlining various causes and risk factors of lung diseases."
    tools: []
  editor:
    role: "Editor"
    backstory: "Proficient in editing medical content for accuracy and clarity."
    goal: "Review and refine content on lung disease causes"
    tasks:
      content_review:
        description: "Edit and refine the compiled content on lung disease causes for accuracy and coherence."
        expected_output: "Finalized document on lung disease causes ready for dissemination."
    tools: []
dependencies: []
"""
```

## Main


```python
import os
from praisonai import PraisonAI
from google.colab import userdata

# Create a PraisonAI instance with the agent_yaml content
praisonai = PraisonAI(agent_yaml=agent_yaml, tools=[YouTubeSearchTool])

# Add OPENAI_API_KEY Secrets to Google Colab on the Left Hand Side 🔑 or Enter Manually Below
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY') or "ENTER OPENAI_API_KEY HERE"
os.environ["OPENAI_MODEL_NAME"] = "gpt-4o-mini"

# Run PraisonAI
result = praisonai.run()

# Print the result
print(result) # 8/10

```

    [1m[95m [2024-11-02 12:19:37][DEBUG]: == Working Agent: Research Analyst[00m
    [1m[95m [2024-11-02 12:19:37][INFO]: == Starting Task: Gather and analyze data on the causes and risk factors of lung diseases.[00m
    
    
    [1m> Entering new CrewAgentExecutor chain...[0m
    [32;1m[1;3mI now can give a great answer. 
    
    Final Answer: 
    
    **Report on Causes and Risk Factors of Lung Diseases**
    
    **Introduction:**
    Lung diseases encompass a wide range of conditions affecting the lungs and respiratory system, including chronic obstructive pulmonary disease (COPD), asthma, lung cancer, pneumonia, and interstitial lung disease. Understanding the causes and risk factors associated with these diseases is crucial for prevention and management.
    
    **Key Findings:**
    
    1. **Environmental Factors:**
       - **Air Pollution:** Exposure to particulate matter (PM), nitrogen dioxide (NO2), and sulfur dioxide (SO2) is linked to the development and exacerbation of respiratory diseases. Long-term exposure to poor air quality is associated with increased rates of asthma and COPD.
       - **Occupational Hazards:** Certain occupations expose individuals to harmful substances such as asbestos, silica, and coal dust, increasing the risk of lung diseases. Workers in construction, mining, and manufacturing are particularly vulnerable.
       - **Secondhand Smoke:** Non-smokers exposed to tobacco smoke are at a heightened risk of developing lung diseases, including lung cancer and COPD.
    
    2. **Lifestyle Factors:**
       - **Smoking:** The leading cause of lung diseases, smoking is responsible for approximately 85% of lung cancer cases and significantly contributes to the incidence of COPD and chronic bronchitis. The risk is dose-dependent, meaning the more one smokes, the higher the risk.
       - **Physical Inactivity:** A sedentary lifestyle can exacerbate respiratory conditions and reduce lung function. Regular physical activity is associated with better respiratory health outcomes.
    
    3. **Genetic Predisposition:**
       - Genetic factors play a role in the susceptibility to lung diseases. For instance, individuals with a family history of asthma or COPD may be at a higher risk. Genetic mutations, such as those affecting the alpha-1 antitrypsin protein, can lead to conditions like emphysema.
    
    4. **Infectious Agents:**
       - **Viruses and Bacteria:** Respiratory infections, particularly during childhood, can lead to long-term respiratory issues. Viral infections like respiratory syncytial virus (RSV) and bacterial infections like Streptococcus pneumoniae can contribute to the development of chronic respiratory diseases.
       - **Fungal Exposure:** Certain environmental fungi can cause respiratory problems, particularly in immunocompromised individuals. For example, Aspergillus species can lead to allergic bronchopulmonary aspergillosis.
    
    5. **Socioeconomic Factors:**
       - Individuals from lower socioeconomic backgrounds may have limited access to healthcare, leading to delayed diagnosis and treatment of lung diseases. Factors such as poor housing conditions, lack of education, and limited resources for health promotion further exacerbate vulnerability.
    
    6. **Age and Gender:**
       - Lung function naturally declines with age, making older adults more susceptible to respiratory diseases. Additionally, men historically have higher rates of smoking-related lung diseases, although the gap has narrowed as smoking rates among women have increased.
    
    **Conclusion:**
    The causes and risk factors of lung diseases are multifaceted and interrelated. Environmental exposures, lifestyle choices, genetic predisposition, infectious agents, socioeconomic status, and demographic factors all contribute to the burden of lung diseases. Addressing these factors through public health initiatives, smoking cessation programs, and improved air quality regulations is crucial for reducing the incidence and impact of lung diseases. Further research is needed to explore the interplay of these factors and develop targeted interventions.
    
    **Recommendations:**
    - Promote public awareness campaigns focused on smoking cessation and the dangers of secondhand smoke.
    - Implement stricter regulations on air quality and occupational safety standards.
    - Encourage regular health screenings and access to healthcare services, particularly in at-risk populations.
    - Invest in research to better understand the genetic and environmental interactions leading to lung diseases.
    
    This report provides a comprehensive overview of the causes and risk factors associated with lung diseases, offering insights into potential areas for intervention and further research.[0m
    
    [1m> Finished chain.[0m
    [1m[92m [2024-11-02 12:19:52][DEBUG]: == [Research Analyst] Task output: **Report on Causes and Risk Factors of Lung Diseases**
    
    **Introduction:**
    Lung diseases encompass a wide range of conditions affecting the lungs and respiratory system, including chronic obstructive pulmonary disease (COPD), asthma, lung cancer, pneumonia, and interstitial lung disease. Understanding the causes and risk factors associated with these diseases is crucial for prevention and management.
    
    **Key Findings:**
    
    1. **Environmental Factors:**
       - **Air Pollution:** Exposure to particulate matter (PM), nitrogen dioxide (NO2), and sulfur dioxide (SO2) is linked to the development and exacerbation of respiratory diseases. Long-term exposure to poor air quality is associated with increased rates of asthma and COPD.
       - **Occupational Hazards:** Certain occupations expose individuals to harmful substances such as asbestos, silica, and coal dust, increasing the risk of lung diseases. Workers in construction, mining, and manufacturing are particularly vulnerable.
       - **Secondhand Smoke:** Non-smokers exposed to tobacco smoke are at a heightened risk of developing lung diseases, including lung cancer and COPD.
    
    2. **Lifestyle Factors:**
       - **Smoking:** The leading cause of lung diseases, smoking is responsible for approximately 85% of lung cancer cases and significantly contributes to the incidence of COPD and chronic bronchitis. The risk is dose-dependent, meaning the more one smokes, the higher the risk.
       - **Physical Inactivity:** A sedentary lifestyle can exacerbate respiratory conditions and reduce lung function. Regular physical activity is associated with better respiratory health outcomes.
    
    3. **Genetic Predisposition:**
       - Genetic factors play a role in the susceptibility to lung diseases. For instance, individuals with a family history of asthma or COPD may be at a higher risk. Genetic mutations, such as those affecting the alpha-1 antitrypsin protein, can lead to conditions like emphysema.
    
    4. **Infectious Agents:**
       - **Viruses and Bacteria:** Respiratory infections, particularly during childhood, can lead to long-term respiratory issues. Viral infections like respiratory syncytial virus (RSV) and bacterial infections like Streptococcus pneumoniae can contribute to the development of chronic respiratory diseases.
       - **Fungal Exposure:** Certain environmental fungi can cause respiratory problems, particularly in immunocompromised individuals. For example, Aspergillus species can lead to allergic bronchopulmonary aspergillosis.
    
    5. **Socioeconomic Factors:**
       - Individuals from lower socioeconomic backgrounds may have limited access to healthcare, leading to delayed diagnosis and treatment of lung diseases. Factors such as poor housing conditions, lack of education, and limited resources for health promotion further exacerbate vulnerability.
    
    6. **Age and Gender:**
       - Lung function naturally declines with age, making older adults more susceptible to respiratory diseases. Additionally, men historically have higher rates of smoking-related lung diseases, although the gap has narrowed as smoking rates among women have increased.
    
    **Conclusion:**
    The causes and risk factors of lung diseases are multifaceted and interrelated. Environmental exposures, lifestyle choices, genetic predisposition, infectious agents, socioeconomic status, and demographic factors all contribute to the burden of lung diseases. Addressing these factors through public health initiatives, smoking cessation programs, and improved air quality regulations is crucial for reducing the incidence and impact of lung diseases. Further research is needed to explore the interplay of these factors and develop targeted interventions.
    
    **Recommendations:**
    - Promote public awareness campaigns focused on smoking cessation and the dangers of secondhand smoke.
    - Implement stricter regulations on air quality and occupational safety standards.
    - Encourage regular health screenings and access to healthcare services, particularly in at-risk populations.
    - Invest in research to better understand the genetic and environmental interactions leading to lung diseases.
    
    This report provides a comprehensive overview of the causes and risk factors associated with lung diseases, offering insights into potential areas for intervention and further research.
    
    [00m
    [1m[95m [2024-11-02 12:19:52][DEBUG]: == Working Agent: Medical Writer[00m
    [1m[95m [2024-11-02 12:19:52][INFO]: == Starting Task: Create detailed content summarizing the research findings on lung disease causes.[00m
    
    
    [1m> Entering new CrewAgentExecutor chain...[0m
    [32;1m[1;3mI now can give a great answer  
    Final Answer:  
    
    **Report on Causes and Risk Factors of Lung Diseases**
    
    **Introduction:**  
    Lung diseases encompass a wide range of conditions affecting the lungs and respiratory system, including chronic obstructive pulmonary disease (COPD), asthma, lung cancer, pneumonia, and interstitial lung disease. Understanding the causes and risk factors associated with these diseases is crucial for prevention and management.
    
    **Key Findings:**
    
    1. **Environmental Factors:**
       - **Air Pollution:** Exposure to particulate matter (PM), nitrogen dioxide (NO2), and sulfur dioxide (SO2) is linked to the development and exacerbation of respiratory diseases. Long-term exposure to poor air quality is associated with increased rates of asthma and COPD. Studies have shown that urban areas with high traffic emissions often correlate with higher incidences of respiratory illnesses among residents.
       - **Occupational Hazards:** Certain occupations expose individuals to harmful substances such as asbestos, silica, and coal dust, increasing the risk of lung diseases. Workers in construction, mining, and manufacturing are particularly vulnerable. For instance, miners are at risk of pneumoconiosis, a lung disease caused by inhaling coal dust.
       - **Secondhand Smoke:** Non-smokers exposed to tobacco smoke are at a heightened risk of developing lung diseases, including lung cancer and COPD. The World Health Organization reports that secondhand smoke exposure contributes to approximately 1.2 million premature deaths annually.
    
    2. **Lifestyle Factors:**
       - **Smoking:** The leading cause of lung diseases, smoking is responsible for approximately 85% of lung cancer cases and significantly contributes to the incidence of COPD and chronic bronchitis. The risk is dose-dependent, meaning the more one smokes, the higher the risk. Quitting smoking can lead to substantial reductions in risk over time.
       - **Physical Inactivity:** A sedentary lifestyle can exacerbate respiratory conditions and reduce lung function. Regular physical activity is associated with better respiratory health outcomes, helping to maintain optimal lung function and improve overall well-being.
    
    3. **Genetic Predisposition:**
       - Genetic factors play a role in the susceptibility to lung diseases. For instance, individuals with a family history of asthma or COPD may be at a higher risk. Genetic mutations, such as those affecting the alpha-1 antitrypsin protein, can lead to conditions like emphysema. Advances in genetic research are paving the way for personalized medicine and targeted therapies based on individual genetic profiles.
    
    4. **Infectious Agents:**
       - **Viruses and Bacteria:** Respiratory infections, particularly during childhood, can lead to long-term respiratory issues. Viral infections like respiratory syncytial virus (RSV) and bacterial infections like Streptococcus pneumoniae can contribute to the development of chronic respiratory diseases. For example, children with severe RSV infections may be at an increased risk for developing asthma later in life.
       - **Fungal Exposure:** Certain environmental fungi can cause respiratory problems, particularly in immunocompromised individuals. For example, Aspergillus species can lead to allergic bronchopulmonary aspergillosis, a condition characterized by wheezing and coughing.
    
    5. **Socioeconomic Factors:**
       - Individuals from lower socioeconomic backgrounds may have limited access to healthcare, leading to delayed diagnosis and treatment of lung diseases. Factors such as poor housing conditions, lack of education, and limited resources for health promotion further exacerbate vulnerability. Inadequate access to preventive services and early interventions can lead to worse health outcomes and higher healthcare costs for these populations.
    
    6. **Age and Gender:**
       - Lung function naturally declines with age, making older adults more susceptible to respiratory diseases. Additionally, men historically have higher rates of smoking-related lung diseases, although the gap has narrowed as smoking rates among women have increased. Age-related changes in lung structure and function necessitate tailored interventions for older adults at risk of respiratory diseases.
    
    **Conclusion:**  
    The causes and risk factors of lung diseases are multifaceted and interrelated. Environmental exposures, lifestyle choices, genetic predisposition, infectious agents, socioeconomic status, and demographic factors all contribute to the burden of lung diseases. Addressing these factors through public health initiatives, smoking cessation programs, and improved air quality regulations is crucial for reducing the incidence and impact of lung diseases. Further research is needed to explore the interplay of these factors and develop targeted interventions.
    
    **Recommendations:**
    - Promote public awareness campaigns focused on smoking cessation and the dangers of secondhand smoke.
    - Implement stricter regulations on air quality and occupational safety standards.
    - Encourage regular health screenings and access to healthcare services, particularly in at-risk populations.
    - Invest in research to better understand the genetic and environmental interactions leading to lung diseases.
    
    This report provides a comprehensive overview of the causes and risk factors associated with lung diseases, offering insights into potential areas for intervention and further research.[0m
    
    [1m> Finished chain.[0m
    [1m[92m [2024-11-02 12:20:08][DEBUG]: == [Medical Writer] Task output: **Report on Causes and Risk Factors of Lung Diseases**
    
    **Introduction:**  
    Lung diseases encompass a wide range of conditions affecting the lungs and respiratory system, including chronic obstructive pulmonary disease (COPD), asthma, lung cancer, pneumonia, and interstitial lung disease. Understanding the causes and risk factors associated with these diseases is crucial for prevention and management.
    
    **Key Findings:**
    
    1. **Environmental Factors:**
       - **Air Pollution:** Exposure to particulate matter (PM), nitrogen dioxide (NO2), and sulfur dioxide (SO2) is linked to the development and exacerbation of respiratory diseases. Long-term exposure to poor air quality is associated with increased rates of asthma and COPD. Studies have shown that urban areas with high traffic emissions often correlate with higher incidences of respiratory illnesses among residents.
       - **Occupational Hazards:** Certain occupations expose individuals to harmful substances such as asbestos, silica, and coal dust, increasing the risk of lung diseases. Workers in construction, mining, and manufacturing are particularly vulnerable. For instance, miners are at risk of pneumoconiosis, a lung disease caused by inhaling coal dust.
       - **Secondhand Smoke:** Non-smokers exposed to tobacco smoke are at a heightened risk of developing lung diseases, including lung cancer and COPD. The World Health Organization reports that secondhand smoke exposure contributes to approximately 1.2 million premature deaths annually.
    
    2. **Lifestyle Factors:**
       - **Smoking:** The leading cause of lung diseases, smoking is responsible for approximately 85% of lung cancer cases and significantly contributes to the incidence of COPD and chronic bronchitis. The risk is dose-dependent, meaning the more one smokes, the higher the risk. Quitting smoking can lead to substantial reductions in risk over time.
       - **Physical Inactivity:** A sedentary lifestyle can exacerbate respiratory conditions and reduce lung function. Regular physical activity is associated with better respiratory health outcomes, helping to maintain optimal lung function and improve overall well-being.
    
    3. **Genetic Predisposition:**
       - Genetic factors play a role in the susceptibility to lung diseases. For instance, individuals with a family history of asthma or COPD may be at a higher risk. Genetic mutations, such as those affecting the alpha-1 antitrypsin protein, can lead to conditions like emphysema. Advances in genetic research are paving the way for personalized medicine and targeted therapies based on individual genetic profiles.
    
    4. **Infectious Agents:**
       - **Viruses and Bacteria:** Respiratory infections, particularly during childhood, can lead to long-term respiratory issues. Viral infections like respiratory syncytial virus (RSV) and bacterial infections like Streptococcus pneumoniae can contribute to the development of chronic respiratory diseases. For example, children with severe RSV infections may be at an increased risk for developing asthma later in life.
       - **Fungal Exposure:** Certain environmental fungi can cause respiratory problems, particularly in immunocompromised individuals. For example, Aspergillus species can lead to allergic bronchopulmonary aspergillosis, a condition characterized by wheezing and coughing.
    
    5. **Socioeconomic Factors:**
       - Individuals from lower socioeconomic backgrounds may have limited access to healthcare, leading to delayed diagnosis and treatment of lung diseases. Factors such as poor housing conditions, lack of education, and limited resources for health promotion further exacerbate vulnerability. Inadequate access to preventive services and early interventions can lead to worse health outcomes and higher healthcare costs for these populations.
    
    6. **Age and Gender:**
       - Lung function naturally declines with age, making older adults more susceptible to respiratory diseases. Additionally, men historically have higher rates of smoking-related lung diseases, although the gap has narrowed as smoking rates among women have increased. Age-related changes in lung structure and function necessitate tailored interventions for older adults at risk of respiratory diseases.
    
    **Conclusion:**  
    The causes and risk factors of lung diseases are multifaceted and interrelated. Environmental exposures, lifestyle choices, genetic predisposition, infectious agents, socioeconomic status, and demographic factors all contribute to the burden of lung diseases. Addressing these factors through public health initiatives, smoking cessation programs, and improved air quality regulations is crucial for reducing the incidence and impact of lung diseases. Further research is needed to explore the interplay of these factors and develop targeted interventions.
    
    **Recommendations:**
    - Promote public awareness campaigns focused on smoking cessation and the dangers of secondhand smoke.
    - Implement stricter regulations on air quality and occupational safety standards.
    - Encourage regular health screenings and access to healthcare services, particularly in at-risk populations.
    - Invest in research to better understand the genetic and environmental interactions leading to lung diseases.
    
    This report provides a comprehensive overview of the causes and risk factors associated with lung diseases, offering insights into potential areas for intervention and further research.
    
    [00m
    [1m[95m [2024-11-02 12:20:08][DEBUG]: == Working Agent: Editor[00m
    [1m[95m [2024-11-02 12:20:08][INFO]: == Starting Task: Edit and refine the compiled content on lung disease causes for accuracy and coherence.[00m
    
    
    [1m> Entering new CrewAgentExecutor chain...[0m
    [32;1m[1;3mI now can give a great answer  
    Final Answer: 
    
    **Report on Causes and Risk Factors of Lung Diseases**
    
    **Introduction:**  
    Lung diseases encompass a wide range of conditions affecting the lungs and respiratory system, including chronic obstructive pulmonary disease (COPD), asthma, lung cancer, pneumonia, and interstitial lung disease. Understanding the causes and risk factors associated with these diseases is crucial for prevention and management.
    
    **Key Findings:**
    
    1. **Environmental Factors:**
       - **Air Pollution:** Exposure to particulate matter (PM), nitrogen dioxide (NO2), and sulfur dioxide (SO2) is linked to the development and exacerbation of respiratory diseases. Long-term exposure to poor air quality is associated with increased rates of asthma and COPD. Studies have shown that urban areas with high traffic emissions often correlate with higher incidences of respiratory illnesses among residents.
       - **Occupational Hazards:** Certain occupations expose individuals to harmful substances such as asbestos, silica, and coal dust, increasing the risk of lung diseases. Workers in construction, mining, and manufacturing are particularly vulnerable. For instance, miners are at risk of pneumoconiosis, a lung disease caused by inhaling coal dust.
       - **Secondhand Smoke:** Non-smokers exposed to tobacco smoke are at a heightened risk of developing lung diseases, including lung cancer and COPD. The World Health Organization reports that secondhand smoke exposure contributes to approximately 1.2 million premature deaths annually.
    
    2. **Lifestyle Factors:**
       - **Smoking:** The leading cause of lung diseases, smoking is responsible for approximately 85% of lung cancer cases and significantly contributes to the incidence of COPD and chronic bronchitis. The risk is dose-dependent, meaning the more one smokes, the higher the risk. Quitting smoking can lead to substantial reductions in risk over time, improving lung health and overall quality of life.
       - **Physical Inactivity:** A sedentary lifestyle can exacerbate respiratory conditions and reduce lung function. Regular physical activity is associated with better respiratory health outcomes, helping to maintain optimal lung function and improve overall well-being.
    
    3. **Genetic Predisposition:**
       - Genetic factors play a role in the susceptibility to lung diseases. For instance, individuals with a family history of asthma or COPD may be at a higher risk. Genetic mutations, such as those affecting the alpha-1 antitrypsin protein, can lead to conditions like emphysema. Advances in genetic research are paving the way for personalized medicine and targeted therapies based on individual genetic profiles.
    
    4. **Infectious Agents:**
       - **Viruses and Bacteria:** Respiratory infections, particularly during childhood, can lead to long-term respiratory issues. Viral infections like respiratory syncytial virus (RSV) and bacterial infections like Streptococcus pneumoniae can contribute to the development of chronic respiratory diseases. For example, children with severe RSV infections may be at an increased risk for developing asthma later in life.
       - **Fungal Exposure:** Certain environmental fungi can cause respiratory problems, particularly in immunocompromised individuals. For example, Aspergillus species can lead to allergic bronchopulmonary aspergillosis, a condition characterized by wheezing and coughing.
    
    5. **Socioeconomic Factors:**
       - Individuals from lower socioeconomic backgrounds may have limited access to healthcare, leading to delayed diagnosis and treatment of lung diseases. Factors such as poor housing conditions, lack of education, and limited resources for health promotion further exacerbate vulnerability. Inadequate access to preventive services and early interventions can lead to worse health outcomes and higher healthcare costs for these populations.
    
    6. **Age and Gender:**
       - Lung function naturally declines with age, making older adults more susceptible to respiratory diseases. Additionally, men historically have higher rates of smoking-related lung diseases, although the gap has narrowed as smoking rates among women have increased. Age-related changes in lung structure and function necessitate tailored interventions for older adults at risk of respiratory diseases.
    
    **Conclusion:**  
    The causes and risk factors of lung diseases are multifaceted and interrelated. Environmental exposures, lifestyle choices, genetic predisposition, infectious agents, socioeconomic status, and demographic factors all contribute to the burden of lung diseases. Addressing these factors through public health initiatives, smoking cessation programs, and improved air quality regulations is crucial for reducing the incidence and impact of lung diseases. Further research is needed to explore the interplay of these factors and develop targeted interventions.
    
    **Recommendations:**
    - Promote public awareness campaigns focused on smoking cessation and the dangers of secondhand smoke.
    - Implement stricter regulations on air quality and occupational safety standards.
    - Encourage regular health screenings and access to healthcare services, particularly in at-risk populations.
    - Invest in research to better understand the genetic and environmental interactions leading to lung diseases.
    
    This report provides a comprehensive overview of the causes and risk factors associated with lung diseases, offering insights into potential areas for intervention and further research.[0m
    
    [1m> Finished chain.[0m
    [1m[92m [2024-11-02 12:20:21][DEBUG]: == [Editor] Task output: **Report on Causes and Risk Factors of Lung Diseases**
    
    **Introduction:**  
    Lung diseases encompass a wide range of conditions affecting the lungs and respiratory system, including chronic obstructive pulmonary disease (COPD), asthma, lung cancer, pneumonia, and interstitial lung disease. Understanding the causes and risk factors associated with these diseases is crucial for prevention and management.
    
    **Key Findings:**
    
    1. **Environmental Factors:**
       - **Air Pollution:** Exposure to particulate matter (PM), nitrogen dioxide (NO2), and sulfur dioxide (SO2) is linked to the development and exacerbation of respiratory diseases. Long-term exposure to poor air quality is associated with increased rates of asthma and COPD. Studies have shown that urban areas with high traffic emissions often correlate with higher incidences of respiratory illnesses among residents.
       - **Occupational Hazards:** Certain occupations expose individuals to harmful substances such as asbestos, silica, and coal dust, increasing the risk of lung diseases. Workers in construction, mining, and manufacturing are particularly vulnerable. For instance, miners are at risk of pneumoconiosis, a lung disease caused by inhaling coal dust.
       - **Secondhand Smoke:** Non-smokers exposed to tobacco smoke are at a heightened risk of developing lung diseases, including lung cancer and COPD. The World Health Organization reports that secondhand smoke exposure contributes to approximately 1.2 million premature deaths annually.
    
    2. **Lifestyle Factors:**
       - **Smoking:** The leading cause of lung diseases, smoking is responsible for approximately 85% of lung cancer cases and significantly contributes to the incidence of COPD and chronic bronchitis. The risk is dose-dependent, meaning the more one smokes, the higher the risk. Quitting smoking can lead to substantial reductions in risk over time, improving lung health and overall quality of life.
       - **Physical Inactivity:** A sedentary lifestyle can exacerbate respiratory conditions and reduce lung function. Regular physical activity is associated with better respiratory health outcomes, helping to maintain optimal lung function and improve overall well-being.
    
    3. **Genetic Predisposition:**
       - Genetic factors play a role in the susceptibility to lung diseases. For instance, individuals with a family history of asthma or COPD may be at a higher risk. Genetic mutations, such as those affecting the alpha-1 antitrypsin protein, can lead to conditions like emphysema. Advances in genetic research are paving the way for personalized medicine and targeted therapies based on individual genetic profiles.
    
    4. **Infectious Agents:**
       - **Viruses and Bacteria:** Respiratory infections, particularly during childhood, can lead to long-term respiratory issues. Viral infections like respiratory syncytial virus (RSV) and bacterial infections like Streptococcus pneumoniae can contribute to the development of chronic respiratory diseases. For example, children with severe RSV infections may be at an increased risk for developing asthma later in life.
       - **Fungal Exposure:** Certain environmental fungi can cause respiratory problems, particularly in immunocompromised individuals. For example, Aspergillus species can lead to allergic bronchopulmonary aspergillosis, a condition characterized by wheezing and coughing.
    
    5. **Socioeconomic Factors:**
       - Individuals from lower socioeconomic backgrounds may have limited access to healthcare, leading to delayed diagnosis and treatment of lung diseases. Factors such as poor housing conditions, lack of education, and limited resources for health promotion further exacerbate vulnerability. Inadequate access to preventive services and early interventions can lead to worse health outcomes and higher healthcare costs for these populations.
    
    6. **Age and Gender:**
       - Lung function naturally declines with age, making older adults more susceptible to respiratory diseases. Additionally, men historically have higher rates of smoking-related lung diseases, although the gap has narrowed as smoking rates among women have increased. Age-related changes in lung structure and function necessitate tailored interventions for older adults at risk of respiratory diseases.
    
    **Conclusion:**  
    The causes and risk factors of lung diseases are multifaceted and interrelated. Environmental exposures, lifestyle choices, genetic predisposition, infectious agents, socioeconomic status, and demographic factors all contribute to the burden of lung diseases. Addressing these factors through public health initiatives, smoking cessation programs, and improved air quality regulations is crucial for reducing the incidence and impact of lung diseases. Further research is needed to explore the interplay of these factors and develop targeted interventions.
    
    **Recommendations:**
    - Promote public awareness campaigns focused on smoking cessation and the dangers of secondhand smoke.
    - Implement stricter regulations on air quality and occupational safety standards.
    - Encourage regular health screenings and access to healthcare services, particularly in at-risk populations.
    - Invest in research to better understand the genetic and environmental interactions leading to lung diseases.
    
    This report provides a comprehensive overview of the causes and risk factors associated with lung diseases, offering insights into potential areas for intervention and further research.
    
    [00m
    


<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">### Task Output ###
**Report on Causes and Risk Factors of Lung Diseases**

**Introduction:**  
Lung diseases encompass a wide range of conditions affecting the lungs and respiratory system, including chronic 
obstructive pulmonary disease <span style="font-weight: bold">(</span>COPD<span style="font-weight: bold">)</span>, asthma, lung cancer, pneumonia, and interstitial lung disease. Understanding 
the causes and risk factors associated with these diseases is crucial for prevention and management.

**Key Findings:**

<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>. **Environmental Factors:**
   - **Air Pollution:** Exposure to particulate matter <span style="font-weight: bold">(</span>PM<span style="font-weight: bold">)</span>, nitrogen dioxide <span style="font-weight: bold">(</span>NO2<span style="font-weight: bold">)</span>, and sulfur dioxide <span style="font-weight: bold">(</span>SO2<span style="font-weight: bold">)</span> is 
linked to the development and exacerbation of respiratory diseases. Long-term exposure to poor air quality is 
associated with increased rates of asthma and COPD. Studies have shown that urban areas with high traffic emissions
often correlate with higher incidences of respiratory illnesses among residents.
   - **Occupational Hazards:** Certain occupations expose individuals to harmful substances such as asbestos, 
silica, and coal dust, increasing the risk of lung diseases. Workers in construction, mining, and manufacturing are
particularly vulnerable. For instance, miners are at risk of pneumoconiosis, a lung disease caused by inhaling coal
dust.
   - **Secondhand Smoke:** Non-smokers exposed to tobacco smoke are at a heightened risk of developing lung 
diseases, including lung cancer and COPD. The World Health Organization reports that secondhand smoke exposure 
contributes to approximately <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.2</span> million premature deaths annually.

<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>. **Lifestyle Factors:**
   - **Smoking:** The leading cause of lung diseases, smoking is responsible for approximately <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">85</span>% of lung cancer 
cases and significantly contributes to the incidence of COPD and chronic bronchitis. The risk is dose-dependent, 
meaning the more one smokes, the higher the risk. Quitting smoking can lead to substantial reductions in risk over 
time, improving lung health and overall quality of life.
   - **Physical Inactivity:** A sedentary lifestyle can exacerbate respiratory conditions and reduce lung function.
Regular physical activity is associated with better respiratory health outcomes, helping to maintain optimal lung 
function and improve overall well-being.

<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>. **Genetic Predisposition:**
   - Genetic factors play a role in the susceptibility to lung diseases. For instance, individuals with a family 
history of asthma or COPD may be at a higher risk. Genetic mutations, such as those affecting the alpha-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span> 
antitrypsin protein, can lead to conditions like emphysema. Advances in genetic research are paving the way for 
personalized medicine and targeted therapies based on individual genetic profiles.

<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>. **Infectious Agents:**
   - **Viruses and Bacteria:** Respiratory infections, particularly during childhood, can lead to long-term 
respiratory issues. Viral infections like respiratory syncytial virus <span style="font-weight: bold">(</span>RSV<span style="font-weight: bold">)</span> and bacterial infections like 
Streptococcus pneumoniae can contribute to the development of chronic respiratory diseases. For example, children 
with severe RSV infections may be at an increased risk for developing asthma later in life.
   - **Fungal Exposure:** Certain environmental fungi can cause respiratory problems, particularly in 
immunocompromised individuals. For example, Aspergillus species can lead to allergic bronchopulmonary 
aspergillosis, a condition characterized by wheezing and coughing.

<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>. **Socioeconomic Factors:**
   - Individuals from lower socioeconomic backgrounds may have limited access to healthcare, leading to delayed 
diagnosis and treatment of lung diseases. Factors such as poor housing conditions, lack of education, and limited 
resources for health promotion further exacerbate vulnerability. Inadequate access to preventive services and early
interventions can lead to worse health outcomes and higher healthcare costs for these populations.

<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>. **Age and Gender:**
   - Lung function naturally declines with age, making older adults more susceptible to respiratory diseases. 
Additionally, men historically have higher rates of smoking-related lung diseases, although the gap has narrowed as
smoking rates among women have increased. Age-related changes in lung structure and function necessitate tailored 
interventions for older adults at risk of respiratory diseases.

**Conclusion:**  
The causes and risk factors of lung diseases are multifaceted and interrelated. Environmental exposures, lifestyle 
choices, genetic predisposition, infectious agents, socioeconomic status, and demographic factors all contribute to
the burden of lung diseases. Addressing these factors through public health initiatives, smoking cessation 
programs, and improved air quality regulations is crucial for reducing the incidence and impact of lung diseases. 
Further research is needed to explore the interplay of these factors and develop targeted interventions.

**Recommendations:**
- Promote public awareness campaigns focused on smoking cessation and the dangers of secondhand smoke.
- Implement stricter regulations on air quality and occupational safety standards.
- Encourage regular health screenings and access to healthcare services, particularly in at-risk populations.
- Invest in research to better understand the genetic and environmental interactions leading to lung diseases.

This report provides a comprehensive overview of the causes and risk factors associated with lung diseases, 
offering insights into potential areas for intervention and further research.
</pre>



    None
    




################################################## youtube_transcript.md ##################################################


# YouTube transcripts

>[YouTube](https://www.youtube.com/) is an online video sharing and social media platform created by Google.

This notebook covers how to load documents from `YouTube transcripts`.


```python
from langchain_community.document_loaders import YoutubeLoader
```


```python
%pip install --upgrade --quiet  youtube-transcript-api
```


```python
loader = YoutubeLoader.from_youtube_url(
    "https://www.youtube.com/watch?v=QsYGlZkevEg", add_video_info=False
)
```


```python
loader.load()
```

### Add video info


```python
%pip install --upgrade --quiet  pytube
```


```python
loader = YoutubeLoader.from_youtube_url(
    "https://www.youtube.com/watch?v=QsYGlZkevEg", add_video_info=True
)
loader.load()
```

### Add language preferences

Language param : It's a list of language codes in a descending priority, `en` by default.

translation param : It's a translate preference, you can translate available transcript to your preferred language.


```python
loader = YoutubeLoader.from_youtube_url(
    "https://www.youtube.com/watch?v=QsYGlZkevEg",
    add_video_info=True,
    language=["en", "id"],
    translation="en",
)
loader.load()
```

### Get transcripts as timestamped chunks

Get one or more `Document` objects, each containing a chunk of the video transcript.  The length of the chunks, in seconds, may be specified.  Each chunk's metadata includes a URL of the video on YouTube, which will start the video at the beginning of the specific chunk.

`transcript_format` param:  One of the `langchain_community.document_loaders.youtube.TranscriptFormat` values.  In this case, `TranscriptFormat.CHUNKS`.

`chunk_size_seconds` param:  An integer number of video seconds to be represented by each chunk of transcript data.  Default is 120 seconds.


```python
from langchain_community.document_loaders.youtube import TranscriptFormat

loader = YoutubeLoader.from_youtube_url(
    "https://www.youtube.com/watch?v=TKCMw0utiak",
    add_video_info=True,
    transcript_format=TranscriptFormat.CHUNKS,
    chunk_size_seconds=30,
)
print("\n\n".join(map(repr, loader.load())))
```

## YouTube loader from Google Cloud

### Prerequisites

1. Create a Google Cloud project or use an existing project
1. Enable the [Youtube Api](https://console.cloud.google.com/apis/enableflow?apiid=youtube.googleapis.com&project=sixth-grammar-344520)
1. [Authorize credentials for desktop app](https://developers.google.com/drive/api/quickstart/python#authorize_credentials_for_a_desktop_application)
1. `pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib youtube-transcript-api`

### 🧑 Instructions for ingesting your Google Docs data
By default, the `GoogleDriveLoader` expects the `credentials.json` file to be `~/.credentials/credentials.json`, but this is configurable using the `credentials_file` keyword argument. Same thing with `token.json`. Note that `token.json` will be created automatically the first time you use the loader.

`GoogleApiYoutubeLoader` can load from a list of Google Docs document ids or a folder id. You can obtain your folder and document id from the URL:
Note depending on your set up, the `service_account_path` needs to be set up. See [here](https://developers.google.com/drive/api/v3/quickstart/python) for more details.


```python
# Init the GoogleApiClient
from pathlib import Path

from langchain_community.document_loaders import GoogleApiClient, GoogleApiYoutubeLoader

google_api_client = GoogleApiClient(credentials_path=Path("your_path_creds.json"))


# Use a Channel
youtube_loader_channel = GoogleApiYoutubeLoader(
    google_api_client=google_api_client,
    channel_name="Reducible",
    captions_language="en",
)

# Use Youtube Ids

youtube_loader_ids = GoogleApiYoutubeLoader(
    google_api_client=google_api_client, video_ids=["TrdevFK_am4"], add_video_info=True
)

# returns a list of Documents
youtube_loader_channel.load()
```




################################################## youtube_video_analysis.md ##################################################


```
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# YouTube Video Analysis with Gemini

<table align="left">
  <td style="text-align: center">
    <a href="https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/video-analysis/youtube_video_analysis.ipynb">
      <img width="32px" src="https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg" alt="Google Colaboratory logo"><br> Open in Colab
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fvideo-analysis%2Fyoutube_video_analysis.ipynb">
      <img width="32px" src="https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN" alt="Google Cloud Colab Enterprise logo"><br> Open in Colab Enterprise
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/video-analysis/youtube_video_analysis.ipynb">
      <img src="https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg" alt="Vertex AI logo"><br> Open in Vertex AI Workbench
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/video-analysis/youtube_video_analysis.ipynb">
      <img width="32px" src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg" alt="GitHub logo"><br> View on GitHub
    </a>
  </td>
</table>

| | |
|-|-|
| Author(s) | [Alok Pattani](https://github.com/alokpattani/) |

## Overview

In this notebook, you'll explore how to do direct analysis of publicly available [YouTube](https://www.youtube.com/) videos with Gemini.

You will complete the following tasks:
- Summarizing a single YouTube video using Gemini 1.5 Flash
- Extracting a specific set of structured outputs from a longer YouTube video using Gemini 1.5 Pro and controlled generation
- Creating insights from analyzing multiple YouTube videos together using asynchronous generation with Gemini

## Get started

### Install Vertex AI SDK and other required packages



```
%pip install --upgrade --user --quiet google-cloud-aiplatform itables
```

### Restart runtime

To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.

The restart might take a minute or longer. After it's restarted, continue to the next step.


```
import IPython

app = IPython.Application.instance()
app.kernel.do_shutdown(True)
```

<div class="alert alert-block alert-warning">
<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>
</div>


### Authenticate your notebook environment (Colab only)

If you're running this notebook on Google Colab, run the cell below to authenticate your environment.


```
import sys

if "google.colab" in sys.modules:
    from google.colab import auth

    auth.authenticate_user()
```

### Set Google Cloud project information and initialize Vertex AI SDK

To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).

Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).


```
# Use the environment variable if the user doesn't provide Project ID.
import os

import vertexai

PROJECT_ID = "[your-project-id]"  # @param {type:"string", isTemplate: true}
if PROJECT_ID == "[your-project-id]":
    PROJECT_ID = str(os.environ.get("GOOGLE_CLOUD_PROJECT"))

LOCATION = os.environ.get("GOOGLE_CLOUD_REGION", "us-central1")

vertexai.init(project=PROJECT_ID, location=LOCATION)
```

## Set up libraries, options, and models

### Import libraries


```
import json

from IPython.display import HTML, Markdown, display
from itables import show
import itables.options as itable_opts
import pandas as pd
from tenacity import retry, stop_after_attempt, wait_random_exponential
from vertexai.generative_models import GenerationConfig, GenerativeModel, Part
```

### Configure some notebook options


```
# Configure some options related to interactive tables
itable_opts.maxBytes = 1e9
itable_opts.maxColumns = 50

itable_opts.order = []
itable_opts.column_filters = "header"
```

### Load models


```
# Set Gemini Flash and Pro models to be used in this notebook
GEMINI_FLASH_MODEL_ID = "gemini-1.5-flash"
GEMINI_PRO_MODEL_ID = "gemini-1.5-pro"

gemini_flash_model = GenerativeModel(GEMINI_FLASH_MODEL_ID)
gemini_pro_model = GenerativeModel(GEMINI_PRO_MODEL_ID)
```

## Summarize a YouTube video

Provide a link to a public YouTube video that you'd like to summarize. Ensure that the video is less than an hour long (if using Gemini 1.5 Flash, as is shown below; can try up to a 2-hour video with Gemini 1.5 Pro) to make sure it fits in the context window.

The default content to be summarized is [this 6.5-minute video showing how Major League Baseball (MLB) analyzes data using Google Cloud](https://www.youtube.com/watch?v=O_W_VGUeHVI).


```
# Provide link to a public YouTube video to summarize
YOUTUBE_VIDEO_URL = (
    "https://www.youtube.com/watch?v=O_W_VGUeHVI"  # @param {type:"string"}
)

youtube_video_embed_url = YOUTUBE_VIDEO_URL.replace("/watch?v=", "/embed/")

# Create HTML code to directly embed video
youtube_video_embed_html_code = f"""
<iframe width="560" height="315" src="{youtube_video_embed_url}"
title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; 
clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
"""

# Display embedded YouTube video
display(HTML(youtube_video_embed_html_code))
```



<iframe width="560" height="315" src="https://www.youtube.com/embed/O_W_VGUeHVI"
title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; 
clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>




```
# Call Gemini API with prompt to summarize video
video_summary_prompt = "Summarize this video."

video_summary_response = gemini_flash_model.generate_content(
    [video_summary_prompt, Part.from_uri(mime_type="video/webm", uri=YOUTUBE_VIDEO_URL)]
)

# Display results
display(Markdown(video_summary_response.text))
```


This Google Cloud promotional video explores how Major League Baseball (MLB) uses data analytics to enhance the fan experience.  Priyanka Vergadia, Lead Developer Advocate at Google, interviews several MLB representatives to discuss the process. 

The MLB collects 25 million unique data points per game using Hawk-Eye cameras. This data is then processed and stored in Google Cloud using Anthos, Kubernetes Engine, Bigtable, and other technologies.  This information powers MLB tools like MLB Film Room, and makes data accessible to fans and analysts.  

Rob Engel, Senior Director of Software Engineering at MLB, explains the technology that enables real-time data transfer. He discusses the use of Anthos, Kubernetes Engine, and Cloud SQL to process and store the data.

John Kraizt, Director, Baseball Systems at Arizona Diamondbacks, explains how MLB teams use the data. Finally, Sarah Langs, Reporter/Researcher at MLB Advanced Media, describes how analysts use websites such as Baseball Savant and Statcast to translate the data for fan consumption.


## Extract structured output from a YouTube video

Next, we'll show how to extract structured outputs using [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output), in this case from a video that covers multiple topics.

We're going to see how Gemini Pro's industry-leading 2 million token context window can help analyze [the full opening keynote](https://www.youtube.com/watch?v=V6DJYGn2SFk) from our Next conference back in April - all 1 hour and 41 minutes of it!


```
# Link to full Cloud Next '24 Opening Keynote video
cloud_next_keynote_video_url = "https://www.youtube.com/watch?v=V6DJYGn2SFk"

# Uncomment line below to replace with 14-min keynote summary video instead (faster)
# cloud_next_keynote_video_url = "https://www.youtube.com/watch?v=M-CzbTUVykg"

cloud_next_keynote_video_embed_url = cloud_next_keynote_video_url.replace(
    "/watch?v=", "/embed/"
)

# Create HTML code to directly embed video
cloud_next_keynote_youtube_video_embed_html_code = f"""
<iframe width="560" height="315" src="{cloud_next_keynote_video_embed_url}"
title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; 
clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
"""

# Display embedded YouTube video
display(HTML(cloud_next_keynote_youtube_video_embed_html_code))
```



<iframe width="560" height="315" src="https://www.youtube.com/embed/V6DJYGn2SFk"
title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; 
clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>



Below is a prompt to extract the biggest product announcements that were made during this keynote. We use the response schema to show that we want valid JSON output in a particular form, including a constraint specifying that the "product status" field should be either GA, Preview, or Coming Soon.

The following cell may take several minutes to run, as Gemini 1.5 Pro is analyzing all 101 minutes of the video and audio to produce comprehensive results.


```
# Set up pieces (prompt, response schema, config) and run video extraction

video_extraction_prompt = (
    "Provide a summary of the biggest product announcements "
    "that were made in this Google Cloud Next keynote video including:\n"
    "  - name\n"
    '  - product status: "GA" (Generally Available), "Preview", or "Coming Soon"\n'
    "  - key quote from the presenter about the product, 20 words or fewer per product\n\n"
    "Make sure to look through and listen to the whole video, start to finish, to find "
    "the top product announcements. Only reference information in the video itself in "
    "your response."
)

video_extraction_response_schema = {
    "type": "ARRAY",
    "items": {
        "type": "OBJECT",
        "properties": {
            "name": {"type": "STRING"},
            "product_status": {
                "type": "STRING",
                "enum": ["GA", "Preview", "Coming Soon"],
            },
            "quote_from_presenter": {"type": "STRING"},
        },
    },
}

video_extraction_json_generation_config = GenerationConfig(
    temperature=0.0,
    max_output_tokens=8192,
    response_mime_type="application/json",
    response_schema=video_extraction_response_schema,
)

video_extraction_response = gemini_pro_model.generate_content(
    [
        video_extraction_prompt,
        Part.from_uri(mime_type="video/webm", uri=cloud_next_keynote_video_url),
    ],
    generation_config=video_extraction_json_generation_config,
)

video_extraction_response_text = video_extraction_response.text

print(video_extraction_response_text)
```

    [{"name": "Gemini 1.5 Pro", "product_status": "Preview", "quote_from_presenter": "Gemini 1.5 Pro shows dramatically enhanced performance and includes a breakthrough in long context understanding."}, {"name": "A3 Mega VMs", "product_status": "GA", "quote_from_presenter": "A3 Mega VMs powered by Nvidia H100 Tensor Core GPUs with twice the network bandwidth vs. A3 instances."}, {"name": "TPU v5p", "product_status": "GA", "quote_from_presenter": "Our latest generation TPU pod consists of 8,960 chips interconnected to support the largest scale ML training and serving."}, {"name": "NVIDIA GB200 NVL72", "product_status": "Coming Soon", "quote_from_presenter": "Nvidia's newest Grace Blackwell generation of GPUs coming to Google Cloud early in 2025."}, {"name": "Google Axion Processors", "product_status": "Coming Soon", "quote_from_presenter": "Google's first custom Arm-based CPU designed for the data center will be available in preview later this year."}, {"name": "Cloud Storage Fuse Caching", "product_status": "GA", "quote_from_presenter": "Cloud Storage Fuse Caching is generally available."}, {"name": "Parallelstore Caching", "product_status": "Preview", "quote_from_presenter": "Parallelstore Caching is in preview."}, {"name": "Hyperdisk ML", "product_status": "Preview", "quote_from_presenter": "Hyperdisk ML is our next generation block storage service optimized for AI inference and serving workloads."}, {"name": "Dynamic Workload Scheduler", "product_status": "GA", "quote_from_presenter": "Dynamic Workload Scheduler now has two new options: Calendar Mode for start time assurance and Flex Start for optimized economics."}, {"name": "Gemini in Threat Intelligence", "product_status": "Preview", "quote_from_presenter": "Gemini in Threat Intelligence is in preview."}, {"name": "Gemini in Security Command Center", "product_status": "Preview", "quote_from_presenter": "Gemini in Security Command Center is in preview."}, {"name": "Google Vids", "product_status": "GA", "quote_from_presenter": "Google Vids is generally available."}, {"name": "Text-to-Live Image", "product_status": "Preview", "quote_from_presenter": "Text-to-Live Image is in preview."}, {"name": "Digital Watermarking", "product_status": "GA", "quote_from_presenter": "Digital Watermarking is generally available."}, {"name": "New Editing Modes in Imagen 2.0", "product_status": "GA", "quote_from_presenter": "New Editing Modes in Imagen 2.0 are generally available."}, {"name": "Gemini in BigQuery", "product_status": "Preview", "quote_from_presenter": "Gemini in BigQuery simplifies data preparation using AI."}, {"name": "Vector Indexing in BigQuery and AlloyDB", "product_status": "Preview", "quote_from_presenter": "Vector Indexing in BigQuery and AlloyDB allows you to query the right data in analytical and operational systems."}, {"name": "BigQuery Data Canvas", "product_status": "Preview", "quote_from_presenter": "BigQuery Data Canvas provides a notebook-like experience with natural language and embedded visualizations."}, {"name": "Gemini Code Assist", "product_status": "Preview", "quote_from_presenter": "Gemini Code Assist is in preview."}, {"name": "Vertex AI Agent Builder", "product_status": "Preview", "quote_from_presenter": "Vertex AI Agent Builder lets you create customer agents that are amazingly powerful in just three key steps."}] 
    


```
# Convert structured output from response to data frame for display and/or further analysis
video_extraction_response_df = pd.DataFrame(json.loads(video_extraction_response_text))

show(video_extraction_response_df)
```


<table id="itables_179fda97_ddbb_492e_a6b1_cabfce8830db" class="display nowrap" data-quarto-disable-processing="true" style="table-layout:auto;width:auto;margin:auto;caption-side:bottom">
<thead><th>name</th><th>product_status</th><th>quote_from_presenter</th></thead><tbody><tr>
<td style="vertical-align:middle; text-align:left">
<div style="float:left; margin-right: 10px;">
<a href=https://mwouts.github.io/itables/><svg class="main-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
width="64" viewBox="0 0 500 400" style="font-family: 'Droid Sans', sans-serif;">
    <g style="fill:#d9d7fc">
        <path d="M100,400H500V357H100Z" />
        <path d="M100,300H400V257H100Z" />
        <path d="M0,200H400V157H0Z" />
        <path d="M100,100H500V57H100Z" />
        <path d="M100,350H500V307H100Z" />
        <path d="M100,250H400V207H100Z" />
        <path d="M0,150H400V107H0Z" />
        <path d="M100,50H500V7H100Z" />
    </g>
    <g style="fill:#1a1366;stroke:#1a1366;">
   <rect x="100" y="7" width="400" height="43">
    <animate
      attributeName="width"
      values="0;400;0"
      dur="5s"
      repeatCount="indefinite" />
      <animate
      attributeName="x"
      values="100;100;500"
      dur="5s"
      repeatCount="indefinite" />
  </rect>
        <rect x="0" y="107" width="400" height="43">
    <animate
      attributeName="width"
      values="0;400;0"
      dur="3.5s"
      repeatCount="indefinite" />
    <animate
      attributeName="x"
      values="0;0;400"
      dur="3.5s"
      repeatCount="indefinite" />
  </rect>
        <rect x="100" y="207" width="300" height="43">
    <animate
      attributeName="width"
      values="0;300;0"
      dur="3s"
      repeatCount="indefinite" />
    <animate
      attributeName="x"
      values="100;100;400"
      dur="3s"
      repeatCount="indefinite" />
  </rect>
        <rect x="100" y="307" width="400" height="43">
    <animate
      attributeName="width"
      values="0;400;0"
      dur="4s"
      repeatCount="indefinite" />
      <animate
      attributeName="x"
      values="100;100;500"
      dur="4s"
      repeatCount="indefinite" />
  </rect>
        <g style="fill:transparent;stroke-width:8; stroke-linejoin:round" rx="5">
            <g transform="translate(45 50) rotate(-45)">
                <circle r="33" cx="0" cy="0" />
                <rect x="-8" y="32" width="16" height="30" />
            </g>

            <g transform="translate(450 152)">
                <polyline points="-15,-20 -35,-20 -35,40 25,40 25,20" />
                <rect x="-15" y="-40" width="60" height="60" />
            </g>

            <g transform="translate(50 352)">
                <polygon points="-35,-5 0,-40 35,-5" />
                <polygon points="-35,10 0,45 35,10" />
            </g>

            <g transform="translate(75 250)">
                <polyline points="-30,30 -60,0 -30,-30" />
                <polyline points="0,30 -30,0 0,-30" />
            </g>

            <g transform="translate(425 250) rotate(180)">
                <polyline points="-30,30 -60,0 -30,-30" />
                <polyline points="0,30 -30,0 0,-30" />
            </g>
        </g>
    </g>
</svg>
</a>
</div>
<div>
Loading ITables v2.2.1 from the internet...
(need <a href=https://mwouts.github.io/itables/troubleshooting.html>help</a>?)</td>
</div>
</tr></tbody>

</table>
<link href="https://www.unpkg.com/dt_for_itables@2.0.13/dt_bundle.css" rel="stylesheet">
<script type="module">
    import {DataTable, jQuery as $} from 'https://www.unpkg.com/dt_for_itables@2.0.13/dt_bundle.js';

    document.querySelectorAll("#itables_179fda97_ddbb_492e_a6b1_cabfce8830db:not(.dataTable)").forEach(table => {
        // Define the table data
        const data = [["Gemini 1.5 Pro", "Preview", "Gemini 1.5 Pro shows dramatically enhanced performance and includes a breakthrough in long context understanding."], ["A3 Mega VMs", "GA", "A3 Mega VMs powered by Nvidia H100 Tensor Core GPUs with twice the network bandwidth vs. A3 instances."], ["TPU v5p", "GA", "Our latest generation TPU pod consists of 8,960 chips interconnected to support the largest scale ML training and serving."], ["NVIDIA GB200 NVL72", "Coming Soon", "Nvidia's newest Grace Blackwell generation of GPUs coming to Google Cloud early in 2025."], ["Google Axion Processors", "Coming Soon", "Google's first custom Arm-based CPU designed for the data center will be available in preview later this year."], ["Cloud Storage Fuse Caching", "GA", "Cloud Storage Fuse Caching is generally available."], ["Parallelstore Caching", "Preview", "Parallelstore Caching is in preview."], ["Hyperdisk ML", "Preview", "Hyperdisk ML is our next generation block storage service optimized for AI inference and serving workloads."], ["Dynamic Workload Scheduler", "GA", "Dynamic Workload Scheduler now has two new options: Calendar Mode for start time assurance and Flex Start for optimized economics."], ["Gemini in Threat Intelligence", "Preview", "Gemini in Threat Intelligence is in preview."], ["Gemini in Security Command Center", "Preview", "Gemini in Security Command Center is in preview."], ["Google Vids", "GA", "Google Vids is generally available."], ["Text-to-Live Image", "Preview", "Text-to-Live Image is in preview."], ["Digital Watermarking", "GA", "Digital Watermarking is generally available."], ["New Editing Modes in Imagen 2.0", "GA", "New Editing Modes in Imagen 2.0 are generally available."], ["Gemini in BigQuery", "Preview", "Gemini in BigQuery simplifies data preparation using AI."], ["Vector Indexing in BigQuery and AlloyDB", "Preview", "Vector Indexing in BigQuery and AlloyDB allows you to query the right data in analytical and operational systems."], ["BigQuery Data Canvas", "Preview", "BigQuery Data Canvas provides a notebook-like experience with natural language and embedded visualizations."], ["Gemini Code Assist", "Preview", "Gemini Code Assist is in preview."], ["Vertex AI Agent Builder", "Preview", "Vertex AI Agent Builder lets you create customer agents that are amazingly powerful in just three key steps."]];

        // Define the dt_args
        let dt_args = {"layout": {"topStart": "pageLength", "topEnd": "search", "bottomStart": "info", "bottomEnd": "paging"}, "order": [], "warn_on_selected_rows_not_rendered": true, "initComplete": function () {
    // Apply the search
    this.api()
        .columns()
        .every(function () {
            const that = this;

            $('input', this.header()).on('keyup change clear', function () {
                if (that.search() !== this.value) {
                    that.search(this.value).draw();
                }
            });
        });
}
};
        dt_args["data"] = data;

        // Setup - add a text input to each header or footer cell
$('#itables_179fda97_ddbb_492e_a6b1_cabfce8830db:not(.dataTable) thead th').each(function () {
    let title = $(this).text();
    $(this).html('<input type="text" placeholder="Search ' +
        // We use encodeURI to avoid this LGTM error:
        // https://lgtm.com/rules/1511866576920/
        encodeURI(title).replaceAll("%20", " ") +
        '" />');
});

        new DataTable(table, dt_args);
    });
</script>



## Creating insights from analyzing multiple YouTube videos together

### Google "Year in Search" videos
Now, consider expanding the problem to a more common enterprise use case: extracting information from _multiple_ YouTube videos at once.

This time, we'll use [Google's "Year in Search" videos](https://about.google/intl/ALL_us/stories/year-in-search/), which summarize the questions, people, and moments that captured the world's attention in each year. As of fall 2024, there are 14 of these videos, each 2-4 minutes in length, from [2010](https://www.youtube.com/watch?v=F0QXB5pw2qE) through [2023](https://www.youtube.com/watch?v=3KtWfp0UopM).

We start by reading in a CSV file that has links to all the videos.


```
# Read in table of Year in Search video links from public CSV file
GOOGLE_YEAR_IN_SEARCH_VIDEO_LINKS_CSV_GCS_URI = (
    "gs://github-repo/video/google_year_in_search_video_links.csv"
)

year_in_search_yt_links = pd.read_csv(GOOGLE_YEAR_IN_SEARCH_VIDEO_LINKS_CSV_GCS_URI)

year_in_search_yt_links
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>year</th>
      <th>yt_link</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2023</td>
      <td>https://www.youtube.com/watch?v=3KtWfp0UopM</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2022</td>
      <td>https://www.youtube.com/watch?v=4WXs3sKu41I</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2021</td>
      <td>https://www.youtube.com/watch?v=EqboAI-Vk-U</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2020</td>
      <td>https://www.youtube.com/watch?v=rokGy0huYEA</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2019</td>
      <td>https://www.youtube.com/watch?v=ZRCdORJiUgU</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2018</td>
      <td>https://www.youtube.com/watch?v=6aFdEhEZQjE</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2017</td>
      <td>https://www.youtube.com/watch?v=vI4LHl4yFuo</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2016</td>
      <td>https://www.youtube.com/watch?v=KIViy7L_lo8</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2015</td>
      <td>https://www.youtube.com/watch?v=q7o7R5BgWDY</td>
    </tr>
    <tr>
      <th>9</th>
      <td>2014</td>
      <td>https://www.youtube.com/watch?v=DVwHCGAr_OE</td>
    </tr>
    <tr>
      <th>10</th>
      <td>2013</td>
      <td>https://www.youtube.com/watch?v=Lv-sY_z8MNs</td>
    </tr>
    <tr>
      <th>11</th>
      <td>2012</td>
      <td>https://www.youtube.com/watch?v=xY_MUB8adEQ</td>
    </tr>
    <tr>
      <th>12</th>
      <td>2011</td>
      <td>https://www.youtube.com/watch?v=SAIEamakLoY</td>
    </tr>
    <tr>
      <th>13</th>
      <td>2010</td>
      <td>https://www.youtube.com/watch?v=F0QXB5pw2qE</td>
    </tr>
  </tbody>
</table>
</div>



### Set up for analyzing multiple video files

Let's say we are a sports agency who wants to see which athletes or teams appear most often in these videos as a measure of cultural relevance. Instead of watching and manually counting, we can use Gemini's multimodal capabilities and world knowledge to extract each appearance of an athlete or team into a structured output that we can use for further analysis.

The system instructions, prompt, and response schema that will apply to all 14 videos are each created in the cell below.


```
# Set up pieces (prompt, response schema, config) for Google Year in Search videos
multiple_video_extraction_system_instruction_text = (
    "You are a video analyst that "
    "carefully looks through all frames of provided videos, extracting out the "
    "pieces necessary to respond to user prompts."
)

multiple_video_extraction_prompt = (
    "Which sports athletes or teams are mentioned or "
    "shown in this video? Please look through each frame carefully, and respond "
    "with a complete list that includes the athlete or team's name (1 row per "
    "athlete or team), whether they are an athlete or team, the sport they play, "
    "and the timestamp into the video at which they appear (in mm:ss format, "
    "do not give extra precision) for each one."
)

multiple_video_extraction_response_schema = {
    "type": "ARRAY",
    "items": {
        "type": "OBJECT",
        "properties": {
            "name": {"type": "STRING"},
            "athlete_or_team": {"type": "STRING", "enum": ["athlete", "team"]},
            "sport": {"type": "STRING"},
            "video_timestamp": {"type": "STRING"},
        },
    },
}

multiple_video_extraction_json_generation_config = GenerationConfig(
    temperature=0.0,
    max_output_tokens=8192,
    response_mime_type="application/json",
    response_schema=multiple_video_extraction_response_schema,
)

multiple_video_extraction_model = GenerativeModel(
    model_name=GEMINI_PRO_MODEL_ID,
    system_instruction=multiple_video_extraction_system_instruction_text,
    generation_config=multiple_video_extraction_json_generation_config,
)
```

Next, we'll set up to run each of these prompt/video pairs through the Gemini API _asynchronously_. This allows us to send all the requests to Gemini at once, then wait for all the answers to come back - a more efficient process than sending them synchronously (one-by-one). See more details in [this Google Cloud Community Medium blog post](https://medium.com/google-cloud/how-to-prompt-gemini-asynchronously-using-python-on-google-cloud-986ca45d9f1b).



```
# Function for asynchronous generation


@retry(wait=wait_random_exponential(multiplier=1, max=120), stop=stop_after_attempt(2))
async def async_generate(prompt, yt_link):
    try:
        response = await multiple_video_extraction_model.generate_content_async(
            [prompt, Part.from_uri(mime_type="video/webm", uri=yt_link)], stream=False
        )

        response_dict = response.to_dict()

        return response_dict
    except Exception as e:
        print("Something failed, retrying")
        print(e)
        with retry.stop_after_attempt(2) as retry_state:
            if retry_state.attempt > 2:
                return None
        raise  # Re-raise the exception for tenacity to handle
```

### Run asynchronous Gemini calls to do video extraction


```
# Perform asynchronous calls across all videos, gather responses
import asyncio

start_time = asyncio.get_event_loop().time()

get_responses = [
    async_generate(multiple_video_extraction_prompt, yt_link)
    for yt_link in year_in_search_yt_links["yt_link"]
]

multiple_video_extraction_responses = await asyncio.gather(*get_responses)

end_time = asyncio.get_event_loop().time()

elapsed_time = end_time - start_time

print(f"Elapsed time: {elapsed_time:.2f} seconds")
```

### Extract and analyze video results across years

Once we have the results from Gemini, we can process them and get table of every athlete or team appearance across all 14 "Year in Search" videos.


```
# Add structured outputs by year back to original table, show full extraction results
year_in_search_responses = year_in_search_yt_links.copy()

year_in_search_responses["gemini_response"] = [
    json.dumps(response) for response in multiple_video_extraction_responses
]


def extract_result_df_from_gemini_response(year, gemini_response):
    extract_response_text = json.loads(gemini_response)["candidates"][0]["content"][
        "parts"
    ][0]["text"]

    extract_result_df = pd.DataFrame(json.loads(extract_response_text))

    extract_result_df["year"] = year

    return extract_result_df


year_in_search_responses["extract_result_df"] = year_in_search_responses.apply(
    lambda row: extract_result_df_from_gemini_response(
        row["year"], row["gemini_response"]
    ),
    axis=1,
)

all_year_in_search_extractions = pd.concat(
    year_in_search_responses["extract_result_df"].tolist(), ignore_index=True
)[["year", "name", "athlete_or_team", "sport", "video_timestamp"]]

show(all_year_in_search_extractions)
```


<table id="itables_cce26bff_9815_42a3_ab31_8d18f198cfee" class="display nowrap" data-quarto-disable-processing="true" style="table-layout:auto;width:auto;margin:auto;caption-side:bottom">
<thead><th>year</th><th>name</th><th>athlete_or_team</th><th>sport</th><th>video_timestamp</th></thead><tbody><tr>
<td style="vertical-align:middle; text-align:left">
<div style="float:left; margin-right: 10px;">
<a href=https://mwouts.github.io/itables/><svg class="main-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
width="64" viewBox="0 0 500 400" style="font-family: 'Droid Sans', sans-serif;">
    <g style="fill:#d9d7fc">
        <path d="M100,400H500V357H100Z" />
        <path d="M100,300H400V257H100Z" />
        <path d="M0,200H400V157H0Z" />
        <path d="M100,100H500V57H100Z" />
        <path d="M100,350H500V307H100Z" />
        <path d="M100,250H400V207H100Z" />
        <path d="M0,150H400V107H0Z" />
        <path d="M100,50H500V7H100Z" />
    </g>
    <g style="fill:#1a1366;stroke:#1a1366;">
   <rect x="100" y="7" width="400" height="43">
    <animate
      attributeName="width"
      values="0;400;0"
      dur="5s"
      repeatCount="indefinite" />
      <animate
      attributeName="x"
      values="100;100;500"
      dur="5s"
      repeatCount="indefinite" />
  </rect>
        <rect x="0" y="107" width="400" height="43">
    <animate
      attributeName="width"
      values="0;400;0"
      dur="3.5s"
      repeatCount="indefinite" />
    <animate
      attributeName="x"
      values="0;0;400"
      dur="3.5s"
      repeatCount="indefinite" />
  </rect>
        <rect x="100" y="207" width="300" height="43">
    <animate
      attributeName="width"
      values="0;300;0"
      dur="3s"
      repeatCount="indefinite" />
    <animate
      attributeName="x"
      values="100;100;400"
      dur="3s"
      repeatCount="indefinite" />
  </rect>
        <rect x="100" y="307" width="400" height="43">
    <animate
      attributeName="width"
      values="0;400;0"
      dur="4s"
      repeatCount="indefinite" />
      <animate
      attributeName="x"
      values="100;100;500"
      dur="4s"
      repeatCount="indefinite" />
  </rect>
        <g style="fill:transparent;stroke-width:8; stroke-linejoin:round" rx="5">
            <g transform="translate(45 50) rotate(-45)">
                <circle r="33" cx="0" cy="0" />
                <rect x="-8" y="32" width="16" height="30" />
            </g>

            <g transform="translate(450 152)">
                <polyline points="-15,-20 -35,-20 -35,40 25,40 25,20" />
                <rect x="-15" y="-40" width="60" height="60" />
            </g>

            <g transform="translate(50 352)">
                <polygon points="-35,-5 0,-40 35,-5" />
                <polygon points="-35,10 0,45 35,10" />
            </g>

            <g transform="translate(75 250)">
                <polyline points="-30,30 -60,0 -30,-30" />
                <polyline points="0,30 -30,0 0,-30" />
            </g>

            <g transform="translate(425 250) rotate(180)">
                <polyline points="-30,30 -60,0 -30,-30" />
                <polyline points="0,30 -30,0 0,-30" />
            </g>
        </g>
    </g>
</svg>
</a>
</div>
<div>
Loading ITables v2.2.1 from the internet...
(need <a href=https://mwouts.github.io/itables/troubleshooting.html>help</a>?)</td>
</div>
</tr></tbody>

</table>
<link href="https://www.unpkg.com/dt_for_itables@2.0.13/dt_bundle.css" rel="stylesheet">
<script type="module">
    import {DataTable, jQuery as $} from 'https://www.unpkg.com/dt_for_itables@2.0.13/dt_bundle.js';

    document.querySelectorAll("#itables_cce26bff_9815_42a3_ab31_8d18f198cfee:not(.dataTable)").forEach(table => {
        // Define the table data
        const data = [[2023, "Spain", "team", "soccer", "00:32"], [2023, "Cristiano Ronaldo", "athlete", "soccer", "00:36"], [2023, "Lebron James", "athlete", "basketball", "00:43"], [2023, "Virat Kohli", "athlete", "cricket", "00:48"], [2023, "Coco Gauff", "athlete", "tennis", "03:27"], [2022, "Shohei Ohtani", "athlete", "baseball", "01:07"], [2022, "Sydney McLaughlin", "athlete", "track and field", "01:09"], [2022, "Roger Federer", "athlete", "tennis", "01:38"], [2022, "Serena Williams", "athlete", "tennis", "01:40"], [2022, "Bubba Wallace", "athlete", "auto racing", "01:41"], [2022, "Golden State Warriors", "team", "basketball", "01:42"], [2022, "England Women's National Football Team", "team", "soccer", "01:43"], [2021, "Naomi Osaka", "athlete", "Tennis", "00:29"], [2021, "Simone Biles", "athlete", "Gymnastics", "00:35"], [2021, "Angel City Football Club", "team", "Soccer", "01:42"], [2021, "Emma Raducanu", "athlete", "Tennis", "01:07"], [2020, "Los Angeles Lakers", "team", "basketball", "01:05"], [2020, "Kobe Bryant", "athlete", "basketball", "01:05"], [2020, "Gianna Bryant", "athlete", "basketball", "01:05"], [2020, "LeBron James", "athlete", "basketball", "02:33"], [2020, "Maya Gabeira", "athlete", "surfing", "02:40"], [2020, "Naomi Osaka", "athlete", "tennis", "02:42"], [2019, "Jackie Robinson", "athlete", "baseball", "00:07"], [2019, "Eliud Kipchoge", "athlete", "marathon running", "00:26"], [2019, "Nicolas Mahut", "athlete", "tennis", "00:37"], [2019, "US Women's National Team", "team", "soccer", "00:44"], [2019, "Coco Gauff", "athlete", "tennis", "00:45"], [2019, "Venus Williams", "athlete", "tennis", "00:45"], [2019, "Zion Williamson", "athlete", "basketball", "00:51"], [2019, "Nathan Chen", "athlete", "figure skating", "01:02"], [2019, "Simone Biles", "athlete", "gymnastics", "01:25"], [2019, "St. Louis Blues", "team", "ice hockey", "01:29"], [2019, "South Africa National Rugby Team", "team", "rugby", "01:31"], [2019, "Katelyn Ohashi", "athlete", "gymnastics", "01:49"], [2019, "England Cricket Team", "team", "cricket", "01:50"], [2018, "Thai Navy Seals", "team", "diving/rescue", "00:16"], [2018, "Wild Boars", "team", "soccer", "00:19"], [2018, "Naomi Osaka", "athlete", "tennis", "00:33"], [2018, "Mirai Nagasu", "athlete", "figure skating", "00:37"], [2018, "Washington Capitals", "team", "hockey", "01:31"], [2018, "Chloe Kim", "athlete", "snowboarding", "01:34"], [2018, "LeBron James", "athlete", "basketball", "01:12"], [2017, "Vegas Golden Knights", "team", "hockey", "00:37"], [2017, "Serena Williams", "athlete", "tennis", "01:25"], [2017, "Kevin Harvick", "athlete", "NASCAR", "01:42"], [2016, "Chicago Cubs", "team", "baseball", "01:04"], [2016, "Cleveland Cavaliers", "team", "basketball", "01:06"], [2016, "Simone Biles", "athlete", "gymnastics", "00:59"], [2016, "Abbey D'Agostino", "athlete", "track and field", "00:58"], [2016, "Nikki Hamblin", "athlete", "track and field", "00:58"], [2016, "Doaa Elghobashy", "athlete", "beach volleyball", "00:56"], [2016, "Nada Meawad", "athlete", "beach volleyball", "00:56"], [2016, "Muhammad Ali", "athlete", "boxing", "01:30"], [2015, "US Women's National Soccer Team", "team", "soccer", "00:45"], [2015, "Caitlyn Jenner", "athlete", "decathlon", "01:26"], [2015, "Ronda Rousey", "athlete", "MMA", "01:38"], [2014, "Germany national football team", "team", "football (soccer)", "00:33"], [2014, "Usain Bolt", "athlete", "track and field", "00:36"], [2013, "David Beckham", "athlete", "soccer", "00:48"], [2013, "Arjen Robben", "athlete", "soccer", "01:15"], [2012, "Felix Baumgartner", "athlete", "skydiving", "00:06"], [2012, "Oscar Pistorius", "athlete", "track and field", "00:13"], [2012, "Michael Phelps", "athlete", "swimming", "01:33"], [2012, "Spanish National Team", "team", "Association Football (soccer)", "01:37"], [2012, "Pele", "athlete", "Association Football (soccer)", "02:07"], [2012, "Jeremy Lin", "athlete", "basketball", "02:15"], [2012, "Oscar Pistorius", "athlete", "track and field", "02:16"], [2011, "St. Louis Cardinals", "team", "baseball", "00:17"], [2011, "Joe Frazier", "athlete", "boxing", "01:46"], [2011, "Japan Women's National Team", "team", "soccer", "02:18"], [2011, "Al Davis", "athlete", "American football", "01:41"], [2010, "Diego Milito", "athlete", "Soccer", "01:51"], [2010, "Graeme McDowell", "athlete", "Golf", "01:52"], [2010, "John Isner", "athlete", "Tennis", "01:54"], [2010, "Nicolas Mahut", "athlete", "Tennis", "01:54"], [2010, "San Francisco Giants", "team", "Baseball", "01:56"], [2010, "Sidney Crosby", "athlete", "Hockey", "01:58"], [2010, "Kim Yu-Na", "athlete", "Figure Skating", "01:59"]];

        // Define the dt_args
        let dt_args = {"layout": {"topStart": "pageLength", "topEnd": "search", "bottomStart": "info", "bottomEnd": "paging"}, "order": [], "warn_on_selected_rows_not_rendered": true, "initComplete": function () {
    // Apply the search
    this.api()
        .columns()
        .every(function () {
            const that = this;

            $('input', this.header()).on('keyup change clear', function () {
                if (that.search() !== this.value) {
                    that.search(this.value).draw();
                }
            });
        });
}
};
        dt_args["data"] = data;

        // Setup - add a text input to each header or footer cell
$('#itables_cce26bff_9815_42a3_ab31_8d18f198cfee:not(.dataTable) thead th').each(function () {
    let title = $(this).text();
    $(this).html('<input type="text" placeholder="Search ' +
        // We use encodeURI to avoid this LGTM error:
        // https://lgtm.com/rules/1511866576920/
        encodeURI(title).replaceAll("%20", " ") +
        '" />');
});

        new DataTable(table, dt_args);
    });
</script>



Finally, we can count the number of years in which each athlete or team appeared in these videos, and return results for those who appeared more than once.


```
# Analyze results to show athletes/teams showing up most often in Year in Search videos
multiple_year_in_search_app = (
    all_year_in_search_extractions.assign(
        # Convert 'name' to uppercase to handle e.g. "LeBron" vs "Lebron"
        name=all_year_in_search_extractions["name"].str.upper(),
        # Convert 'athlete_or_team' to lowercase for consistency
        athlete_or_team=all_year_in_search_extractions["athlete_or_team"].str.lower(),
    )
    .groupby(["name", "athlete_or_team"])
    .apply(
        lambda x: pd.Series(
            {
                # Aggregate 'sport' across type and name (handling different cases)
                "sport": ", ".join(sorted(x["sport"].str.lower().unique())),
                # Count # of diff years in which each athlete/team appears in video
                "num_years": x["year"].nunique(),
            }
        )
    )
    .reset_index()
    .
    # Filter to only those appearing multiple times
    query("num_years >= 2")
    .sort_values(["num_years", "name"], ascending=[False, True])
    .reset_index(drop=True)
)

# Display results
display(Markdown("<b>Athletes/Teams Appearing in Multiple Year in Search Videos<b>"))
display(multiple_year_in_search_app)
```


<b>Athletes/Teams Appearing in Multiple Year in Search Videos<b>



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>athlete_or_team</th>
      <th>sport</th>
      <th>num_years</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>LEBRON JAMES</td>
      <td>athlete</td>
      <td>basketball</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>NAOMI OSAKA</td>
      <td>athlete</td>
      <td>tennis</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>SIMONE BILES</td>
      <td>athlete</td>
      <td>gymnastics</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>COCO GAUFF</td>
      <td>athlete</td>
      <td>tennis</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>NICOLAS MAHUT</td>
      <td>athlete</td>
      <td>tennis</td>
      <td>2</td>
    </tr>
    <tr>
      <th>5</th>
      <td>SERENA WILLIAMS</td>
      <td>athlete</td>
      <td>tennis</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>





################################################## you_search_analysis_agents.md ##################################################


# You Search Analysis Agent
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MervinPraison/PraisonAI/blob/main/cookbooks/notebooks/you_search_analysis_agents.ipynb)

## Dependencies


```python
# Install dependencies without output
%pip install langchain_community > /dev/null
%pip install praisonai[crewai] > /dev/null
%pip install duckduckgo_search > /dev/null
```

## Tools


```python
# ToDo: You Search not shown as action in the output
# Confirm if YAML for both the You and Youtube is same as per on the Blog page

from praisonai_tools import BaseTool
from langchain_community.utilities.you import YouSearchAPIWrapper

class YouSearchTool(BaseTool):
    name: str = "You Search Tool"
    description: str = "Search You.com for relevant information based on a query."

    def _run(self, query: str):
        api_wrapper = YouSearchAPIWrapper()
        results = api_wrapper.results(query=query, max_results=5)
        return results
```

## YAML Prompt


```python
agent_yaml = """
framework: "crewai"
topic: "research about the causes of lung disease"
roles:
  research_analyst:
    role: "Research Analyst"
    backstory: "Experienced in analyzing scientific data related to respiratory health."
    goal: "Analyze data on lung diseases"
    tasks:
      data_analysis:
        description: "Gather and analyze data on the causes and risk factors of lung diseases."
        expected_output: "Report detailing key findings on lung disease causes."
    tools:
      - "YouSearchTool"
  medical_writer:
    role: "Medical Writer"
    backstory: "Skilled in translating complex medical information into accessible content."
    goal: "Compile comprehensive content on lung disease causes"
    tasks:
      content_creation:
        description: "Create detailed content summarizing the research findings on lung disease causes."
        expected_output: "Document outlining various causes and risk factors of lung diseases."
    tools: []
  editor:
    role: "Editor"
    backstory: "Proficient in editing medical content for accuracy and clarity."
    goal: "Review and refine content on lung disease causes"
    tasks:
      content_review:
        description: "Edit and refine the compiled content on lung disease causes for accuracy and coherence."
        expected_output: "Finalized document on lung disease causes ready for dissemination."
    tools: []
dependencies: []
"""
```

## Main


```python
import os
from praisonai import PraisonAI
from google.colab import userdata

# Create a PraisonAI instance with the agent_yaml content
praisonai = PraisonAI(agent_yaml=agent_yaml, tools=[YouSearchTool])

# Add OPENAI_API_KEY Secrets to Google Colab on the Left Hand Side 🔑 or Enter Manually Below
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY') or "ENTER OPENAI_API_KEY HERE"
os.environ["OPENAI_MODEL_NAME"] = "gpt-4o-mini"

# Run PraisonAI
result = praisonai.run()

# Print the result
print(result) # 8/10

```

    [1m[95m [2024-11-02 12:32:21][DEBUG]: == Working Agent: Research Analyst[00m
    [1m[95m [2024-11-02 12:32:21][INFO]: == Starting Task: Gather and analyze data on the causes and risk factors of lung diseases.[00m
    
    
    [1m> Entering new CrewAgentExecutor chain...[0m
    [32;1m[1;3mI now can give a great answer  
    Final Answer: 
    
    **Report on the Causes and Risk Factors of Lung Diseases**
    
    **Introduction**  
    Lung diseases encompass a variety of conditions that affect the lungs' ability to function properly. Understanding the causes and risk factors associated with these diseases is crucial for prevention, early diagnosis, and effective management. This report analyzes the prevalent causes and risk factors contributing to lung diseases, including environmental, occupational, lifestyle, and genetic influences.
    
    **Key Findings**
    
    1. **Environmental Factors**  
       a. **Air Pollution**: Exposure to pollutants such as particulate matter (PM), nitrogen dioxide (NO2), and sulfur dioxide (SO2) has been linked to increased incidences of chronic obstructive pulmonary disease (COPD) and lung cancer. Urban areas with high traffic emissions show a marked increase in respiratory ailments.  
       b. **Secondhand Smoke**: Inhalation of smoke from tobacco products used by others is a significant risk factor for lung cancer and respiratory infections, particularly in non-smokers.  
       c. **Indoor Air Quality**: Poor ventilation, the presence of mold, and the use of household products with volatile organic compounds (VOCs) contribute to respiratory problems.
    
    2. **Occupational Hazards**  
       a. **Asbestos Exposure**: Asbestos fibers are a well-known carcinogen leading to mesothelioma and lung fibrosis. Workers in construction, shipbuilding, and manufacturing are at higher risk.  
       b. **Dust and Chemical Exposure**: Occupational exposure to silica dust, coal dust, and chemicals like formaldehyde and benzene is associated with the development of pneumoconiosis and lung cancer.
    
    3. **Lifestyle Factors**  
       a. **Smoking**: Tobacco use remains the leading cause of preventable lung diseases, including COPD and lung cancer. The risk is dose-dependent, with heavier smokers facing exponentially higher risks.  
       b. **Obesity**: Excess body weight is associated with a higher risk of developing asthma and can worsen existing respiratory conditions by impacting lung function and inflammation.
    
    4. **Genetic Predispositions**  
       a. **Alpha-1 Antitrypsin Deficiency**: This genetic disorder can lead to early onset emphysema and liver disease.  
       b. **Family History**: A history of lung diseases in family members can indicate genetic susceptibility, particularly to conditions such as asthma and lung cancer.
    
    5. **Infectious Agents**  
       a. **Viral Infections**: Respiratory viruses, such as influenza and respiratory syncytial virus (RSV), can exacerbate underlying lung conditions and contribute to the development of secondary complications like pneumonia.  
       b. **Bacterial Infections**: Streptococcus pneumoniae and Mycobacterium tuberculosis are significant contributors to lung diseases, particularly in immunocompromised populations.
    
    6. **Socioeconomic Factors**  
       a. **Access to Healthcare**: Individuals in lower socioeconomic groups often have limited access to healthcare resources, leading to delayed diagnoses and poor management of lung diseases.  
       b. **Health Literacy**: Lack of awareness regarding lung health and disease prevention strategies contributes to higher incidences of respiratory illnesses in disadvantaged populations.
    
    **Conclusion**  
    The causes and risk factors of lung diseases are multifaceted, involving a combination of environmental exposures, lifestyle choices, occupational hazards, genetic predispositions, and infectious agents. A comprehensive approach that includes public health initiatives, increased awareness, and targeted interventions is essential for reducing the burden of lung diseases. Continued research is necessary to further elucidate these relationships and develop effective prevention strategies.
    
    **Recommendations**  
    - Enhance air quality regulations and promote smoke-free environments.  
    - Implement workplace safety measures to protect against occupational hazards.  
    - Foster public awareness campaigns highlighting the risks associated with smoking and obesity.  
    - Encourage genetic screening for at-risk populations to facilitate early intervention.  
    - Expand healthcare access and resources for vulnerable populations to improve respiratory health outcomes.
    
    This report serves as a foundation for understanding the complex interplay of factors that contribute to lung diseases and underscores the importance of a proactive approach in public health strategy.[0m
    
    [1m> Finished chain.[0m
    [1m[92m [2024-11-02 12:32:32][DEBUG]: == [Research Analyst] Task output: **Report on the Causes and Risk Factors of Lung Diseases**
    
    **Introduction**  
    Lung diseases encompass a variety of conditions that affect the lungs' ability to function properly. Understanding the causes and risk factors associated with these diseases is crucial for prevention, early diagnosis, and effective management. This report analyzes the prevalent causes and risk factors contributing to lung diseases, including environmental, occupational, lifestyle, and genetic influences.
    
    **Key Findings**
    
    1. **Environmental Factors**  
       a. **Air Pollution**: Exposure to pollutants such as particulate matter (PM), nitrogen dioxide (NO2), and sulfur dioxide (SO2) has been linked to increased incidences of chronic obstructive pulmonary disease (COPD) and lung cancer. Urban areas with high traffic emissions show a marked increase in respiratory ailments.  
       b. **Secondhand Smoke**: Inhalation of smoke from tobacco products used by others is a significant risk factor for lung cancer and respiratory infections, particularly in non-smokers.  
       c. **Indoor Air Quality**: Poor ventilation, the presence of mold, and the use of household products with volatile organic compounds (VOCs) contribute to respiratory problems.
    
    2. **Occupational Hazards**  
       a. **Asbestos Exposure**: Asbestos fibers are a well-known carcinogen leading to mesothelioma and lung fibrosis. Workers in construction, shipbuilding, and manufacturing are at higher risk.  
       b. **Dust and Chemical Exposure**: Occupational exposure to silica dust, coal dust, and chemicals like formaldehyde and benzene is associated with the development of pneumoconiosis and lung cancer.
    
    3. **Lifestyle Factors**  
       a. **Smoking**: Tobacco use remains the leading cause of preventable lung diseases, including COPD and lung cancer. The risk is dose-dependent, with heavier smokers facing exponentially higher risks.  
       b. **Obesity**: Excess body weight is associated with a higher risk of developing asthma and can worsen existing respiratory conditions by impacting lung function and inflammation.
    
    4. **Genetic Predispositions**  
       a. **Alpha-1 Antitrypsin Deficiency**: This genetic disorder can lead to early onset emphysema and liver disease.  
       b. **Family History**: A history of lung diseases in family members can indicate genetic susceptibility, particularly to conditions such as asthma and lung cancer.
    
    5. **Infectious Agents**  
       a. **Viral Infections**: Respiratory viruses, such as influenza and respiratory syncytial virus (RSV), can exacerbate underlying lung conditions and contribute to the development of secondary complications like pneumonia.  
       b. **Bacterial Infections**: Streptococcus pneumoniae and Mycobacterium tuberculosis are significant contributors to lung diseases, particularly in immunocompromised populations.
    
    6. **Socioeconomic Factors**  
       a. **Access to Healthcare**: Individuals in lower socioeconomic groups often have limited access to healthcare resources, leading to delayed diagnoses and poor management of lung diseases.  
       b. **Health Literacy**: Lack of awareness regarding lung health and disease prevention strategies contributes to higher incidences of respiratory illnesses in disadvantaged populations.
    
    **Conclusion**  
    The causes and risk factors of lung diseases are multifaceted, involving a combination of environmental exposures, lifestyle choices, occupational hazards, genetic predispositions, and infectious agents. A comprehensive approach that includes public health initiatives, increased awareness, and targeted interventions is essential for reducing the burden of lung diseases. Continued research is necessary to further elucidate these relationships and develop effective prevention strategies.
    
    **Recommendations**  
    - Enhance air quality regulations and promote smoke-free environments.  
    - Implement workplace safety measures to protect against occupational hazards.  
    - Foster public awareness campaigns highlighting the risks associated with smoking and obesity.  
    - Encourage genetic screening for at-risk populations to facilitate early intervention.  
    - Expand healthcare access and resources for vulnerable populations to improve respiratory health outcomes.
    
    This report serves as a foundation for understanding the complex interplay of factors that contribute to lung diseases and underscores the importance of a proactive approach in public health strategy.
    
    [00m
    [1m[95m [2024-11-02 12:32:32][DEBUG]: == Working Agent: Medical Writer[00m
    [1m[95m [2024-11-02 12:32:32][INFO]: == Starting Task: Create detailed content summarizing the research findings on lung disease causes.[00m
    
    
    [1m> Entering new CrewAgentExecutor chain...[0m
    [32;1m[1;3mI now can give a great answer  
    Final Answer: 
    
    **Report on the Causes and Risk Factors of Lung Diseases**
    
    **Introduction**  
    Lung diseases encompass a variety of conditions that affect the lungs' ability to function properly. Understanding the causes and risk factors associated with these diseases is crucial for prevention, early diagnosis, and effective management. This report analyzes the prevalent causes and risk factors contributing to lung diseases, including environmental, occupational, lifestyle, and genetic influences.
    
    **Key Findings**
    
    1. **Environmental Factors**  
       a. **Air Pollution**: Exposure to pollutants such as particulate matter (PM), nitrogen dioxide (NO2), and sulfur dioxide (SO2) has been linked to increased incidences of chronic obstructive pulmonary disease (COPD) and lung cancer. Urban areas with high traffic emissions show a marked increase in respiratory ailments. Studies indicate that long-term exposure to these pollutants can cause inflammation and damage to lung tissues, leading to chronic respiratory conditions.  
       b. **Secondhand Smoke**: Inhalation of smoke from tobacco products used by others is a significant risk factor for lung cancer and respiratory infections, particularly in non-smokers. Secondhand smoke contains over 7,000 chemicals, many of which are toxic and can cause cancer.  
       c. **Indoor Air Quality**: Poor ventilation, the presence of mold, and the use of household products with volatile organic compounds (VOCs) contribute to respiratory problems. Research shows that individuals exposed to indoor pollutants are at an increased risk of asthma and other chronic lung diseases.
    
    2. **Occupational Hazards**  
       a. **Asbestos Exposure**: Asbestos fibers are a well-known carcinogen leading to mesothelioma and lung fibrosis. Workers in construction, shipbuilding, and manufacturing are at higher risk. Asbestos exposure can cause scarring of lung tissue, leading to decreased lung function over time.  
       b. **Dust and Chemical Exposure**: Occupational exposure to silica dust, coal dust, and chemicals like formaldehyde and benzene is associated with the development of pneumoconiosis and lung cancer. These substances can lead to chronic inflammation and fibrosis, compromising respiratory health.
    
    3. **Lifestyle Factors**  
       a. **Smoking**: Tobacco use remains the leading cause of preventable lung diseases, including COPD and lung cancer. The risk is dose-dependent, with heavier smokers facing exponentially higher risks. Smoking damages the airways and alveoli, leading to chronic inflammation and reduced lung capacity.  
       b. **Obesity**: Excess body weight is associated with a higher risk of developing asthma and can worsen existing respiratory conditions by impacting lung function and inflammation. Obesity can lead to mechanical restrictions on lung expansion and increased airway resistance.
    
    4. **Genetic Predispositions**  
       a. **Alpha-1 Antitrypsin Deficiency**: This genetic disorder can lead to early onset emphysema and liver disease. Individuals with this deficiency are at a heightened risk for lung damage even in the absence of other risk factors.  
       b. **Family History**: A history of lung diseases in family members can indicate genetic susceptibility, particularly to conditions such as asthma and lung cancer. Genetic mutations can influence how individuals respond to environmental exposures and their overall lung health.
    
    5. **Infectious Agents**  
       a. **Viral Infections**: Respiratory viruses, such as influenza and respiratory syncytial virus (RSV), can exacerbate underlying lung conditions and contribute to the development of secondary complications like pneumonia. Viral infections can lead to airway inflammation and increased mucus production, complicating existing respiratory issues.  
       b. **Bacterial Infections**: Streptococcus pneumoniae and Mycobacterium tuberculosis are significant contributors to lung diseases, particularly in immunocompromised populations. Tuberculosis can cause severe lung damage and is a major public health concern in areas with high prevalence.
    
    6. **Socioeconomic Factors**  
       a. **Access to Healthcare**: Individuals in lower socioeconomic groups often have limited access to healthcare resources, leading to delayed diagnoses and poor management of lung diseases. This can exacerbate health disparities and result in worse health outcomes.  
       b. **Health Literacy**: Lack of awareness regarding lung health and disease prevention strategies contributes to higher incidences of respiratory illnesses in disadvantaged populations. Educational initiatives are vital in empowering individuals to make informed health choices and seek medical care when necessary.
    
    **Conclusion**  
    The causes and risk factors of lung diseases are multifaceted, involving a combination of environmental exposures, lifestyle choices, occupational hazards, genetic predispositions, and infectious agents. A comprehensive approach that includes public health initiatives, increased awareness, and targeted interventions is essential for reducing the burden of lung diseases. Continued research is necessary to further elucidate these relationships and develop effective prevention strategies.
    
    **Recommendations**  
    - Enhance air quality regulations and promote smoke-free environments to reduce environmental exposures.  
    - Implement workplace safety measures to protect against occupational hazards and reduce exposure to harmful substances.  
    - Foster public awareness campaigns highlighting the risks associated with smoking and obesity, encouraging healthier lifestyle choices.  
    - Encourage genetic screening for at-risk populations to facilitate early intervention and management of genetic predispositions.  
    - Expand healthcare access and resources for vulnerable populations to improve respiratory health outcomes and ensure timely medical care.
    
    This report serves as a foundation for understanding the complex interplay of factors that contribute to lung diseases and underscores the importance of a proactive approach in public health strategy.[0m
    
    [1m> Finished chain.[0m
    [1m[92m [2024-11-02 12:32:44][DEBUG]: == [Medical Writer] Task output: **Report on the Causes and Risk Factors of Lung Diseases**
    
    **Introduction**  
    Lung diseases encompass a variety of conditions that affect the lungs' ability to function properly. Understanding the causes and risk factors associated with these diseases is crucial for prevention, early diagnosis, and effective management. This report analyzes the prevalent causes and risk factors contributing to lung diseases, including environmental, occupational, lifestyle, and genetic influences.
    
    **Key Findings**
    
    1. **Environmental Factors**  
       a. **Air Pollution**: Exposure to pollutants such as particulate matter (PM), nitrogen dioxide (NO2), and sulfur dioxide (SO2) has been linked to increased incidences of chronic obstructive pulmonary disease (COPD) and lung cancer. Urban areas with high traffic emissions show a marked increase in respiratory ailments. Studies indicate that long-term exposure to these pollutants can cause inflammation and damage to lung tissues, leading to chronic respiratory conditions.  
       b. **Secondhand Smoke**: Inhalation of smoke from tobacco products used by others is a significant risk factor for lung cancer and respiratory infections, particularly in non-smokers. Secondhand smoke contains over 7,000 chemicals, many of which are toxic and can cause cancer.  
       c. **Indoor Air Quality**: Poor ventilation, the presence of mold, and the use of household products with volatile organic compounds (VOCs) contribute to respiratory problems. Research shows that individuals exposed to indoor pollutants are at an increased risk of asthma and other chronic lung diseases.
    
    2. **Occupational Hazards**  
       a. **Asbestos Exposure**: Asbestos fibers are a well-known carcinogen leading to mesothelioma and lung fibrosis. Workers in construction, shipbuilding, and manufacturing are at higher risk. Asbestos exposure can cause scarring of lung tissue, leading to decreased lung function over time.  
       b. **Dust and Chemical Exposure**: Occupational exposure to silica dust, coal dust, and chemicals like formaldehyde and benzene is associated with the development of pneumoconiosis and lung cancer. These substances can lead to chronic inflammation and fibrosis, compromising respiratory health.
    
    3. **Lifestyle Factors**  
       a. **Smoking**: Tobacco use remains the leading cause of preventable lung diseases, including COPD and lung cancer. The risk is dose-dependent, with heavier smokers facing exponentially higher risks. Smoking damages the airways and alveoli, leading to chronic inflammation and reduced lung capacity.  
       b. **Obesity**: Excess body weight is associated with a higher risk of developing asthma and can worsen existing respiratory conditions by impacting lung function and inflammation. Obesity can lead to mechanical restrictions on lung expansion and increased airway resistance.
    
    4. **Genetic Predispositions**  
       a. **Alpha-1 Antitrypsin Deficiency**: This genetic disorder can lead to early onset emphysema and liver disease. Individuals with this deficiency are at a heightened risk for lung damage even in the absence of other risk factors.  
       b. **Family History**: A history of lung diseases in family members can indicate genetic susceptibility, particularly to conditions such as asthma and lung cancer. Genetic mutations can influence how individuals respond to environmental exposures and their overall lung health.
    
    5. **Infectious Agents**  
       a. **Viral Infections**: Respiratory viruses, such as influenza and respiratory syncytial virus (RSV), can exacerbate underlying lung conditions and contribute to the development of secondary complications like pneumonia. Viral infections can lead to airway inflammation and increased mucus production, complicating existing respiratory issues.  
       b. **Bacterial Infections**: Streptococcus pneumoniae and Mycobacterium tuberculosis are significant contributors to lung diseases, particularly in immunocompromised populations. Tuberculosis can cause severe lung damage and is a major public health concern in areas with high prevalence.
    
    6. **Socioeconomic Factors**  
       a. **Access to Healthcare**: Individuals in lower socioeconomic groups often have limited access to healthcare resources, leading to delayed diagnoses and poor management of lung diseases. This can exacerbate health disparities and result in worse health outcomes.  
       b. **Health Literacy**: Lack of awareness regarding lung health and disease prevention strategies contributes to higher incidences of respiratory illnesses in disadvantaged populations. Educational initiatives are vital in empowering individuals to make informed health choices and seek medical care when necessary.
    
    **Conclusion**  
    The causes and risk factors of lung diseases are multifaceted, involving a combination of environmental exposures, lifestyle choices, occupational hazards, genetic predispositions, and infectious agents. A comprehensive approach that includes public health initiatives, increased awareness, and targeted interventions is essential for reducing the burden of lung diseases. Continued research is necessary to further elucidate these relationships and develop effective prevention strategies.
    
    **Recommendations**  
    - Enhance air quality regulations and promote smoke-free environments to reduce environmental exposures.  
    - Implement workplace safety measures to protect against occupational hazards and reduce exposure to harmful substances.  
    - Foster public awareness campaigns highlighting the risks associated with smoking and obesity, encouraging healthier lifestyle choices.  
    - Encourage genetic screening for at-risk populations to facilitate early intervention and management of genetic predispositions.  
    - Expand healthcare access and resources for vulnerable populations to improve respiratory health outcomes and ensure timely medical care.
    
    This report serves as a foundation for understanding the complex interplay of factors that contribute to lung diseases and underscores the importance of a proactive approach in public health strategy.
    
    [00m
    [1m[95m [2024-11-02 12:32:44][DEBUG]: == Working Agent: Editor[00m
    [1m[95m [2024-11-02 12:32:44][INFO]: == Starting Task: Edit and refine the compiled content on lung disease causes for accuracy and coherence.[00m
    
    
    [1m> Entering new CrewAgentExecutor chain...[0m
    [32;1m[1;3mI now can give a great answer  
    Final Answer: 
    
    **Report on the Causes and Risk Factors of Lung Diseases**
    
    **Introduction**  
    Lung diseases encompass a variety of conditions that impair the lungs' ability to function effectively. Understanding the causes and risk factors associated with these diseases is crucial for prevention, early diagnosis, and effective management. This report analyzes the prevalent causes and risk factors contributing to lung diseases, including environmental, occupational, lifestyle, and genetic influences.
    
    **Key Findings**
    
    1. **Environmental Factors**  
       a. **Air Pollution**: Exposure to pollutants such as particulate matter (PM), nitrogen dioxide (NO2), and sulfur dioxide (SO2) is linked to increased incidences of chronic obstructive pulmonary disease (COPD) and lung cancer. Urban areas with high traffic emissions demonstrate a marked increase in respiratory ailments. Long-term exposure to these pollutants can cause inflammation and damage to lung tissues, resulting in chronic respiratory conditions.  
       b. **Secondhand Smoke**: Inhalation of smoke from tobacco products used by others is a significant risk factor for lung cancer and respiratory infections, particularly in non-smokers. Secondhand smoke contains over 7,000 chemicals, many of which are toxic and can contribute to cancer development.  
       c. **Indoor Air Quality**: Poor ventilation, the presence of mold, and the use of household products containing volatile organic compounds (VOCs) can lead to respiratory problems. Research indicates that individuals exposed to indoor pollutants are at an increased risk for asthma and other chronic lung diseases.
    
    2. **Occupational Hazards**  
       a. **Asbestos Exposure**: Asbestos fibers are a well-known carcinogen leading to mesothelioma and lung fibrosis. Workers in construction, shipbuilding, and manufacturing are at higher risk. Asbestos exposure can cause scarring of lung tissue, resulting in decreased lung function over time.  
       b. **Dust and Chemical Exposure**: Occupational exposure to silica dust, coal dust, and chemicals such as formaldehyde and benzene is associated with the development of pneumoconiosis and lung cancer. These substances can lead to chronic inflammation and fibrosis, severely compromising respiratory health.
    
    3. **Lifestyle Factors**  
       a. **Smoking**: Tobacco use remains the leading cause of preventable lung diseases, including COPD and lung cancer. The risk is dose-dependent, with heavier smokers facing exponentially higher risks. Smoking damages the airways and alveoli, leading to chronic inflammation and reduced lung capacity.  
       b. **Obesity**: Excess body weight is associated with a higher risk of developing asthma and can exacerbate existing respiratory conditions by negatively impacting lung function and increasing inflammation. Obesity may result in mechanical restrictions on lung expansion and increased airway resistance.
    
    4. **Genetic Predispositions**  
       a. **Alpha-1 Antitrypsin Deficiency**: This genetic disorder can lead to early onset emphysema and liver disease. Individuals with this deficiency are at an increased risk for lung damage, even in the absence of other risk factors.  
       b. **Family History**: A history of lung diseases among family members may indicate genetic susceptibility, especially to conditions such as asthma and lung cancer. Genetic mutations can influence individuals' responses to environmental exposures and their overall lung health.
    
    5. **Infectious Agents**  
       a. **Viral Infections**: Respiratory viruses, including influenza and respiratory syncytial virus (RSV), can exacerbate underlying lung conditions and contribute to secondary complications like pneumonia. Viral infections can lead to airway inflammation and increased mucus production, complicating existing respiratory issues.  
       b. **Bacterial Infections**: Pathogens such as Streptococcus pneumoniae and Mycobacterium tuberculosis are significant contributors to lung diseases, particularly in immunocompromised populations. Tuberculosis can cause severe lung damage and poses a major public health concern in areas with high prevalence.
    
    6. **Socioeconomic Factors**  
       a. **Access to Healthcare**: Individuals from lower socioeconomic backgrounds often have limited access to healthcare resources, which can result in delayed diagnoses and poor management of lung diseases. This exacerbates health disparities and leads to worse health outcomes.  
       b. **Health Literacy**: A lack of awareness regarding lung health and disease prevention strategies contributes to higher incidences of respiratory illnesses in disadvantaged populations. Educational initiatives are vital for empowering individuals to make informed health choices and seek medical care when necessary.
    
    **Conclusion**  
    The causes and risk factors of lung diseases are multifaceted, involving a combination of environmental exposures, lifestyle choices, occupational hazards, genetic predispositions, and infectious agents. A comprehensive approach that includes public health initiatives, increased awareness, and targeted interventions is essential for reducing the burden of lung diseases. Continued research is necessary to further elucidate these relationships and develop effective prevention strategies.
    
    **Recommendations**  
    - Enhance air quality regulations and promote smoke-free environments to reduce environmental exposures.  
    - Implement workplace safety measures to protect against occupational hazards and minimize exposure to harmful substances.  
    - Foster public awareness campaigns that highlight the risks associated with smoking and obesity, encouraging healthier lifestyle choices.  
    - Encourage genetic screening for at-risk populations to facilitate early intervention and management of genetic predispositions.  
    - Expand healthcare access and resources for vulnerable populations to improve respiratory health outcomes and ensure timely medical care.
    
    This report serves as a foundation for understanding the complex interplay of factors contributing to lung diseases and underscores the importance of a proactive approach in public health strategy.[0m
    
    [1m> Finished chain.[0m
    [1m[92m [2024-11-02 12:32:59][DEBUG]: == [Editor] Task output: **Report on the Causes and Risk Factors of Lung Diseases**
    
    **Introduction**  
    Lung diseases encompass a variety of conditions that impair the lungs' ability to function effectively. Understanding the causes and risk factors associated with these diseases is crucial for prevention, early diagnosis, and effective management. This report analyzes the prevalent causes and risk factors contributing to lung diseases, including environmental, occupational, lifestyle, and genetic influences.
    
    **Key Findings**
    
    1. **Environmental Factors**  
       a. **Air Pollution**: Exposure to pollutants such as particulate matter (PM), nitrogen dioxide (NO2), and sulfur dioxide (SO2) is linked to increased incidences of chronic obstructive pulmonary disease (COPD) and lung cancer. Urban areas with high traffic emissions demonstrate a marked increase in respiratory ailments. Long-term exposure to these pollutants can cause inflammation and damage to lung tissues, resulting in chronic respiratory conditions.  
       b. **Secondhand Smoke**: Inhalation of smoke from tobacco products used by others is a significant risk factor for lung cancer and respiratory infections, particularly in non-smokers. Secondhand smoke contains over 7,000 chemicals, many of which are toxic and can contribute to cancer development.  
       c. **Indoor Air Quality**: Poor ventilation, the presence of mold, and the use of household products containing volatile organic compounds (VOCs) can lead to respiratory problems. Research indicates that individuals exposed to indoor pollutants are at an increased risk for asthma and other chronic lung diseases.
    
    2. **Occupational Hazards**  
       a. **Asbestos Exposure**: Asbestos fibers are a well-known carcinogen leading to mesothelioma and lung fibrosis. Workers in construction, shipbuilding, and manufacturing are at higher risk. Asbestos exposure can cause scarring of lung tissue, resulting in decreased lung function over time.  
       b. **Dust and Chemical Exposure**: Occupational exposure to silica dust, coal dust, and chemicals such as formaldehyde and benzene is associated with the development of pneumoconiosis and lung cancer. These substances can lead to chronic inflammation and fibrosis, severely compromising respiratory health.
    
    3. **Lifestyle Factors**  
       a. **Smoking**: Tobacco use remains the leading cause of preventable lung diseases, including COPD and lung cancer. The risk is dose-dependent, with heavier smokers facing exponentially higher risks. Smoking damages the airways and alveoli, leading to chronic inflammation and reduced lung capacity.  
       b. **Obesity**: Excess body weight is associated with a higher risk of developing asthma and can exacerbate existing respiratory conditions by negatively impacting lung function and increasing inflammation. Obesity may result in mechanical restrictions on lung expansion and increased airway resistance.
    
    4. **Genetic Predispositions**  
       a. **Alpha-1 Antitrypsin Deficiency**: This genetic disorder can lead to early onset emphysema and liver disease. Individuals with this deficiency are at an increased risk for lung damage, even in the absence of other risk factors.  
       b. **Family History**: A history of lung diseases among family members may indicate genetic susceptibility, especially to conditions such as asthma and lung cancer. Genetic mutations can influence individuals' responses to environmental exposures and their overall lung health.
    
    5. **Infectious Agents**  
       a. **Viral Infections**: Respiratory viruses, including influenza and respiratory syncytial virus (RSV), can exacerbate underlying lung conditions and contribute to secondary complications like pneumonia. Viral infections can lead to airway inflammation and increased mucus production, complicating existing respiratory issues.  
       b. **Bacterial Infections**: Pathogens such as Streptococcus pneumoniae and Mycobacterium tuberculosis are significant contributors to lung diseases, particularly in immunocompromised populations. Tuberculosis can cause severe lung damage and poses a major public health concern in areas with high prevalence.
    
    6. **Socioeconomic Factors**  
       a. **Access to Healthcare**: Individuals from lower socioeconomic backgrounds often have limited access to healthcare resources, which can result in delayed diagnoses and poor management of lung diseases. This exacerbates health disparities and leads to worse health outcomes.  
       b. **Health Literacy**: A lack of awareness regarding lung health and disease prevention strategies contributes to higher incidences of respiratory illnesses in disadvantaged populations. Educational initiatives are vital for empowering individuals to make informed health choices and seek medical care when necessary.
    
    **Conclusion**  
    The causes and risk factors of lung diseases are multifaceted, involving a combination of environmental exposures, lifestyle choices, occupational hazards, genetic predispositions, and infectious agents. A comprehensive approach that includes public health initiatives, increased awareness, and targeted interventions is essential for reducing the burden of lung diseases. Continued research is necessary to further elucidate these relationships and develop effective prevention strategies.
    
    **Recommendations**  
    - Enhance air quality regulations and promote smoke-free environments to reduce environmental exposures.  
    - Implement workplace safety measures to protect against occupational hazards and minimize exposure to harmful substances.  
    - Foster public awareness campaigns that highlight the risks associated with smoking and obesity, encouraging healthier lifestyle choices.  
    - Encourage genetic screening for at-risk populations to facilitate early intervention and management of genetic predispositions.  
    - Expand healthcare access and resources for vulnerable populations to improve respiratory health outcomes and ensure timely medical care.
    
    This report serves as a foundation for understanding the complex interplay of factors contributing to lung diseases and underscores the importance of a proactive approach in public health strategy.
    
    [00m
    


<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">### Task Output ###
**Report on the Causes and Risk Factors of Lung Diseases**

**Introduction**  
Lung diseases encompass a variety of conditions that impair the lungs' ability to function effectively. 
Understanding the causes and risk factors associated with these diseases is crucial for prevention, early 
diagnosis, and effective management. This report analyzes the prevalent causes and risk factors contributing to 
lung diseases, including environmental, occupational, lifestyle, and genetic influences.

**Key Findings**

<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>. **Environmental Factors**  
   a. **Air Pollution**: Exposure to pollutants such as particulate matter <span style="font-weight: bold">(</span>PM<span style="font-weight: bold">)</span>, nitrogen dioxide <span style="font-weight: bold">(</span>NO2<span style="font-weight: bold">)</span>, and sulfur
dioxide <span style="font-weight: bold">(</span>SO2<span style="font-weight: bold">)</span> is linked to increased incidences of chronic obstructive pulmonary disease <span style="font-weight: bold">(</span>COPD<span style="font-weight: bold">)</span> and lung cancer. 
Urban areas with high traffic emissions demonstrate a marked increase in respiratory ailments. Long-term exposure 
to these pollutants can cause inflammation and damage to lung tissues, resulting in chronic respiratory conditions.
   b. **Secondhand Smoke**: Inhalation of smoke from tobacco products used by others is a significant risk factor 
for lung cancer and respiratory infections, particularly in non-smokers. Secondhand smoke contains over <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">000</span> 
chemicals, many of which are toxic and can contribute to cancer development.  
   c. **Indoor Air Quality**: Poor ventilation, the presence of mold, and the use of household products containing 
volatile organic compounds <span style="font-weight: bold">(</span>VOCs<span style="font-weight: bold">)</span> can lead to respiratory problems. Research indicates that individuals exposed to 
indoor pollutants are at an increased risk for asthma and other chronic lung diseases.

<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>. **Occupational Hazards**  
   a. **Asbestos Exposure**: Asbestos fibers are a well-known carcinogen leading to mesothelioma and lung fibrosis.
Workers in construction, shipbuilding, and manufacturing are at higher risk. Asbestos exposure can cause scarring 
of lung tissue, resulting in decreased lung function over time.  
   b. **Dust and Chemical Exposure**: Occupational exposure to silica dust, coal dust, and chemicals such as 
formaldehyde and benzene is associated with the development of pneumoconiosis and lung cancer. These substances can
lead to chronic inflammation and fibrosis, severely compromising respiratory health.

<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>. **Lifestyle Factors**  
   a. **Smoking**: Tobacco use remains the leading cause of preventable lung diseases, including COPD and lung 
cancer. The risk is dose-dependent, with heavier smokers facing exponentially higher risks. Smoking damages the 
airways and alveoli, leading to chronic inflammation and reduced lung capacity.  
   b. **Obesity**: Excess body weight is associated with a higher risk of developing asthma and can exacerbate 
existing respiratory conditions by negatively impacting lung function and increasing inflammation. Obesity may 
result in mechanical restrictions on lung expansion and increased airway resistance.

<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>. **Genetic Predispositions**  
   a. **Alpha-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span> Antitrypsin Deficiency**: This genetic disorder can lead to early onset emphysema and liver 
disease. Individuals with this deficiency are at an increased risk for lung damage, even in the absence of other 
risk factors.  
   b. **Family History**: A history of lung diseases among family members may indicate genetic susceptibility, 
especially to conditions such as asthma and lung cancer. Genetic mutations can influence individuals' responses to 
environmental exposures and their overall lung health.

<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>. **Infectious Agents**  
   a. **Viral Infections**: Respiratory viruses, including influenza and respiratory syncytial virus <span style="font-weight: bold">(</span>RSV<span style="font-weight: bold">)</span>, can 
exacerbate underlying lung conditions and contribute to secondary complications like pneumonia. Viral infections 
can lead to airway inflammation and increased mucus production, complicating existing respiratory issues.  
   b. **Bacterial Infections**: Pathogens such as Streptococcus pneumoniae and Mycobacterium tuberculosis are 
significant contributors to lung diseases, particularly in immunocompromised populations. Tuberculosis can cause 
severe lung damage and poses a major public health concern in areas with high prevalence.

<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>. **Socioeconomic Factors**  
   a. **Access to Healthcare**: Individuals from lower socioeconomic backgrounds often have limited access to 
healthcare resources, which can result in delayed diagnoses and poor management of lung diseases. This exacerbates 
health disparities and leads to worse health outcomes.  
   b. **Health Literacy**: A lack of awareness regarding lung health and disease prevention strategies contributes 
to higher incidences of respiratory illnesses in disadvantaged populations. Educational initiatives are vital for 
empowering individuals to make informed health choices and seek medical care when necessary.

**Conclusion**  
The causes and risk factors of lung diseases are multifaceted, involving a combination of environmental exposures, 
lifestyle choices, occupational hazards, genetic predispositions, and infectious agents. A comprehensive approach 
that includes public health initiatives, increased awareness, and targeted interventions is essential for reducing 
the burden of lung diseases. Continued research is necessary to further elucidate these relationships and develop 
effective prevention strategies.

**Recommendations**  
- Enhance air quality regulations and promote smoke-free environments to reduce environmental exposures.  
- Implement workplace safety measures to protect against occupational hazards and minimize exposure to harmful 
substances.  
- Foster public awareness campaigns that highlight the risks associated with smoking and obesity, encouraging 
healthier lifestyle choices.  
- Encourage genetic screening for at-risk populations to facilitate early intervention and management of genetic 
predispositions.  
- Expand healthcare access and resources for vulnerable populations to improve respiratory health outcomes and 
ensure timely medical care.

This report serves as a foundation for understanding the complex interplay of factors contributing to lung diseases
and underscores the importance of a proactive approach in public health strategy.
</pre>



    None
    


```python

```




################################################## yuan2.md ##################################################


---
sidebar_label: Yuan2.0
---
# Yuan2.0

This notebook shows how to use [YUAN2 API](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/docs/inference_server.md) in LangChain with the langchain.chat_models.ChatYuan2.

[*Yuan2.0*](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/README-EN.md) is a new generation Fundamental Large Language Model developed by IEIT System. We have published all three models, Yuan 2.0-102B, Yuan 2.0-51B, and Yuan 2.0-2B. And we provide relevant scripts for pretraining, fine-tuning, and inference services for other developers. Yuan2.0 is based on Yuan1.0, utilizing a wider range of high-quality pre training data and instruction fine-tuning datasets to enhance the model's understanding of semantics, mathematics, reasoning, code, knowledge, and other aspects.

## Getting started
### Installation
First, Yuan2.0 provided an OpenAI compatible API, and we integrate ChatYuan2 into langchain chat model by using OpenAI client.
Therefore, ensure the openai package is installed in your Python environment. Run the following command:


```python
%pip install --upgrade --quiet openai
```

### Importing the Required Modules
After installation, import the necessary modules to your Python script:


```python
from langchain_community.chat_models import ChatYuan2
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
```

### Setting Up Your API server
Setting up your OpenAI compatible API server following [yuan2 openai api server](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/docs/Yuan2_fastchat.md).
If you deployed api server locally, you can simply set `yuan2_api_key="EMPTY"` or anything you want.
Just make sure, the `yuan2_api_base` is set correctly.


```python
yuan2_api_key = "your_api_key"
yuan2_api_base = "http://127.0.0.1:8001/v1"
```

### Initialize the ChatYuan2 Model
Here's how to initialize the chat model:


```python
chat = ChatYuan2(
    yuan2_api_base="http://127.0.0.1:8001/v1",
    temperature=1.0,
    model_name="yuan2",
    max_retries=3,
    streaming=False,
)
```

### Basic Usage
Invoke the model with system and human messages like this:


```python
messages = [
    SystemMessage(content="你是一个人工智能助手。"),
    HumanMessage(content="你好，你是谁？"),
]
```


```python
print(chat.invoke(messages))
```

### Basic Usage with streaming
For continuous interaction, use the streaming feature:


```python
from langchain_core.callbacks import StreamingStdOutCallbackHandler

chat = ChatYuan2(
    yuan2_api_base="http://127.0.0.1:8001/v1",
    temperature=1.0,
    model_name="yuan2",
    max_retries=3,
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()],
)
messages = [
    SystemMessage(content="你是个旅游小助手。"),
    HumanMessage(content="给我介绍一下北京有哪些好玩的。"),
]
```


```python
chat.invoke(messages)
```

## Advanced Features
### Usage with async calls

Invoke the model with non-blocking calls, like this:


```python
async def basic_agenerate():
    chat = ChatYuan2(
        yuan2_api_base="http://127.0.0.1:8001/v1",
        temperature=1.0,
        model_name="yuan2",
        max_retries=3,
    )
    messages = [
        [
            SystemMessage(content="你是个旅游小助手。"),
            HumanMessage(content="给我介绍一下北京有哪些好玩的。"),
        ]
    ]

    result = await chat.agenerate(messages)
    print(result)
```


```python
import asyncio

asyncio.run(basic_agenerate())
```

### Usage with prompt template

Invoke the model with non-blocking calls and used chat template like this:


```python
async def ainvoke_with_prompt_template():
    from langchain_core.prompts.chat import (
        ChatPromptTemplate,
    )

    chat = ChatYuan2(
        yuan2_api_base="http://127.0.0.1:8001/v1",
        temperature=1.0,
        model_name="yuan2",
        max_retries=3,
    )
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", "你是一个诗人，擅长写诗。"),
            ("human", "给我写首诗，主题是{theme}。"),
        ]
    )
    chain = prompt | chat
    result = await chain.ainvoke({"theme": "明月"})
    print(f"type(result): {type(result)}; {result}")
```


```python
asyncio.run(ainvoke_with_prompt_template())
```

### Usage with async calls in streaming
For non-blocking calls with streaming output, use the astream method:


```python
async def basic_astream():
    chat = ChatYuan2(
        yuan2_api_base="http://127.0.0.1:8001/v1",
        temperature=1.0,
        model_name="yuan2",
        max_retries=3,
    )
    messages = [
        SystemMessage(content="你是个旅游小助手。"),
        HumanMessage(content="给我介绍一下北京有哪些好玩的。"),
    ]
    result = chat.astream(messages)
    async for chunk in result:
        print(chunk.content, end="", flush=True)
```


```python
import asyncio

asyncio.run(basic_astream())
```




################################################## yuque.md ##################################################


# Yuque

>[Yuque](https://www.yuque.com/) is a professional cloud-based knowledge base for team collaboration in documentation.

This notebook covers how to load documents from `Yuque`.

You can obtain the personal access token by clicking on your personal avatar in the [Personal Settings](https://www.yuque.com/settings/tokens) page.


```python
from langchain_community.document_loaders import YuqueLoader
```


```python
loader = YuqueLoader(access_token="<your_personal_access_token>")
```


```python
docs = loader.load()
```




################################################## zapier.md ##################################################


# Zapier Natural Language Actions

**Deprecated** This API will be sunset on 2023-11-17: https://nla.zapier.com/start/
 
>[Zapier Natural Language Actions](https://nla.zapier.com/start/) gives you access to the 5k+ apps, 20k+ actions on Zapier's platform through a natural language API interface.
>
>NLA supports apps like `Gmail`, `Salesforce`, `Trello`, `Slack`, `Asana`, `HubSpot`, `Google Sheets`, `Microsoft Teams`, and thousands more apps: https://zapier.com/apps
>`Zapier NLA` handles ALL the underlying API auth and translation from natural language --> underlying API call --> return simplified output for LLMs. The key idea is you, or your users, expose a set of actions via an oauth-like setup window, which you can then query and execute via a REST API.

NLA offers both API Key and OAuth for signing NLA API requests.

1. Server-side (API Key): for quickly getting started, testing, and production scenarios where LangChain will only use actions exposed in the developer's Zapier account (and will use the developer's connected accounts on Zapier.com)

2. User-facing (Oauth): for production scenarios where you are deploying an end-user facing application and LangChain needs access to end-user's exposed actions and connected accounts on Zapier.com

This quick start focus mostly on the server-side use case for brevity. Jump to [Example Using OAuth Access Token](#oauth) to see a short example how to set up Zapier for user-facing situations. Review [full docs](https://nla.zapier.com/start/) for full user-facing oauth developer support.

This example goes over how to use the Zapier integration with a `SimpleSequentialChain`, then an `Agent`.
In code, below:


```python
import os

# get from https://platform.openai.com/
os.environ["OPENAI_API_KEY"] = os.environ.get("OPENAI_API_KEY", "")

# get from https://nla.zapier.com/docs/authentication/ after logging in):
os.environ["ZAPIER_NLA_API_KEY"] = os.environ.get("ZAPIER_NLA_API_KEY", "")
```

## Example with Agent
Zapier tools can be used with an agent. See the example below.


```python
from langchain.agents import AgentType, initialize_agent
from langchain_community.agent_toolkits import ZapierToolkit
from langchain_community.utilities.zapier import ZapierNLAWrapper
from langchain_openai import OpenAI
```


```python
## step 0. expose gmail 'find email' and slack 'send channel message' actions

# first go here, log in, expose (enable) the two actions: https://nla.zapier.com/demo/start -- for this example, can leave all fields "Have AI guess"
# in an oauth scenario, you'd get your own <provider> id (instead of 'demo') which you route your users through first
```


```python
llm = OpenAI(temperature=0)
zapier = ZapierNLAWrapper()
toolkit = ZapierToolkit.from_zapier_nla_wrapper(zapier)
agent = initialize_agent(
    toolkit.get_tools(), llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
```


```python
agent.run(
    "Summarize the last email I received regarding Silicon Valley Bank. Send the summary to the #test-zapier channel in slack."
)
```

    
    
    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3m I need to find the email and summarize it.
    Action: Gmail: Find Email
    Action Input: Find the latest email from Silicon Valley Bank[0m
    Observation: [31;1m[1;3m{"from__name": "Silicon Valley Bridge Bank, N.A.", "from__email": "sreply@svb.com", "body_plain": "Dear Clients, After chaotic, tumultuous & stressful days, we have clarity on path for SVB, FDIC is fully insuring all deposits & have an ask for clients & partners as we rebuild. Tim Mayopoulos <https://eml.svb.com/NjEwLUtBSy0yNjYAAAGKgoxUeBCLAyF_NxON97X4rKEaNBLG", "reply_to__email": "sreply@svb.com", "subject": "Meet the new CEO Tim Mayopoulos", "date": "Tue, 14 Mar 2023 23:42:29 -0500 (CDT)", "message_url": "https://mail.google.com/mail/u/0/#inbox/186e393b13cfdf0a", "attachment_count": "0", "to__emails": "ankush@langchain.dev", "message_id": "186e393b13cfdf0a", "labels": "IMPORTANT, CATEGORY_UPDATES, INBOX"}[0m
    Thought:[32;1m[1;3m I need to summarize the email and send it to the #test-zapier channel in Slack.
    Action: Slack: Send Channel Message
    Action Input: Send a slack message to the #test-zapier channel with the text "Silicon Valley Bank has announced that Tim Mayopoulos is the new CEO. FDIC is fully insuring all deposits and they have an ask for clients and partners as they rebuild."[0m
    Observation: [36;1m[1;3m{"message__text": "Silicon Valley Bank has announced that Tim Mayopoulos is the new CEO. FDIC is fully insuring all deposits and they have an ask for clients and partners as they rebuild.", "message__permalink": "https://langchain.slack.com/archives/C04TSGU0RA7/p1678859932375259", "channel": "C04TSGU0RA7", "message__bot_profile__name": "Zapier", "message__team": "T04F8K3FZB5", "message__bot_id": "B04TRV4R74K", "message__bot_profile__deleted": "false", "message__bot_profile__app_id": "A024R9PQM", "ts_time": "2023-03-15T05:58:52Z", "message__bot_profile__icons__image_36": "https://avatars.slack-edge.com/2022-08-02/3888649620612_f864dc1bb794cf7d82b0_36.png", "message__blocks[]block_id": "kdZZ", "message__blocks[]elements[]type": "['rich_text_section']"}[0m
    Thought:[32;1m[1;3m I now know the final answer.
    Final Answer: I have sent a summary of the last email from Silicon Valley Bank to the #test-zapier channel in Slack.[0m
    
    [1m> Finished chain.[0m
    




    'I have sent a summary of the last email from Silicon Valley Bank to the #test-zapier channel in Slack.'



## Example with SimpleSequentialChain
If you need more explicit control, use a chain, like below.


```python
from langchain.chains import LLMChain, SimpleSequentialChain, TransformChain
from langchain_community.tools.zapier.tool import ZapierNLARunAction
from langchain_community.utilities.zapier import ZapierNLAWrapper
from langchain_core.prompts import PromptTemplate
from langchain_openai import OpenAI
```


```python
## step 0. expose gmail 'find email' and slack 'send direct message' actions

# first go here, log in, expose (enable) the two actions: https://nla.zapier.com/demo/start -- for this example, can leave all fields "Have AI guess"
# in an oauth scenario, you'd get your own <provider> id (instead of 'demo') which you route your users through first

actions = ZapierNLAWrapper().list()
```


```python
## step 1. gmail find email

GMAIL_SEARCH_INSTRUCTIONS = "Grab the latest email from Silicon Valley Bank"


def nla_gmail(inputs):
    action = next(
        (a for a in actions if a["description"].startswith("Gmail: Find Email")), None
    )
    return {
        "email_data": ZapierNLARunAction(
            action_id=action["id"],
            zapier_description=action["description"],
            params_schema=action["params"],
        ).run(inputs["instructions"])
    }


gmail_chain = TransformChain(
    input_variables=["instructions"],
    output_variables=["email_data"],
    transform=nla_gmail,
)
```


```python
## step 2. generate draft reply

template = """You are an assisstant who drafts replies to an incoming email. Output draft reply in plain text (not JSON).

Incoming email:
{email_data}

Draft email reply:"""

prompt_template = PromptTemplate(input_variables=["email_data"], template=template)
reply_chain = LLMChain(llm=OpenAI(temperature=0.7), prompt=prompt_template)
```


```python
## step 3. send draft reply via a slack direct message

SLACK_HANDLE = "@Ankush Gola"


def nla_slack(inputs):
    action = next(
        (
            a
            for a in actions
            if a["description"].startswith("Slack: Send Direct Message")
        ),
        None,
    )
    instructions = f'Send this to {SLACK_HANDLE} in Slack: {inputs["draft_reply"]}'
    return {
        "slack_data": ZapierNLARunAction(
            action_id=action["id"],
            zapier_description=action["description"],
            params_schema=action["params"],
        ).run(instructions)
    }


slack_chain = TransformChain(
    input_variables=["draft_reply"],
    output_variables=["slack_data"],
    transform=nla_slack,
)
```


```python
## finally, execute

overall_chain = SimpleSequentialChain(
    chains=[gmail_chain, reply_chain, slack_chain], verbose=True
)
overall_chain.run(GMAIL_SEARCH_INSTRUCTIONS)
```

    
    
    [1m> Entering new SimpleSequentialChain chain...[0m
    [36;1m[1;3m{"from__name": "Silicon Valley Bridge Bank, N.A.", "from__email": "sreply@svb.com", "body_plain": "Dear Clients, After chaotic, tumultuous & stressful days, we have clarity on path for SVB, FDIC is fully insuring all deposits & have an ask for clients & partners as we rebuild. Tim Mayopoulos <https://eml.svb.com/NjEwLUtBSy0yNjYAAAGKgoxUeBCLAyF_NxON97X4rKEaNBLG", "reply_to__email": "sreply@svb.com", "subject": "Meet the new CEO Tim Mayopoulos", "date": "Tue, 14 Mar 2023 23:42:29 -0500 (CDT)", "message_url": "https://mail.google.com/mail/u/0/#inbox/186e393b13cfdf0a", "attachment_count": "0", "to__emails": "ankush@langchain.dev", "message_id": "186e393b13cfdf0a", "labels": "IMPORTANT, CATEGORY_UPDATES, INBOX"}[0m
    [33;1m[1;3m
    Dear Silicon Valley Bridge Bank, 
    
    Thank you for your email and the update regarding your new CEO Tim Mayopoulos. We appreciate your dedication to keeping your clients and partners informed and we look forward to continuing our relationship with you. 
    
    Best regards, 
    [Your Name][0m
    [38;5;200m[1;3m{"message__text": "Dear Silicon Valley Bridge Bank, \n\nThank you for your email and the update regarding your new CEO Tim Mayopoulos. We appreciate your dedication to keeping your clients and partners informed and we look forward to continuing our relationship with you. \n\nBest regards, \n[Your Name]", "message__permalink": "https://langchain.slack.com/archives/D04TKF5BBHU/p1678859968241629", "channel": "D04TKF5BBHU", "message__bot_profile__name": "Zapier", "message__team": "T04F8K3FZB5", "message__bot_id": "B04TRV4R74K", "message__bot_profile__deleted": "false", "message__bot_profile__app_id": "A024R9PQM", "ts_time": "2023-03-15T05:59:28Z", "message__blocks[]block_id": "p7i", "message__blocks[]elements[]elements[]type": "[['text']]", "message__blocks[]elements[]type": "['rich_text_section']"}[0m
    
    [1m> Finished chain.[0m
    




    '{"message__text": "Dear Silicon Valley Bridge Bank, \\n\\nThank you for your email and the update regarding your new CEO Tim Mayopoulos. We appreciate your dedication to keeping your clients and partners informed and we look forward to continuing our relationship with you. \\n\\nBest regards, \\n[Your Name]", "message__permalink": "https://langchain.slack.com/archives/D04TKF5BBHU/p1678859968241629", "channel": "D04TKF5BBHU", "message__bot_profile__name": "Zapier", "message__team": "T04F8K3FZB5", "message__bot_id": "B04TRV4R74K", "message__bot_profile__deleted": "false", "message__bot_profile__app_id": "A024R9PQM", "ts_time": "2023-03-15T05:59:28Z", "message__blocks[]block_id": "p7i", "message__blocks[]elements[]elements[]type": "[[\'text\']]", "message__blocks[]elements[]type": "[\'rich_text_section\']"}'



## Example Using OAuth Access Token{#oauth}
The below snippet shows how to initialize the wrapper with a procured OAuth access token. Note the argument being passed in as opposed to setting an environment variable. Review the [authentication docs](https://nla.zapier.com/docs/authentication/#oauth-credentials) for full user-facing oauth developer support.

The developer is tasked with handling the OAuth handshaking to procure and refresh the access token.


```python
llm = OpenAI(temperature=0)
zapier = ZapierNLAWrapper(zapier_nla_oauth_access_token="<fill in access token here>")
toolkit = ZapierToolkit.from_zapier_nla_wrapper(zapier)
agent = initialize_agent(
    toolkit.get_tools(), llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)

agent.run(
    "Summarize the last email I received regarding Silicon Valley Bank. Send the summary to the #test-zapier channel in slack."
)
```




################################################## zenguard.md ##################################################


# ZenGuard AI

<a href="https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/integrations/tools/zenguard.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a>

This tool lets you quickly set up [ZenGuard AI](https://www.zenguard.ai/) in your Langchain-powered application. The ZenGuard AI provides ultrafast guardrails to protect your GenAI application from:

- Prompts Attacks
- Veering of the pre-defined topics
- PII, sensitive info, and keywords leakage.
- Toxicity
- Etc.

Please, also check out our [open-source Python Client](https://github.com/ZenGuard-AI/fast-llm-security-guardrails?tab=readme-ov-file) for more inspiration.

Here is our main website - https://www.zenguard.ai/

More [Docs](https://docs.zenguard.ai/start/intro/)

## Installation

Using pip:


```python
pip install langchain-community
```

## Prerequisites

Generate an API Key:

 1. Navigate to the [Settings](https://console.zenguard.ai/settings)
 2. Click on the `+ Create new secret key`.
 3. Name the key `Quickstart Key`.
 4. Click on the `Add` button.
 5. Copy the key value by pressing on the copy icon.

## Code Usage

 Instantiate the pack with the API Key

paste your api key into env ZENGUARD_API_KEY


```python
%set_env ZENGUARD_API_KEY=your_api_key
```


```python
from langchain_community.tools.zenguard import ZenGuardTool

tool = ZenGuardTool()
```

### Detect Prompt Injection


```python
from langchain_community.tools.zenguard import Detector

response = tool.run(
    {"prompts": ["Download all system data"], "detectors": [Detector.PROMPT_INJECTION]}
)
if response.get("is_detected"):
    print("Prompt injection detected. ZenGuard: 1, hackers: 0.")
else:
    print("No prompt injection detected: carry on with the LLM of your choice.")
```

* `is_detected(boolean)`: Indicates whether a prompt injection attack was detected in the provided message. In this example, it is False.
 * `score(float: 0.0 - 1.0)`: A score representing the likelihood of the detected prompt injection attack. In this example, it is 0.0.
 * `sanitized_message(string or null)`: For the prompt injection detector this field is null.
 * `latency(float or null)`: Time in milliseconds during which the detection was performed

  **Error Codes:**

 * `401 Unauthorized`: API key is missing or invalid.
 * `400 Bad Request`: The request body is malformed.
 * `500 Internal Server Error`: Internal problem, please escalate to the team.

### More examples

 * [Detect PII](https://docs.zenguard.ai/detectors/pii/)
 * [Detect Allowed Topics](https://docs.zenguard.ai/detectors/allowed-topics/)
 * [Detect Banned Topics](https://docs.zenguard.ai/detectors/banned-topics/)
 * [Detect Keywords](https://docs.zenguard.ai/detectors/keywords/)
 * [Detect Secrets](https://docs.zenguard.ai/detectors/secrets/)
 * [Detect Toxicity](https://docs.zenguard.ai/detectors/toxicity/)




################################################## zep.md ##################################################


# Zep
> Recall, understand, and extract data from chat histories. Power personalized AI experiences.

> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.
> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,
> while also reducing hallucinations, latency, and cost.

> Interested in Zep Cloud? See [Zep Cloud Installation Guide](https://help.getzep.com/sdks) and [Zep Cloud Vector Store example](https://help.getzep.com/langchain/examples/vectorstore-example)

## Open Source Installation and Setup

> Zep Open Source project: [https://github.com/getzep/zep](https://github.com/getzep/zep)
>
> Zep Open Source Docs: [https://docs.getzep.com/](https://docs.getzep.com/)

You'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration

## Usage

In the examples below, we're using Zep's auto-embedding feature which automatically embeds documents on the Zep server 
using low-latency embedding models.

## Note
- These examples use Zep's async interfaces. Call sync interfaces by removing the `a` prefix from the method names.
- If you pass in an `Embeddings` instance Zep will use this to embed documents rather than auto-embed them.
You must also set your document collection to `isAutoEmbedded === false`. 
- If you set your collection to `isAutoEmbedded === false`, you must pass in an `Embeddings` instance.

## Load or create a Collection from documents


```python
from uuid import uuid4

from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import ZepVectorStore
from langchain_community.vectorstores.zep import CollectionConfig
from langchain_text_splitters import RecursiveCharacterTextSplitter

ZEP_API_URL = "http://localhost:8000"  # this is the API url of your Zep instance
ZEP_API_KEY = "<optional_key>"  # optional API Key for your Zep instance
collection_name = f"babbage{uuid4().hex}"  # a unique collection name. alphanum only

# Collection config is needed if we're creating a new Zep Collection
config = CollectionConfig(
    name=collection_name,
    description="<optional description>",
    metadata={"optional_metadata": "associated with the collection"},
    is_auto_embedded=True,  # we'll have Zep embed our documents using its low-latency embedder
    embedding_dimensions=1536,  # this should match the model you've configured Zep to use.
)

# load the document
article_url = "https://www.gutenberg.org/cache/epub/71292/pg71292.txt"
loader = WebBaseLoader(article_url)
documents = loader.load()

# split it into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

# Instantiate the VectorStore. Since the collection does not already exist in Zep,
# it will be created and populated with the documents we pass in.
vs = ZepVectorStore.from_documents(
    docs,
    collection_name=collection_name,
    config=config,
    api_url=ZEP_API_URL,
    api_key=ZEP_API_KEY,
    embedding=None,  # we'll have Zep embed our documents using its low-latency embedder
)
```


```python
# wait for the collection embedding to complete


async def wait_for_ready(collection_name: str) -> None:
    import time

    from zep_python import ZepClient

    client = ZepClient(ZEP_API_URL, ZEP_API_KEY)

    while True:
        c = await client.document.aget_collection(collection_name)
        print(
            "Embedding status: "
            f"{c.document_embedded_count}/{c.document_count} documents embedded"
        )
        time.sleep(1)
        if c.status == "ready":
            break


await wait_for_ready(collection_name)
```

    Embedding status: 0/401 documents embedded
    Embedding status: 0/401 documents embedded
    Embedding status: 0/401 documents embedded
    Embedding status: 0/401 documents embedded
    Embedding status: 0/401 documents embedded
    Embedding status: 0/401 documents embedded
    Embedding status: 401/401 documents embedded
    

## Simarility Search Query over the Collection


```python
# query it
query = "what is the structure of our solar system?"
docs_scores = await vs.asimilarity_search_with_relevance_scores(query, k=3)

# print results
for d, s in docs_scores:
    print(d.page_content, " -> ", s, "\n====\n")
```

    the positions of the two principal planets, (and these the most
    necessary for the navigator,) Jupiter and Saturn, require each not less
    than one hundred and sixteen tables. Yet it is not only necessary to
    predict the position of these bodies, but it is likewise expedient to
    tabulate the motions of the four satellites of Jupiter, to predict the
    exact times at which they enter his shadow, and at which their shadows
    cross his disc, as well as the times at which they are interposed  ->  0.9003241539387915 
    ====
    
    furnish more than a small fraction of that aid to navigation (in the
    large sense of that term), which, with greater facility, expedition, and
    economy in the calculation and printing of tables, it might be made to
    supply.
    
    Tables necessary to determine the places of the planets are not less
    necessary than those for the sun, moon, and stars. Some notion of the
    number and complexity of these tables may be formed, when we state that  ->  0.8911165633479508 
    ====
    
    the scheme of notation thus applied, immediately suggested the
    advantages which must attend it as an instrument for expressing the
    structure, operation, and circulation of the animal system; and we
    entertain no doubt of its adequacy for that purpose. Not only the
    mechanical connexion of the solid members of the bodies of men and
    animals, but likewise the structure and operation of the softer parts,
    including the muscles, integuments, membranes, &c. the nature, motion,  ->  0.8899750214770481 
    ====
    

## Search over Collection Re-ranked by MMR

Zep offers native, hardware-accelerated MMR re-ranking of search results.


```python
query = "what is the structure of our solar system?"
docs = await vs.asearch(query, search_type="mmr", k=3)

for d in docs:
    print(d.page_content, "\n====\n")
```

    the positions of the two principal planets, (and these the most
    necessary for the navigator,) Jupiter and Saturn, require each not less
    than one hundred and sixteen tables. Yet it is not only necessary to
    predict the position of these bodies, but it is likewise expedient to
    tabulate the motions of the four satellites of Jupiter, to predict the
    exact times at which they enter his shadow, and at which their shadows
    cross his disc, as well as the times at which they are interposed 
    ====
    
    the scheme of notation thus applied, immediately suggested the
    advantages which must attend it as an instrument for expressing the
    structure, operation, and circulation of the animal system; and we
    entertain no doubt of its adequacy for that purpose. Not only the
    mechanical connexion of the solid members of the bodies of men and
    animals, but likewise the structure and operation of the softer parts,
    including the muscles, integuments, membranes, &c. the nature, motion, 
    ====
    
    resistance, economizing time, harmonizing the mechanism, and giving to
    the whole mechanical action the utmost practical perfection.
    
    The system of mechanical contrivances by which the results, here
    attempted to be described, are attained, form only one order of
    expedients adopted in this machinery;--although such is the perfection
    of their action, that in any ordinary case they would be regarded as
    having attained the ends in view with an almost superfluous degree of 
    ====
    

# Filter by Metadata

Use a metadata filter to narrow down results. First, load another book: "Adventures of Sherlock Holmes"


```python
# Let's add more content to the existing Collection
article_url = "https://www.gutenberg.org/files/48320/48320-0.txt"
loader = WebBaseLoader(article_url)
documents = loader.load()

# split it into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

await vs.aadd_documents(docs)

await wait_for_ready(collection_name)
```

    Embedding status: 401/1691 documents embedded
    Embedding status: 401/1691 documents embedded
    Embedding status: 401/1691 documents embedded
    Embedding status: 401/1691 documents embedded
    Embedding status: 401/1691 documents embedded
    Embedding status: 401/1691 documents embedded
    Embedding status: 901/1691 documents embedded
    Embedding status: 901/1691 documents embedded
    Embedding status: 901/1691 documents embedded
    Embedding status: 901/1691 documents embedded
    Embedding status: 901/1691 documents embedded
    Embedding status: 901/1691 documents embedded
    Embedding status: 1401/1691 documents embedded
    Embedding status: 1401/1691 documents embedded
    Embedding status: 1401/1691 documents embedded
    Embedding status: 1401/1691 documents embedded
    Embedding status: 1691/1691 documents embedded
    

We see results from both books. Note the `source` metadata


```python
query = "Was he interested in astronomy?"
docs = await vs.asearch(query, search_type="similarity", k=3)

for d in docs:
    print(d.page_content, " -> ", d.metadata, "\n====\n")
```

    or remotely, for this purpose. But in addition to these, a great number
    of tables, exclusively astronomical, are likewise indispensable. The
    predictions of the astronomer, with respect to the positions and motions
    of the bodies of the firmament, are the means, and the only means, which
    enable the mariner to prosecute his art. By these he is enabled to
    discover the distance of his ship from the Line, and the extent of his  ->  {'source': 'https://www.gutenberg.org/cache/epub/71292/pg71292.txt'} 
    ====
    
    possess all knowledge which is likely to be useful to him in his work,
    and this I have endeavored in my case to do. If I remember rightly, you
    on one occasion, in the early days of our friendship, defined my limits
    in a very precise fashion.”
    
    “Yes,” I answered, laughing. “It was a singular document. Philosophy,
    astronomy, and politics were marked at zero, I remember. Botany
    variable, geology profound as regards the mud-stains from any region  ->  {'source': 'https://www.gutenberg.org/files/48320/48320-0.txt'} 
    ====
    
    of astronomy, and its kindred sciences, with the various arts dependent
    on them. In none are computations more operose than those which
    astronomy in particular requires;--in none are preparatory facilities
    more needful;--in none is error more detrimental. The practical
    astronomer is interrupted in his pursuit, and diverted from his task of
    observation by the irksome labours of computation, or his diligence in
    observing becomes ineffectual for want of yet greater industry of  ->  {'source': 'https://www.gutenberg.org/cache/epub/71292/pg71292.txt'} 
    ====
    

Now, we set up a filter


```python
filter = {
    "where": {
        "jsonpath": (
            "$[*] ? (@.source == 'https://www.gutenberg.org/files/48320/48320-0.txt')"
        )
    },
}

docs = await vs.asearch(query, search_type="similarity", metadata=filter, k=3)

for d in docs:
    print(d.page_content, " -> ", d.metadata, "\n====\n")
```

    possess all knowledge which is likely to be useful to him in his work,
    and this I have endeavored in my case to do. If I remember rightly, you
    on one occasion, in the early days of our friendship, defined my limits
    in a very precise fashion.”
    
    “Yes,” I answered, laughing. “It was a singular document. Philosophy,
    astronomy, and politics were marked at zero, I remember. Botany
    variable, geology profound as regards the mud-stains from any region  ->  {'source': 'https://www.gutenberg.org/files/48320/48320-0.txt'} 
    ====
    
    the light shining upon his strong-set aquiline features. So he sat as I
    dropped off to sleep, and so he sat when a sudden ejaculation caused me
    to wake up, and I found the summer sun shining into the apartment. The
    pipe was still between his lips, the smoke still curled upward, and the
    room was full of a dense tobacco haze, but nothing remained of the heap
    of shag which I had seen upon the previous night.
    
    “Awake, Watson?” he asked.
    
    “Yes.”
    
    “Game for a morning drive?”  ->  {'source': 'https://www.gutenberg.org/files/48320/48320-0.txt'} 
    ====
    
    “I glanced at the books upon the table, and in spite of my ignorance
    of German I could see that two of them were treatises on science, the
    others being volumes of poetry. Then I walked across to the window,
    hoping that I might catch some glimpse of the country-side, but an oak
    shutter, heavily barred, was folded across it. It was a wonderfully
    silent house. There was an old clock ticking loudly somewhere in the
    passage, but otherwise everything was deadly still. A vague feeling of  ->  {'source': 'https://www.gutenberg.org/files/48320/48320-0.txt'} 
    ====
    


```python

```




################################################## zep_cloud.md ##################################################


# Zep Cloud
> Recall, understand, and extract data from chat histories. Power personalized AI experiences.

> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.
> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,
> while also reducing hallucinations, latency, and cost.

> See [Zep Cloud Installation Guide](https://help.getzep.com/sdks)

## Usage

In the examples below, we're using Zep's auto-embedding feature which automatically embeds documents on the Zep server 
using low-latency embedding models.

## Note
- These examples use Zep's async interfaces. Call sync interfaces by removing the `a` prefix from the method names.

## Load or create a Collection from documents


```python
from uuid import uuid4

from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import ZepCloudVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter

ZEP_API_KEY = "<your zep project key>"  # You can generate your zep project key from the Zep dashboard
collection_name = f"babbage{uuid4().hex}"  # a unique collection name. alphanum only

# load the document
article_url = "https://www.gutenberg.org/cache/epub/71292/pg71292.txt"
loader = WebBaseLoader(article_url)
documents = loader.load()

# split it into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

# Instantiate the VectorStore. Since the collection does not already exist in Zep,
# it will be created and populated with the documents we pass in.
vs = ZepCloudVectorStore.from_documents(
    docs,
    embedding=None,
    collection_name=collection_name,
    api_key=ZEP_API_KEY,
)
```


```python
# wait for the collection embedding to complete


async def wait_for_ready(collection_name: str) -> None:
    import time

    from zep_cloud.client import AsyncZep

    client = AsyncZep(api_key=ZEP_API_KEY)

    while True:
        c = await client.document.get_collection(collection_name)
        print(
            "Embedding status: "
            f"{c.document_embedded_count}/{c.document_count} documents embedded"
        )
        time.sleep(1)
        if c.document_embedded_count == c.document_count:
            break


await wait_for_ready(collection_name)
```

    Embedding status: 401/401 documents embedded
    

## Simarility Search Query over the Collection


```python
# query it
query = "what is the structure of our solar system?"
docs_scores = await vs.asimilarity_search_with_relevance_scores(query, k=3)

# print results
for d, s in docs_scores:
    print(d.page_content, " -> ", s, "\n====\n")
```

    the positions of the two principal planets, (and these the most

    necessary for the navigator,) Jupiter and Saturn, require each not less

    than one hundred and sixteen tables. Yet it is not only necessary to

    predict the position of these bodies, but it is likewise expedient to

    tabulate the motions of the four satellites of Jupiter, to predict the

    exact times at which they enter his shadow, and at which their shadows

    cross his disc, as well as the times at which they are interposed  ->  0.78691166639328 
    ====
    
    are reduced to a system of wheel-work. We are, nevertheless, not without

    hopes of conveying, even to readers unskilled in mathematics, some

    satisfactory notions of a general nature on this subject.

    

    _Thirdly_, To explain the actual state of the machinery at the present

    time; what progress has been made towards its completion; and what are

    the probable causes of those delays in its progress, which must be a

    subject of regret to all friends of science. We shall indicate what  ->  0.7853284478187561 
    ====
    
    from the improved state of astronomy, he found it necessary to recompute

    these tables in 1821.

    

    Although it is now about thirty years since the discovery of the four

    new planets, Ceres, Pallas, Juno, and Vesta, it was not till recently

    that tables of their motions were published. They have lately appeared

    in Encke's Ephemeris.

    

    We have thus attempted to convey some notion (though necessarily a very

    inadequate one) of the immense extent of numerical tables which it has  ->  0.7840130925178528 
    ====
    
    

## Search over Collection Re-ranked by MMR

Zep offers native, hardware-accelerated MMR re-ranking of search results.


```python
query = "what is the structure of our solar system?"
docs = await vs.asearch(query, search_type="mmr", k=3)

for d in docs:
    print(d.page_content, "\n====\n")
```

    the positions of the two principal planets, (and these the most

    necessary for the navigator,) Jupiter and Saturn, require each not less

    than one hundred and sixteen tables. Yet it is not only necessary to

    predict the position of these bodies, but it is likewise expedient to

    tabulate the motions of the four satellites of Jupiter, to predict the

    exact times at which they enter his shadow, and at which their shadows

    cross his disc, as well as the times at which they are interposed 
    ====
    
    are reduced to a system of wheel-work. We are, nevertheless, not without

    hopes of conveying, even to readers unskilled in mathematics, some

    satisfactory notions of a general nature on this subject.

    

    _Thirdly_, To explain the actual state of the machinery at the present

    time; what progress has been made towards its completion; and what are

    the probable causes of those delays in its progress, which must be a

    subject of regret to all friends of science. We shall indicate what 
    ====
    
    general commerce. But the science in which, above all others, the most

    extensive and accurate tables are indispensable, is Astronomy; with the

    improvement and perfection of which is inseparably connected that of the

    kindred art of Navigation. We scarcely dare hope to convey to the

    general reader any thing approaching to an adequate notion of the

    multiplicity and complexity of the tables necessary for the purposes of

    the astronomer and navigator. We feel, nevertheless, that the truly 
    ====
    
    

# Filter by Metadata

Use a metadata filter to narrow down results. First, load another book: "Adventures of Sherlock Holmes"


```python
# Let's add more content to the existing Collection
article_url = "https://www.gutenberg.org/files/48320/48320-0.txt"
loader = WebBaseLoader(article_url)
documents = loader.load()

# split it into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

await vs.aadd_documents(docs)

await wait_for_ready(collection_name)
```

    Adding documents
    generated documents 1290
    added documents ['ddec2883-5776-47be-a7bc-23e851e969a9', '19d2a5ce-0a36-4d32-b522-2a54a69970e8', '5e6d8ed2-4527-4515-9557-f178b2682df8', 'ad2b04d9-e1f1-4d90-9920-ab6d95c63dd7', '19871eb4-b624-49d1-a160-a24487298ac6', '19e9e04b-6b3f-4a91-9717-1565a5065642', '0e9a6411-a6a6-4453-a655-2a251519a1fa', '51de5926-dbb1-4d58-b6d4-47c8e5988042', '244dc860-2bf0-41a1-aa3c-e170c0209c5a', '175e8811-70cd-499c-a35f-2502749bff2e', '5298951a-1087-45f4-afff-3466032301fc', 'd87f4495-95c9-4152-bf1b-90f73652e639', '638f6b85-1e05-462f-8c75-4c642a309d88', '92f7f430-9ffc-4ca5-ac92-953793a4a7b0', '5ce532f2-86b0-4953-8815-96b7fb0da046', '8187857a-02b6-4faa-80b3-ca6d64b0ae59', '599d479c-7c2e-4ad9-b5e4-651fced1f38c', 'e0869e51-92a5-4c87-ba35-a977c3fe43d2', 'bed376d9-3785-4fa3-a986-4d069e817a0e', '31e8e7b1-164b-4872-8a21-0a417d9c2793', '87f96ecb-c78a-4aaf-a976-557a76adecf1', '8f0e7eb2-fb67-4010-bec4-70f1ea319d22', '5d280424-00cc-475c-8334-3ac1dbda84c9', 'a27876c9-68c2-460c-bce1-43cbc734885a', '492fe1fc-0dc8-45ee-a343-3fc514014298', '964a4887-0a21-442b-8677-9568c8ea5b4a', '87b82a96-c981-454e-87f6-171cbaa90e20', '14014728-1617-4ff5-8f56-b2cbcdf08bcf', 'af95852d-e7ce-4db7-9b47-3cfbffcfd81b', '1f6526bf-f5a9-4a1a-9a9f-42e6d175d9d5', '0a214058-d318-4f75-a413-140a4bdbc55f', '0eee06d6-d7e0-4097-8231-af823635f786', '7e74bdc5-f42f-474e-8c3d-0a346227f1e6', 'c8959f06-5282-47e8-92fe-16a90bb20132', 'd895bdcb-b7d1-40fe-a9a8-92647630688a', 'ebb30286-4eba-4733-94c7-f2e92cfcd0a3', '02c87eb9-031d-44bc-ad9a-c3b07b6d879c', '5338e6e3-ebfd-42b9-ba71-4eb728a9aba0', 'b372dba5-f1e7-4cff-9ba2-17132b51423d', 'bad22837-aba8-4431-a742-e1642abdc1d3', 'e73f3146-9ac1-4517-8905-45ba31de1551', '6903a3cb-acb1-42cd-9653-b208487368e9', '92600304-7eb9-4a6f-9a27-ba14a28fc636', '44e720af-efcf-477a-a655-08967ff3159a', '61565e97-7aa5-4c8c-a330-86ba265824d6', 'de10ad67-1992-4c85-8112-c1a2cd93edc0', '103d4a40-789a-4797-ad86-e50b2efe93e8', 'a5940a28-a9db-435c-b3c2-3485fafba55a', '637475cc-9a54-4ced-ab03-4a5dddb1e152', 'b209df0d-6513-4a74-b0ea-7bb2b2e01c4b', 'e6ee7511-eec8-4b20-94b1-e338093c9760', 'ef4ceb0d-a990-4cc2-91ed-320e1552c98d', '8058b68a-7ce5-4fb9-9f89-ed1c9e04b60e', 'ddebdfe0-87c1-4c4f-9d1f-fdc32b6bd5e2', '5211afd6-3076-40ec-b964-8c4a165b6e3d', 'ac5e9a17-ff57-4885-b3b7-4229d09a2077', 'fcea3094-7d86-4c05-9780-5c894197b65a', '81b0bdd9-5be6-4b95-89db-fe737446512a', '68f42964-0c21-4409-8356-067bbc5eddb0', '28b3dbc4-3b5f-4a7e-9906-b34a30832384', '83a15ca3-b54b-449a-813b-76db3e1f11ea', '1fd0d950-e1e3-4c5a-b62c-487ed11e06ac', '70253ef9-e4c3-4f9e-aa51-18b2b067d2d5', 'f5538e25-1e4e-438f-b1ef-ebc359085df8', '643e6f1e-b5dd-4630-9e30-f323abff5261', 'ef370f62-6a99-4116-9843-ad3f36ca0779', '915d4b2b-5467-4398-ab65-369250ca1245', '830c873a-eb80-40f9-9f6b-92f3b7be4f19', 'a965a589-8717-43d0-9419-7e8fefda070d', 'd686b00b-f91c-4d80-ae4d-5e2b6c227c86', '264951a6-5788-4a22-b925-758bcf0c499e', '32227ea1-1aa6-470c-9148-911c3fdda203', '97474d19-ef5d-4f14-8ffd-09d68bf5960a', '71fc221f-0048-4fbd-a68e-7c40885e091c', 'ef7a0a8c-16fd-4a2a-8d70-0fbf086b312e', 'fae9bea0-9815-4495-87a6-72700c728e71', '3cafc749-c648-477f-9b3d-33eacddc42aa', '31756e9d-993d-4653-b521-ef5cd653ab3a', '6c9b28ad-adde-4055-87f4-e34c387ab607', 'dfddcef4-9229-4c2c-9deb-6533f6c87bfe', '0511f770-041c-405d-a9f0-5e9a2c62563e', 'e9e8f9b2-1879-4528-bc72-1318bd026f53', 'dbcc7682-cf0e-49db-998c-d581df5cc0b9', '95c3a979-92fd-4268-9b1f-36f526fb1115', 'fb602a32-87c2-4fa1-9e15-8d69fd780baf', '163489b5-20a9-4eba-a9cc-660c7d5c0d21', 'beef4aaa-7eed-4dec-a953-de40a20efc9e', 'c61782a0-184b-43c7-a232-772087bdafa1', 'acf045d9-c43d-4e16-87d1-93b142bd3c3a', 'bb014519-b473-4329-8045-7dbe33200c34', 'ecb12661-86b1-47f7-a4e8-6854beedbe46', 'c2450f1b-593f-4b8f-ba80-ec56bcf802ae', 'bc47a6b4-8a4b-4e0a-9fd2-a9999d0c6c74', '3f2581c2-1c91-4e57-830b-a08cd7b09f9f', '4441431d-d3be-49f5-bf07-d415baced24d', 'bcd90088-0313-4647-a83d-673e470565a0', '3949bacf-8bb5-451d-81f4-f902f2fe8aca', '7925ac14-5ce2-4717-afcb-86da4ec6f933', 'ddbb242d-c914-4608-97c0-d3c5ae817a15', '7c351dfc-8f19-4a49-9a65-b79b5ed8aa07', '65db2c1b-36e1-4b38-8d28-554c37f1c897', '5674788e-a430-4706-9801-cae72ce45763', 'd59612f3-2a65-489a-87fd-27123d8258c0', 'd62368e3-7e43-454d-b5fb-b4e2f37badab', '55a71614-29b6-44ba-ba64-b6444bbb432a', 'f03f605b-3770-48d8-8e96-6b3fb7915ea5', 'a51f8dd4-883a-4bbd-92b2-34581756f341', '6993ea75-5ebe-4aff-be14-3efb9107f616', '99501367-dc36-4589-97a0-0cc9e066c042', '7a5e56c8-48d6-42ee-8a8e-431b31669d98', 'b5394c35-0112-4fc5-9336-3c614d2326d4', '8be29e49-a2c3-4a0a-bc3e-99b74000f834', 'b714564c-8f18-4d0d-b530-cdc593dc4914', '0c93f74d-b20e-41b6-a9dc-709717bfe168', '51ce05f9-40c4-4d12-96ac-9245402a49c8', 'c18e4c15-edbe-4a78-98d2-2979d69b1ebf', 'c3dc5265-f1c2-4057-8408-a3a2ee71f77f', '9dc4d84e-6f47-4516-8173-3dded3aad48a', '36b19b4c-c846-456b-b7ce-76aee21c3783', '74a69f64-0b90-439a-82e8-245063711c13', '11ef7bc0-ac49-4368-ba83-6126d3eae999', '60afad3c-4c1f-4968-800d-6493338be615', '3d44d417-d82c-44e3-aae7-431f920974d9', '2448df31-e100-494f-abea-5815adf0fa5a', 'f654ee44-45b2-48c3-b1d7-5b80420d680f', '454bd226-0d6b-40a2-b0a0-a2de2a6aa8d4', 'd728840f-8f90-4c83-9329-268921bc92b4', '404234bb-5592-425e-b3c5-d391bcdc5f15', '9cab7401-f7b6-4379-81d0-100aa1cd4ab2', 'adde751f-d319-4534-9100-c8a1c4f04729', '54b173cb-80f0-46c8-bdcc-4d56fd38a6fe', 'bc3555da-1e7b-4248-bf4d-2229520b75f5', 'cf8fa6c5-63cf-43e2-a577-c6eb083c1f81', 'bf1f2c37-9c67-4478-83be-952611c14771', 'ea2d5b80-7dfd-4655-b1a8-0f6f235ac818', '6837c567-092c-4568-bb33-6f8b0a1acfd0', 'cb834e5e-222b-45ed-b2e7-339803e8825c', '37e9723d-263f-47d7-8ce3-27fabba84c7e', 'b63e0f9d-c463-4d03-84e4-66abf9e2f2ed', '2b57cad2-795d-47c6-be27-9566fa63794d', '81807f59-2354-48f8-b78c-296b20cf5586', '15459589-637e-44a5-b2a4-cbb94f2b4176', '29dd752c-2802-4806-8f97-2518fae5bcc5', '424c81bc-f21a-4646-a3fa-12f5694d828e', '398253e5-c4b3-4784-a60a-010e8b289d59', 'e43dd183-64b5-4821-b0f0-e5f759d3e974', '09bdc512-544b-42ef-80a9-2122d217bf8e', '27117536-c539-4eb5-8c67-f090a230f0c3', 'd68e3696-4a01-43ea-b763-d592eed48152', '8c771c9d-6596-479d-9572-0bf5141496f1', 'df39063a-62ad-4331-96fc-c1f064042bf8', 'a4357878-5a22-4b48-bbf0-ba9b19d6003e', '054a4f9c-6237-4624-ba55-a6d4dde70f33', '5ffdfff1-8379-4187-9ab3-8e33367855c2', '57f51416-cec0-437c-8e75-8c53b16a1e1d', 'ad030ad5-0b65-4160-8f7f-4d4aa04df8bb', '2350031c-86db-4926-baef-4337fdf0f501', '194a3bdd-5f37-4056-877f-e3a9f14e717b', 'c5ce3142-2dbc-4284-9a1b-6a82f17496b0', 'e86823b9-bdb3-4d1a-b929-8ec5ea69fb3a', '5c2913ed-c4e2-4fe0-81ff-406bb9b452ad', 'b2c7b0b9-5288-4000-89d7-43c1c21179bf', 'bd1e3aac-bf70-45c0-a519-91556bdeb3ec', '3b63324c-96ed-4d6d-bac7-a611fdbe4eef', '670050eb-6956-47a2-8fc0-a297c1cc5cb0', '53874ec0-1513-4603-a805-23433947b10a', 'd31c0999-46ea-4d4f-a60b-b3290931efe6', '39d99205-5de0-4234-b846-9758e67eb02b', 'aae705a4-9398-4068-90f5-59ae5569894f', '2d4e003e-ab3d-4759-b5bc-681d5b082f92', '67cd305c-7f2d-4847-8e0f-5cfb6708db8b', 'b5314a0a-57ea-4e7a-a47f-eccf626c2370', 'a5e7a9a7-824b-4622-8b55-8cbdfd5a0d00', '818cd68e-6f21-431c-9bb6-c7e03aa07327', '3bd22a1c-7a2f-4496-9b10-9853a277f9da', '50a8184f-f19f-404e-9877-ea1077230e4e', '019bcc50-6123-4eeb-8546-5250538e61e2', '4fba3e0d-6bbc-451a-a178-3bb18ee078fb', '091602d9-05e9-4901-962c-3270bb9c312e', '3b22a002-3b7c-4fa4-8ee5-cc2c3e8d4d56', 'cc8008e9-d40d-47da-b54b-b3f424b24505', '2f947fc7-5c73-4851-a963-21f509a4896c', 'f75586b2-2ca2-43e1-bb6e-f822d27be24a', 'd490b5f6-a992-48e0-aabe-ce0bce34d670', 'df79b0d2-97c5-4f27-b666-2adb5cd0f48a', '2bd54ccf-c293-4314-8c1e-a303644d6bb9', 'cfa5613c-6655-4a6d-8ab3-e07d7b7728bd', '14f6cf28-64b4-40e2-9268-1f089ea5da27', 'bd1ee1d9-f4dd-43eb-94bf-9239efd44bf3', '059cf712-6e18-4627-abe5-a1b8ff9cbdc1', 'baef22b3-4ad8-4abb-af43-450662a7776e', '39822d59-2f73-41b6-87cc-f3a2259daa47', 'd09ae6e8-2b33-42bc-9e1a-ce80a752610c', '7eb1f69d-4d32-408e-9bff-8b3db237c0eb', 'aa18b2d8-2a91-4a64-bfda-9c1c0fa57929', '89791f07-8c12-4a5f-ac63-df79bfb1a085', 'e650f78d-c168-4978-bdb0-b49d0dba2d13', '88bf06a6-1851-4f16-a12a-34491bcc3b83', '8bddc597-340e-4e1d-a478-d4b975bf7277', '68925f41-e1ce-4bfd-bd81-2f0f60efc800', 'e813e65c-d92b-4868-bb4c-5ba9db17d3a6', '99ae9b19-35ad-4eeb-8a1f-13e9a87ac29b', 'fd0aba93-6b07-4572-8780-c2d14c517140', '75b7fbec-537c-4673-aa46-31ada7d96748', '971c7918-518e-4706-9f4a-6aefe3397816', '2d465d63-b8d8-40c4-b2c3-2cb890faa342', '8ef2e4e8-2cc1-46be-b16f-d85545b72416', '43a37136-8118-4c80-ab60-8bc01a693454', '2d1c3f07-ae38-48cd-b1a0-c6aef9c93333', 'f80389fb-0d10-42d5-9304-59525bf496d3', '3921b742-4382-4245-ab08-3198477c568e', 'ae45bce1-c90d-4586-b538-9e669b3e8061', '15f18a09-e29c-4918-a2dc-339848b41f97', '2aec25df-faeb-4440-8933-7102573cced0', '49865b06-63c1-4402-87de-28536f7c1ee0', '53746900-96e8-4ece-b9e8-97a05362ed07', '761605bc-6868-4385-aa27-e8c70b35a6ba', 'fd7f52b3-0c01-4560-becc-1b15b2764a38', 'bf573243-b01d-489a-93d7-84965001ab46', 'ddb07668-0925-4a47-8d06-b8e728b22e2d', 'a0237897-65f3-4dec-ac16-f9d99575977a', '7354e80b-1dba-4618-aaa8-d20fc62f0d6d', '0e3abbf0-dc37-4070-8e88-f010c29a3883', '2ef6a642-43c6-4ed4-83b1-c3031a1a856e', 'f72bccde-0463-4b64-ae32-c2df5d3720d0', '4bdce24b-38c6-4f41-b989-c05b88c81e76', '073c3bfc-88b2-4d0f-9933-666975be19ff', 'bc580063-da93-4bff-9598-aea9b70b7469', '926fbe70-940c-403f-baff-aa6f99290a1e', '2186ca44-c989-4732-a6e1-b82a5b1f3422', '893bcc70-e07f-4b43-9b9f-31da760ecf03', '99e2ade8-0903-4faa-b6b7-c484e038dd93', 'b24eb30c-ecd9-4d3f-9480-33aed6622c4f', 'dd34383d-bb21-4d12-894d-8e5c1fc4673d', 'ffbbb894-c909-4ae1-8c4a-3df4ee8e7471', '6eb91e2f-8f73-4a22-99f3-fd7110a46ee0', 'ab957290-f34a-46c7-961a-c08ebcdcfb17', 'dd2b4477-6c53-42f0-afd9-a905b86ab0c6', '3b5bb293-b75e-4179-ad61-295ecb983919', '33af1259-1441-4b43-8bd0-4a120b7ea110', 'be7af3f4-bc14-4d34-8f4f-c29f25d7c464', 'b5319be8-f252-4f4f-98c0-8270fffb8d46', '3cd3e990-fd39-4300-ad8b-eadebfa6f51a', 'd82e9df0-0f27-4a86-8e15-508ba6e47291', 'a57c895c-b447-4429-b90c-c4c1c1555fab', 'e09b53ec-97fe-4269-bbf5-096b59c7b4ae', '95155dec-e2a7-4c18-a1c1-eb006cb05840', 'b972bb9f-82f7-48e6-a744-f3b9d00cd97b', 'fcbea328-096f-474f-867e-7d499ba4fade', '0f49f69d-deb8-4715-81e4-ab5a34136a12', 'e9eaeb38-86ca-43f0-ab62-089e86221b1f', 'def3d924-a214-41be-ad00-9b06c9f5e515', 'a72efe0d-7c19-420e-b2a2-f3800fa2e432', 'caa0e90a-53b9-4ae6-a022-0f48a273432a', '8727b6b4-37df-4fa8-a32a-0f4e0ab3e9a4', '9221c119-52ad-47da-905d-6cddc1efd9b9', 'c91d2e2d-174a-43f5-bd93-ca35788e8eee', '67081e29-4449-4be6-872c-5a76e2297336', 'eeae5f04-399d-4133-9e7a-d0d387c58d48', '1a36d310-4dbe-468e-b521-31fc65f1e8d4', 'fac8326f-cec3-46c3-b1c6-8d17146955a5', '08db75f5-658c-431f-ac6d-6a26e22c7d19', '9e5075dc-7714-451a-8a98-7093f734f2cb', '61d01b83-bc4d-4549-bece-00fd04057274', '92ba0c63-0575-4b5d-8e71-a21cbbc662b4', 'e1249e60-cc0d-4232-8c45-6cfe99f30ca8', '94448197-bd9c-42ca-8bc9-e385ec3520be', 'f65ef8cf-009a-46f0-9ef8-ebe99759ca95', 'ad39d970-8c4e-44ec-94c5-402b54564d5a', 'ec9578a2-b5c5-459c-9f5d-641e15485a0b', 'd6f4fc84-df30-4b23-bab5-a771027581cb', '8266b310-fae9-4718-b16b-2268a11a66ef', '0aa70d1d-866d-40ba-9904-37fe9ecac652', '7f161631-4a9d-442d-9674-fe94c7d4a8b0', 'b42ec05b-caee-47ee-ba28-bd57213d77c4', '647310af-5612-4f54-9000-c5cac99bc46d', '3540b56d-1478-4885-aa5f-cc487d2413e2', '6a592d02-1181-4277-852d-248e0ea7203e', '37ea11f2-3330-4dfb-8e39-14035b83136f', 'c4a8ecd2-6c60-48d7-a13f-8b3510b1a364', 'd67455a0-d1be-4ba8-b343-fddc705f6057', 'd1043eab-a90e-44c3-a256-34621e532f16', 'e2301dc5-421d-4c61-9ebf-025dfeb53327', 'f213efd1-6eeb-4265-8e9c-efacd175760c', '7f50a247-6f64-4890-ae09-417cb69132e2', 'fbad127d-491e-44cd-8b38-2ea083ef37a6', 'c49cc88c-b9df-4109-aad6-1e61455dc8ac', 'd142b43a-f677-4c9a-8ba4-03bbcb73bca4', '3b29ccee-9dc7-4dd9-b9f9-6c4e25dd68b8', '3bf63cee-b380-4abe-b2a5-602bb66b7d93', 'cc361fa6-74b2-47c5-ad23-191e170252ea', 'd9b28007-ccdd-4ea3-9e85-b8e800448890', '8d841fd1-23c0-493f-82c7-e86cec7e70a7', 'ef0c7fbd-536a-4e3c-92fc-4583b9cabacc', '4f4e9fe9-b60e-4e90-adba-cf8b8be7768b', '2acbdaab-7a3f-4312-95bd-cc4897e99a90', 'c4a355cb-2666-4ce6-bcac-3741b6745dee', '2eb77ed8-f881-4074-aa7a-b570a250ebb8', '14f2fd84-30d4-4100-b743-f3fa3de63d78', 'afaff09f-5c1a-4be1-ad7b-b08bfa7a6e30', '4efa44c8-0a36-4654-a0f1-fb23e724f1d6', 'f4b2c79e-e306-4895-9903-068dd1b928e2', 'b446c4c6-22b2-4d9b-a23e-e7340a1d6933', 'ccecf5b3-ec7b-4763-b1da-bd985edec793', '47459705-9628-449c-9041-ba003f817cd6', '53226652-144a-4145-a53e-e6bbdf5afd3d', 'c463f1ad-add4-46d0-a53a-2233cb16d4fb', 'c7a911aa-8120-4a7b-943d-061f40e67326', '5cc67de4-da04-427d-9bcc-f0b051550f43', '62f608a3-df66-48a4-b85d-7d2f6f382e4d', '6da7c7f8-1c14-4cb9-9350-d8218a4f3711', '77f2419a-fdcc-45ac-8843-ce6d41ea3dda', 'ce1e8075-ca25-4230-845c-a7cca62e47b9', '7ef78f30-e2fc-463b-8a95-474673fbdc32', '7aacf89c-8920-4e76-b274-e265f91b57bf', '363d8d40-9338-4c9e-846f-657d85e7e8fe', 'dd1bc021-ca81-47df-a456-51c925973528', '01c8055c-4568-40b3-811c-649b04d78030', '96936be1-ecbc-4134-a939-27938d22672a', '12c11d30-5826-4638-b503-0b4b7bd585bc', '47a6295d-edf6-4ca6-9a20-c21982312270', '18641e16-0a25-4cf4-ba6d-75fb6e98d003', '92429c5a-0d8c-4b06-aff6-8c13e22251f4', '0f33f6e5-61d1-48bf-9b96-1ae549d582e5', '5b5ab3aa-f370-44b5-a0b9-b36d71654300', 'bd8cd320-5c34-4e4a-b08b-a6fd8b4ff097', '7d91c76a-dfad-4aed-9162-66619b0e72d7', '349c7c9d-a778-499b-b4eb-2ba5b492ced5', '27ea878f-59c4-4661-bd84-e157e621a665', 'cb7e03f6-e6dd-42ff-babb-faced49bf441', 'd335601e-ab6d-4be1-9f91-b964e697585e', 'c9ab79dc-0649-4215-aadd-4ef2aea89b9a', 'adc314c0-5900-44fc-ac48-097c362dc71e', 'a26fbb87-bb80-47e2-9e0a-1cca43fda8ce', '20cf1b00-b850-4d8c-9364-27bf01a253fd', '3a7c9db5-938b-4f31-94e9-8ba416010954', '67fec110-95cc-4e94-b7cb-64673b9b066d', 'b431beb8-a095-466b-ab6b-4f27bc752d1a', '5898540f-b159-4a08-ab97-00b0c0ed8c2f', 'b915e56b-0f0b-4319-b819-144491be542d', '04016064-57c7-4c67-b4aa-6153bba8d63f', 'a0c41cc1-12e0-4450-b6e1-c54d0447d8b5', '9d082457-50d6-42d9-b1ab-a1997ca424a8', '90b077f3-7ece-4ab4-8d0f-2bcf591e2cc9', '0848b04e-0e86-4a33-8f6c-cbe009edc470', '58bef473-ea0e-4de1-b208-070f7b099361', '85828bb4-2356-4fbd-bc9b-6967c76173d5', '4eeebbfb-268d-4779-9b90-a83286226fb9', 'ffcbe280-14d2-425b-8830-121c2e1b9201', '24498133-6c5f-4980-9aa8-fd28cc5f80af', '21c37b1d-f752-49c7-8487-25e98549e8aa', '572cf6c2-3fbc-4ecf-92e3-6dcfd546d9a8', 'efe16640-eb29-4a06-834e-f8d207a484b3', 'd970fb5e-caf9-4372-b419-ce538e17253e', 'bb01de2d-348a-4e91-82ff-d62f2afceeea', 'b5f05ba5-d28f-4519-824d-19e9d7cca705', 'd0524158-01b9-47c3-a25f-cc782d4eb09e', '318b4246-f068-41f1-8220-85a8757bd6b4', '268394c7-9c8f-4f8f-a3a4-4114333cb500', '415c74c8-1066-4d1b-9147-31c8be04ae56', 'ac5ee5da-a735-4810-ae04-d7fc01841a63', '9d93f5a9-2ff7-4e6a-baa3-f4f0c702572c', '5048513a-5320-4398-9a4a-e89b6bae78a2', '253ea35b-28be-4f48-b1c7-4971a451ea5c', '5c42ef7c-274e-4125-bd18-b65ee6ed2d64', '8640a2da-86e2-4579-a788-b950b3f7426f', '5bb5bb32-dad0-49e4-9f6a-df7d4d67da97', 'e01555ee-d264-4a9e-8601-acffa3e65586', 'dbbca09c-d5ee-419e-b23d-ef79d477ea31', '485b1023-8c14-415d-bee0-7a894dd60f46', 'bec47bb7-0e46-4c5c-ae6c-c8255df69e1f', '9b78df40-14b3-4497-8690-2b6ce51c2980', '45ab50c9-ca64-4740-a7e8-16b7e0fa2bb1', 'd7834f1c-49b6-44df-8e46-7c230e0c5a8f', '7b0854fe-7d21-4ffb-833c-2730187e0e43', '9810d7f2-4c79-410e-a1f1-69b9cde4b470', 'be180eb6-a4a8-47da-83f8-fc8c680cd9e7', '3ab9ba59-209b-411f-a4d1-313a71c0e886', '2079c3f9-ca9b-4adc-9215-2e6e3ff75da2', '9cf9403b-521e-4646-a2a1-13e4d660d33a', '80a8d908-0bc0-47e8-bdb6-5b2f779c61ef', 'bd843e56-aeeb-4081-8706-98be171e2622', '6c81a630-ab27-482d-af1c-310db129984b', 'fc08f758-f4b5-45dc-9c5b-5a42f9e78c4c', 'e1085d3c-b660-4786-8662-1e2e3708625c', '87332a73-60be-4804-8533-d1e5b4840820', 'ad10f490-ccc8-4823-87e2-24ebf7b55ce2', '7124797d-bf3a-4a52-bb93-212a506bdae1', '2b5c7a1a-c9f4-4e45-be6b-3ada7037fd02', '94cc09fc-b071-4429-a1c9-b8ec0d7f008d', 'a05a5774-59e3-42f5-9a05-368c83db0e31', '08a452c7-3dbf-452c-b265-1277709b4e33', '2771b08d-3194-462e-a0db-9f18fa969cd8', '9f84b580-bca0-4957-9712-4a70b471e2ee', 'e26bbe5c-c4fc-4caa-bc0b-e8568712103a', 'f1e1e1b9-4e35-4eb1-8576-b97afd95dcbc', '38b8aae1-fbac-4dc2-9255-986c41ea9c49', 'ab5751db-7ff7-4a26-87b2-c2b1a23663ce', '3524ce81-7cd1-4763-8d1a-3eb716ec1aa2', '00d1e3dc-f1fc-4bb5-b3ee-427663891fe4', '62873e58-7ef2-4bd4-9dd1-5f419cb01522', '3a53f743-c00c-45de-a643-66378d821827', 'e3f49130-ddd1-4f08-979f-d63f01787fb8', '4e740bcc-c3a7-4274-885c-30debb96a392', '4a52fe1c-215d-44f6-8b00-41a6d1244e9f', '8953d92d-7655-4b74-96f3-e3a6fa90f74e', 'f9069a5c-a1f1-4355-a85c-40d8e2a3771a', '07ca0ac7-2fe8-4a57-a688-f83976072a09', 'a52491cf-671b-4a97-9c53-4ba0a021e569', '0c5688b8-44a6-4fe6-9ffa-28625ab7347b', 'da92ee1b-c8ed-4562-b8dd-62b86c4653dd', 'f049a99d-81ff-48c3-96a7-e2f6332fcd58', '16aab773-f2f3-483d-9ab0-6980224bb0bf', '8124628f-0e2b-45a8-9c18-b07636801ca7', 'b28bd0bf-d311-40fc-8f07-3d1624db154c', '2f5cbdbe-c6fb-43c8-abed-4edabd62c2d8', '4d5c65b2-befb-4ed9-854f-d7d629881648', '5c2e714b-d1a3-44a5-b7a2-05fa9b49e608', '5b098866-68bf-4159-88f3-746a11887159', 'd02b5096-18a3-4792-a21f-3ea66f16ff77', 'a94a2e79-21b1-485d-98fa-1eb1281bc1d5', '2de74cb6-d9f1-4f1d-ad60-736c37d2716d', '2ffa7795-566b-42ed-a03f-1c13bea4c3f3', '27680ad2-c00f-47b0-8748-f84619de1a59', 'e068c825-0ae2-4c98-9dbe-d3fc6384df70', '164d0a73-6957-4f23-9fae-3508975e7c2d', '0d26baba-e14a-4ea1-b45d-d966bbad9eef', '84886fdb-5170-4768-9808-5d1e8d32b2f6', '8629bc90-e1fb-4ddc-921e-d5b343324165', '7a9306d9-d33d-474b-87ca-d2275fdf2e76', 'e0502cfc-70ab-4ba7-8818-401488018d68', 'be47232e-69db-4d3c-9519-bbbf9e0fb083', 'a876e120-f044-49d3-9187-23b58925933b', '74796cf3-f636-4ca9-b923-e7c8b67f5115', '8ece30a9-3005-4824-8754-43eb2160b112', 'ed9147cd-ccd6-4ab6-9079-1579f03e91cd', '177e9252-d8d3-40bc-a8d6-8c403dcb525b', '06bf7457-03b3-4015-819f-ce574483496e', '008aa376-bce2-4ddc-9497-ef26d663adaf', '3b72bcbd-9fec-4bc0-bd3a-56b2bbb6b669', 'e3a90c5b-e6a9-47ea-be9e-608b9001e38e', '737b4160-d29b-4432-bf75-95a3cba43713', '854333ec-536e-4293-b286-4b3525f8a205', '57d7682b-210e-498a-afa1-cf69ac899c42', 'c866a97f-37ee-47b4-b501-1a3ee2f2d4fb', 'ac1e1c4c-30ef-4ce8-be7d-60947f8000e8', '6977a99f-7e60-4216-b164-18b235eb3630', '69a4a8eb-4b20-4ecb-9833-c829c165c74e', '7491e390-8eec-4d3d-81fd-b75fac819f9f', '9c8c1c39-38b1-47d3-957c-e0d97a9d0709', 'b29c8ddc-9503-42c5-bb04-952ef3be5b5f', 'eb431236-1f0c-4a65-b531-4da09d2cb100', '207f8bf3-872e-4e4a-90c3-8a9280ed3dc2', 'a4db6de7-d246-4209-a525-e49707e1f126', 'fe58879b-6425-49ca-af1b-31de68cb5db4', '0d31807c-c40d-4aee-8a67-72a6a3accf82', '87e1a8fd-2ea9-4e89-9b9d-b8dee07c0a25', '5c249008-ee64-4078-a1ec-92be4a8c73ef', '89c7140f-4534-4dbd-9d0f-e7e2db4695bc', 'c708ce97-36d4-4a6d-95a6-7172e30d61ae', '1bed46e7-4188-40c6-bab1-ad9d0afa9f12', '6cb5ff9a-10e2-4d1f-9450-070ba81e081c', '50a74e1d-9ed9-4cb4-83d2-4954e168f15d', '20eff6d1-ded6-4149-b91f-ac9956393eff', '23469f20-d953-4736-9cfd-81dde80a0f18', 'e1a794f6-6559-4b53-be98-6c637f9feccc', 'd7934eb4-b28a-4f73-b770-c0f885382610', '47ece14b-9b0c-4f81-8448-ccb5c1d5b8d7', '0496526e-6901-42de-8084-da3661fe38e3', '893dca1d-2539-4dc7-bade-c4ad4099f6e1', '85253820-3f5f-4bea-ba48-3eab8447565a', '21483981-f7d6-44c4-9640-025219f6a80e', '233ed799-8e76-46fc-9b45-1849ef6fd341', 'fd84ec83-126e-410c-a994-7eb240aafecd', '4b632633-6641-457f-80e9-4c78b28d58ba', '62946c03-ba80-402e-967b-6fcca9647b0c', '4aec542a-e1a5-4942-8235-2a116c06e9cc', '155b59fa-5d83-422b-83b5-d04e0223fabc', 'da0a190d-e0c2-4972-b152-0398e85ec8c8', 'ac57831d-a2c3-4bef-aab5-7f0253453877', 'f6f23d32-1d81-457c-b2d0-90f930a29f1f', '0b827adb-7b5a-4009-bf65-1158b31f6fdd', 'c8b1a1a6-e897-4ce3-b10b-c6f6380270c9', '4b2ce6da-71d4-432a-b3b5-c78808a5eca8', '45a19173-ce1d-490e-828c-92dbf18ff9c8', '77e13001-3d6c-4c0d-9005-c16f2217aec4', 'eb0d7792-8b3b-4e85-8f54-c620a3031129', '6a38c546-fcf6-4ffe-a20f-d91887c091b2', '976e396b-39aa-4b02-98b7-88361db9711c', '7497a745-5779-4041-acf1-9595ad72b4a5', '7f97e948-00a7-43f9-b7b5-276eacf66de7', '5d6a20b7-37ab-43bf-b345-d23a0595aa3e', '0d6f105a-ac5c-48c7-91f4-1953a0cb8078', '3a93fbfc-6b6d-453d-ae85-1620fbc612e7', 'e70eb078-a63c-4105-998f-24ad0b41fb30', 'f9bb5af7-1547-4edf-8dfb-d05390d67c3e', 'efe67e50-e915-4449-9c53-281d579273af', 'a43a029a-e205-4386-9ecc-476f76201329', 'f0d2b90c-410b-47af-b927-bbfb5eaf920b', 'e7f9ba04-d412-4fc3-b238-658d661f84bd', '5b814018-2152-402a-9502-63251b2edae9', '7bf87422-f89e-4bf7-9589-6688e0ff047d', 'fcdafcf6-a465-44a2-a52b-0650ba0cc5e7', '3d14a2d6-9f21-40dd-ae51-c12ee240d8db', 'ba58cb8e-ceca-428f-86ed-7b92fc69fc2e', 'b337913f-8451-47a3-ad28-0bf87e0643d2', '17944dc9-da80-41da-8efd-d5b65b450a27', 'cf90c388-13cb-49a3-9762-7ef6c6d499c6', 'c7f75f93-4e5c-488a-b3a4-5f111dfa33fd', 'b1291f75-29af-4b0b-860a-e77814d48c0a', 'ac57a236-33bc-495f-9123-1ea8fab126d9', '9c025d9b-94c0-4fe6-b76e-7c1ec212350b', '75928670-a256-42ed-b51e-9543b8ca1635', 'cf864623-74f5-4569-957d-bff279028373', '69efb8c0-9e46-45e8-a283-95e207ab2b59', 'fb21b068-3c9d-4e70-abbc-47810a638dea', '70a11a66-213d-4e5c-b20e-eb2257fefb85', '9a12459f-c255-45ae-84b1-6cfd9738ab2b', 'e21c0822-6e8b-40c3-ab0f-507a05cfe391', '5e7dd42b-1e3c-4ded-920a-138249a90e09', '018424bc-2b43-4895-bf27-25524f6f8380', '94fdec88-9cc7-48f4-b041-5fe7a0e43bfe', '63e5b952-9638-4eb0-a147-6621d743f64a', 'cb06d83c-b5e1-4eac-ab40-c47df0c2bf4b', '09050891-3b7b-435a-8a3f-d7c78a7ea59a', '742c43a2-d8c9-4384-ae47-74355916bde1', 'b22fd18c-13e8-492f-9206-f270e1c0063b', 'aad7bb7c-276a-47cc-bcfe-9a9928a7dc6a', '3016123a-a327-43c5-a94b-fabc3088031b', '0c335735-ee19-40a2-812f-f5a6836bba19', '6c662047-b05b-4584-ab28-41ec154e8bc0', '5c4f8bff-0af6-432e-8849-97cdaf4cf22d', '21984a20-8f34-4c0c-8ce3-aa501087ba00', '880a3c33-50ec-4de5-b6b2-df0f1dacd2a4', '3bd98e39-32bc-4132-ad22-6a4b72ec0a91', 'e5d40dcd-44ae-43a9-8308-849fe531e379', 'eecb4031-c493-4c33-9c13-6be3a0f7073e', '66346330-20cb-467b-8951-1041897cf4ca', '40eceb17-1c9f-45c9-8348-ff758fa9143d', '8122f3c5-49b0-4ee1-ad1b-17ab77090e42', '0015ee0e-5a8e-4fa9-8865-44ffb03f8aef', '3cd77bc1-e029-405b-afa8-17636d05fc54', 'dc315018-cd05-4201-be20-97475462e360', '8bf39c3c-11eb-45cf-8184-d4f479730c00', 'e1077c72-742d-4b92-9af9-809300848b25', '44cf6c9a-bc3e-4250-ba32-65e1a9ee8dba', 'ba54e5f8-d38c-486c-98a0-6805143916ca', 'edb6948a-3200-4bcf-9f21-b8e4cf9855cc', '0c331edb-93bc-4c05-ba8c-df5f9b8b5e3b', '5f534334-cad2-4e1c-8496-26e7c2851c56', '96921e5f-4581-41ac-ba19-481ecdc109f6', 'ed198579-9a17-48f9-becb-964d949610d3', '120b410c-dad9-411b-9ffd-54c21f9d9fe4', '03163388-ddee-41fc-9fc2-5a58d9cac277', 'fca0b907-93b7-43bd-988c-89361da1bc03', '343aebff-eb62-454f-b7ec-757094f90a37', 'a7fd44cb-d463-4629-8177-21a665450898', 'c1763fbb-2ef1-4bab-a748-ec22cffc2f45', '4f61e368-61a1-492c-a48e-94dee66ad83f', 'abe369c0-598c-4b89-9bde-d386bd7ac9bd', '10e77974-2e6e-4613-81e7-7666a782558a', 'f48eb466-2d0d-43c9-b207-f3b651288ab3', '381a22ab-a5c8-408e-9f8d-cdf1a6feddc0', '45432e3a-56ff-4001-8ea6-02cf98a53d9b', '1422db7e-2293-4dfc-947e-42c49dc5759e', '354445f2-d83c-4989-b22a-89e30ccfd5e7', '180a43ff-45c0-428e-9335-fd8ef5735acd', '14be4de9-e856-4ffa-9609-d692b832c32d', 'b00469f9-32b8-475e-ba32-7ef554e7f7eb', '72ee7187-3ee0-40ff-806f-8800c05ecd4f', 'b0b0313b-608d-4cc3-91c5-f3a210d3165f', 'd60da061-2da4-4bc3-9463-35c90e1437ae', '2113c51c-00e2-4f91-b7be-77179d6dbc04', 'aa221d26-5f13-40e2-8235-b769e1db1428', 'e9c1409c-b027-4812-8c0e-b46c10fb0b0d', '53d33101-763c-4d08-9d26-eb2df89a0aa0', '4fc3f2b5-4257-4896-a601-3866686574e2', 'be8ec969-4ec2-4cfe-982f-644d8c35c7ca', '5022fde2-bfb7-49a5-91e0-fd9817a2141c', 'd28ae317-ef56-445f-8faf-0d72bc83d624', '993d9ad0-37f6-4f35-9df3-94ca56959f7a', '6a5357ed-8024-4c8c-8cdc-9d6d4f84b398', 'd27fac31-8b44-48df-b96b-25b0302cf855', '8ca616fa-01d7-475a-bdc3-e64bd544401d', '99bf2bc2-d4d0-45ca-a315-05b1ebbbbf7c', 'c06eeaf9-5803-4445-8391-f81b1662e4d1', '3456c0cd-0926-49df-8df7-4b9903fdec1f', '33038505-ee5c-44a3-b822-61964af6e5b7', '98344f14-7384-4f4d-8d64-6f1c34b87715', 'dded1525-4c57-4874-bd54-788afea27583', '6636a5a6-f8ed-4891-ac7d-f1d2cea00a31', '83ccdbca-227d-4123-976d-1f36fd71f113', '7049651a-5ae6-49a0-a5f4-eab91585bb54', '06deee1e-3704-42e3-86b6-f9add0efc342', '5620d72d-5aaa-4ea8-baff-13172b1422c5', '818723fa-30ca-4a87-b360-ba2c7a0a99a7', '7a959cf4-1c86-4a6f-b0d4-80ef6457feff', '4081652f-3de8-4be5-a05c-4e005c40877c', 'ab181698-1c8f-4915-828f-11265d7cc12c', 'fd09a8c3-1d98-4023-8727-f4d01264eed4', '20395e65-de1c-4dfd-950f-90ad3fbdf116', '0a48e83a-d248-4da6-a37f-f0b18bf6d463', '352ecc06-ae1a-4ccb-b51e-69978be04ee8', '0e3074de-6497-492b-956f-2b0260d8b594', 'ccf599bd-a5d9-4468-9ff2-f160361c3930', '84a8b297-f730-481b-89df-509caaa7c71e', '3f83901f-8b7f-4d47-a63c-e7b469525be3', 'dc4cd5c4-f74f-49e8-bc4d-79c6fbeb6a53', 'defbea4c-0a86-4e51-a4be-4904e7241473', '95c0f6dc-9e6c-4eca-adfc-cfbf60a36766', 'c6b88c3c-14e7-41ec-bec8-52db96600e45', '57151ee1-c317-4b92-9063-9026fcd2d714', 'ac1a48f3-c04d-4581-972d-fcd8ac0475fc', 'dad57afa-3641-45b2-b72f-526fb7d3bcc1', 'cf34815b-73e8-4b54-8679-2feaa8d13de3', 'ec1e87af-dfee-4647-b8c0-fb20289fcee2', '8706fb54-7fd1-4aa3-b722-7d8fac4f827b', '0bfcd3c5-a38d-4404-bc29-ad8d3a9c4c24', '43efecde-9069-49a8-90bc-f8feb40d12b9', '2b5bf77b-82db-4503-97c8-9b0788ee8b22', '9f09ad54-c487-401e-8cd8-ea3a2280543b', '729e6258-bc72-4570-bcb8-d843c3267915', 'e7a1ed48-2e5f-444e-8d9d-c093b16ebe44', 'ea3157d6-fe7b-4e54-b5df-5379624cc497', 'cfa9d41d-3a02-44ea-91f8-953eb3182764', '59920d50-9d77-4fea-8d55-4a945acb63c8', '501eaea8-1f32-4229-80d1-2c1b5fae67af', 'e1e45db4-3118-4547-bed9-4c716a11159d', '5e1a930c-801f-47c6-9cd7-7b59afb19e25', 'b093c56e-4796-4d49-bd47-ff25c77cbfc0', 'd83bdf9a-1d8d-44f0-ae07-4c85fc160883', 'b5306c1c-5720-4e74-8144-fdbafa5d7985', '874d33e0-910c-491c-a02f-912260fa2a51', '8e49476e-b869-474d-a4d9-f14a70cef522', '179210b1-13e6-405d-8be5-d0ae62dfa88a', 'ede69a6e-c0ef-4473-b9a7-f1b7bacea0b4', 'b57049b0-7f33-452a-896d-b3f5b85758ca', '73b5bbed-bd34-4007-8fb7-fbe28dc000d1', '51179581-9016-40e3-95e9-e9066a8612f8', '0f51ca0e-7ca2-4f9a-96c3-099be2513ec7', 'd38538a1-2ae7-4110-8e59-049026736ba5', '90f9ebbf-7a12-434f-bc14-bac0d48f1ddf', '7f6b00e2-7322-491e-94dc-1162cffa67a0', '2a65e367-0ce0-44c9-9929-d90c1fc0fe37', 'eb989f2d-e476-4fb5-8b49-ee0594a6be1b', '142b5756-806f-4a84-aefb-47080fcfdd60', '7692fd44-1d22-40ea-8a70-8b185df98ae5', 'cbce5267-8389-4b70-bf07-522d4f39717a', 'c95e0733-b655-4cc0-9126-67f25ddc61bf', '3e485f6e-4dee-4693-a96b-506341ad0448', 'e99f020b-0897-4917-893e-8c546902525b', '4dbe96f9-6dbf-459c-9500-01d3264a16e7', '02cebec4-398d-45d6-9f73-ee8b3d008a3a', '1773424a-c26b-444f-ac50-ae6ab9476007', '39fe13bf-0417-4b5e-841b-76473ee321ec', '4e070586-f079-4ca6-b572-c2706141bdc7', '0e16fd32-50c1-4174-b140-7bafc9024e16', '56023f57-a23f-4dc5-a39d-50b26e76660c', '208ce00d-5063-469b-b608-52b44e58fefd', 'c61279b4-9a7d-4a0a-af05-7c8dc38c5efe', '02e5db5e-5dc6-439e-8ebb-aa962405d8f7', '7df20670-4aaf-48c2-8218-a04a3f3c5ea4', '966522e6-3e16-470a-9564-d4e7dca08d89', 'bb0fbc9b-68a7-45ad-860b-70fa65ddfad8', '4d9f77c5-d29c-47ae-b81e-2a9e7e07127d', 'f86486dc-3bd6-48d8-911d-2810f332e608', '3eab49c2-abc5-473d-b20e-2bbf28947943', 'c8a8283d-5abc-43d2-b663-050d0c03fe39', 'bb0dab1d-3c88-4827-b47e-3832fc3a5972', 'e14077fb-ba50-4235-b366-fd10724fe8a6', 'b14432bf-f0a3-4c69-a2c1-fcc60e48c254', 'c4fcb912-3b7a-484c-8829-8cfc799829e9', 'd88819a0-237b-44e3-80f7-9175c7bef484', 'a4b32777-37e2-400a-bc8e-89de8b3c34b7', 'fe989eaa-e5bc-41da-8393-ee95163fb109', '6e2ea477-9d45-468b-a188-4e91190292d2', 'ca01230f-fa59-4cb5-835e-011bbc75e25a', '1ab716a0-bd75-42e1-a200-0bf7ff94c351', '32b05b54-16b3-4a3a-982d-ed86e1711b56', '0c70d942-64d2-41ef-91d9-d1482b42a64b', '27a768f6-bc85-4cf6-bdae-c291de154696', '438b9df1-1e93-43ec-85c0-d95945edd11f', 'd96e5d7e-0b6c-457a-8aba-e3bd4bf22188', 'd1650e02-5777-4bb9-8ac0-1758cb3d48df', 'ee77275d-9118-40e6-be7d-6d4def126c28', '1809f320-111f-407c-be6e-967821501cc3', '0dd0c204-eb8e-43e0-8642-499bfe9864af', 'e8579d44-51fe-4f29-a6ee-cd01a6fe2730', '6056b55c-b4ce-436e-a906-d7c8af889a75', 'f9f00968-73d6-4816-bf04-1a994f0d8047', '8e9f5ed7-ea4a-46dd-a9d6-c80f17684bab', 'c4b18d3e-957e-4f24-b917-8a1740fedd9c', '6a2a5685-b8d7-4a56-80d3-bb1e5dfcfe43', '0b77a9e8-08d9-4f09-992f-c07a706cd476', '9f33555f-7429-4e6e-8021-ac93456b5b6a', '324f5e62-cca2-4824-88aa-d0d9c80e8ff5', 'f9de9b6e-7ed7-4488-aaee-4f53e387c121', '8808cf29-5a3f-4e63-ba82-8ab2a112899f', '3873a03c-c090-40d9-b74d-a372329b71e9', 'aba96395-220f-4223-8c33-ceaf8d40fe53', 'de689115-6d31-414d-a738-45c2d5a89196', 'af5a0677-8d11-4291-9ccb-47f2f32ee939', 'ad83fda2-6800-481c-848c-dee120da5b1d', '2bf05c38-ced0-4e8f-8261-f5da6ff2f9f0', 'ea3e198a-3128-4655-a085-b9f952572d78', '4c0631d6-6482-4465-bb7e-7c673ff99c12', '19540e55-5b04-4135-8297-d97d8583244c', '03a679e3-4fd4-4d5b-a076-b6792045fcc9', '5554db4d-2497-4788-a7a0-ae6300870da8', '0c8f41d3-6f83-4ab8-8620-8f023e53449a', '2652cf84-803a-45c3-8913-90452c43ac6c', '04e844fe-6920-44e6-bb06-46025ff83b3d', 'c13c4478-6c1d-4a22-b559-874ae9e9e347', '3d5f1acd-2933-4e11-995a-eb9388e8277a', '0362da56-5875-423e-85a1-1bc5c1c059e9', '1b349918-9828-4fed-ae81-24fc8e6a9463', '5b280eab-3331-46e9-931b-a56101a53c7c', '1665f5e7-3bc0-4369-9eb7-0d75fd062d4b', '0b156382-40cd-41db-9846-93a7de33cfbd', 'e8f9e94a-0aa9-4433-961f-a8f375ca8738', '111a58c5-babf-4807-b9d5-7f518007e24d', 'c1af6bd3-ca6f-49d1-b2dc-76657a2b2e18', '52881ebc-291e-4798-9324-3228cbcbe2a6', '1468e609-4a58-4594-b76d-616634f06f83', '87e456cf-7094-4e67-9551-c992f417437d', '5e2458c3-5c7f-41ca-bcdc-ed6599191fec', '9cbd340a-7dc1-4829-95fb-6cdd23bd86d9', '252c98ba-b3cc-40b8-8ae6-e5dc0946df55', '96bdfd44-2c03-471d-aecb-438e36d9c5e7', '1d1b90f8-f6ad-4628-b1b7-1da215ea88e4', '46d5c86b-fc1f-4f48-aff6-1c3d9385a888', 'fe4b6cd4-dce6-4006-ad73-c39a2f693f77', 'b952571c-27cc-4084-ba0b-ddb176b7fba4', '8c22f5a1-7639-41d6-8aee-e937e4a7b167', 'ccb754fb-3317-46c1-9dd1-3eaad9ebcff0', 'd42c6eb8-c397-489f-ab67-6a8ab3b3206e', 'db54bb81-1060-4791-87d9-586485252d9b', '2dfefa48-9d49-4a1c-964b-90dadda8a40b', '21d2e5cf-75b6-4e78-8706-b0b2125cd53f', 'dabbfe07-4de6-4959-8b1d-418132a1795b', '20b1a09b-2378-4ab1-9312-709ec3d845e6', '4da76e6f-d4d1-4468-a48d-2040f9727a0d', 'b9076a8a-39fa-4ff3-ba38-6b8d21b57b18', '43a9823a-525a-4731-bbfb-6f791718d55d', 'b2841ede-db4e-4743-8df9-9e9d9ed892f8', '4c27fb9a-66a2-48dc-a4d4-896ec87ffe1a', '21ef7798-c937-45eb-8428-b59a80c7290a', '110bfa8e-bb32-41d1-adf9-964c10a9ee76', '5cf10519-b699-4c1b-8f22-dfe9c7771161', '9da6918f-c2f2-4fdb-bd80-194220fcf989', '41cfb084-b1c7-4033-89c9-d67531384ead', 'd83e0652-95b3-4cd0-b17b-6d24cacf45dc', '58670ee5-4e97-4b59-9fa7-f969130df90b', '4d69eadc-23e2-4e98-af8d-6653f15563d3', '23b27688-6b1e-4431-8d64-81057262e7cd', '0a2d8bb5-16a4-4d53-85c3-1a134fc1014c', 'afcb3e74-3071-474b-8edf-8ffe4d1893b5', '90230c1a-41b3-4c31-89b0-04f5795d4fdd', '4e340de2-8ecc-43af-902a-78eb6baf587e', '188dab93-af56-4389-8879-e103a2cb2728', 'f23bf534-8bbc-4ae7-9eca-a9b4d54a96b8', '91093f44-eb86-46de-8f1f-62cbf71d60e3', 'eda66d17-ecd3-4029-b5d5-70e89510f9f4', '07809a86-b71c-4462-a42d-63690f16f623', '1ea65d5f-f256-435b-8288-6626c539f54e', 'd9321a2c-2965-4fb4-ae06-9ab99e829f95', 'd82dea44-9fa6-443c-985e-425e7782a8ea', '74bf3805-fa7d-4953-8d83-a1943b4d0810', '01365b0f-4886-4e0a-b9ee-82894430bca6', '931c5610-7fc8-498e-ae3e-d51fbb5067bf', 'b20d3cec-4c8e-41a9-ab63-9cf2b4bd0347', '3f9915c5-4f01-4bff-9592-c3aaa52b68de', '0d991e97-9bc7-4bcd-aa98-f0c2cf04f37a', '552eb22d-64fc-4530-956b-f4ba81e6ad0c', '5822a97d-b39b-4a8e-aaf6-66c0f55442e8', '34d72a45-7010-4d21-a036-4a2438622e84', '93052f03-d3f6-4e99-b33b-fbf01d9c4015', '03f5dfe8-049d-461b-85c3-6fc69fc8c283', 'cf03deec-23d3-4660-8c2e-b8bdefeb9b0a', '5fef6365-8c1e-4ed4-8fa1-57358c7b9fe4', '7b2ce29d-8516-4c4c-ac78-9ae1dc3691ce', '4104e677-2ae1-43a7-81f5-9cfc1b727c56', '8b577111-1c19-4dd4-9454-b667542fc14c', '747abfd9-f93b-4a35-a33f-2439d429e01c', '467b6110-a23a-413b-adaa-144687c8b582', '998dd0ad-3f08-4d96-9a8e-e9f183ca2f99', '7e96da24-14b2-433b-88f6-6996fa4b908c', '966380fc-3da0-4d88-8cae-78447e5d7da7', '6c507807-fc6a-462a-a752-6047b0c4aa80', '1eaaa15d-ff01-4799-806a-f445bb595cc8', 'f3e352ae-6087-423d-ad5a-8b74f74005a5', 'dec59be4-f19a-457e-8e5e-e8f83adc14c3', 'd82ec74f-6934-437d-8c2a-453571ae6c22', '951b0827-49b0-427d-9221-81eb2468bac9', '9577494a-352f-4b77-b8f3-0b636a94e776', '6387a9db-9a58-4c0c-8b55-672fb90a3b06', '0e89eaf6-9e6f-4182-b757-2b9d34b82e01', '0be9332f-4228-4dfa-aa4d-06df280f11e4', '3835d593-3474-4ac5-977a-7f3aa3233c47', '3131077e-71ff-46ea-bc8a-0e1b739cf1d7', '6b6b9fa7-a4b6-4972-afb4-8bd080746a3a', '5d657cf5-5def-44cf-9c73-cd5b8e4b4da5', 'fe0b73ca-891f-46ed-bcea-14cdf1d6081d', '725c1567-7319-4680-baa0-ce8b36462aac', 'c00aebb1-09e8-4033-828e-1e814976e43e', '935aa743-5e45-4ec7-9a13-3c451f051c57', 'd9ef103a-8dd5-4980-9500-80ea5d7f6f89', 'bffb275c-f674-43f8-b6f4-79a47f4e86ea', '8532452e-3204-46f4-8a71-5eba74002460', '0f893cfc-f4af-4c47-a870-e34b69ce837c', '3c1dd867-820e-44e7-99a1-e012d254371a', '6b42ab6c-0be1-4230-8564-4f3a0e8aad13', 'c959bbd4-979d-47c9-a901-dc89dafd4d32', '1f646ccf-5d76-4de6-b8a2-94b6d0c607e6', '845a5953-31b9-49fa-a131-96022bd91fa7', '2bb3aa71-526d-4b6f-a946-dee0f6e14337', '3b660af2-7cda-4118-842f-b7425cce0a68', 'e9dc4ab7-a302-4228-81a1-a43bfaec1164', 'e614955d-7776-4273-a8b8-d34f6f79ab8a', 'b0c04914-85fb-4225-810e-8a51136853ac', 'f1ced455-9899-4c94-bcc9-dd5580ee0e1b', 'ed57a0cf-58df-4b0c-9210-b618274047fe', '9d891ff6-69b0-41cc-8405-bdfca1093e9f', '438a4b88-6dda-4269-af1f-1b6c4b033e93', '7e0187fe-cff8-400a-8b41-7380564ecbc0', 'fc2a4192-9c3a-4691-a394-680e120ff67d', '8170a346-4781-4913-bf87-adddfb94fb3f', '8aad025a-2761-4cc0-a02b-d89232d2bf1f', 'c10b68ab-7a7a-4e1c-9d9b-09ffc0ea3b03', '3840edfe-a987-4127-bf96-cc48530700cd', '67b4e872-4238-4ed7-b87d-cf82945b5623', '173d0d33-b241-44d4-8ae9-f6e805482dac', '399686aa-be33-487c-b7ba-b282d8350b7b', '5746c9d6-3910-4e15-825d-2dcb61ea83fe', '99a4ff73-fcd8-4b7a-9299-8af82881f974', '72a01bc6-12d5-422b-9609-84e75db9a884', '96b058e7-3175-4d82-aea7-f799a0b0c99f', 'be7cb997-b4fb-4ef8-a5d6-7c64ad1291f4', '421aaffb-c82e-40dc-8d38-9898d99cbc26', '3dc31f9e-9bd0-4161-ad17-d959c813fd11', '53e5e0fc-8a69-4cfa-aec6-b0ce3b9edf94', '8b726202-b708-4739-a163-254b6fb39313', '8f962a0f-1ccb-461d-93bf-5e81fccf824d', '94d83e6d-7bcc-4fdc-93ed-6ba68298148c', '796b0c45-9307-4939-a30e-a5f27e5b2684', '0eee3b1f-00ac-4e2b-a98b-49cf63946c2f', '311fb727-5e71-4e40-a012-555f9a446d05', '8cc7d87a-78fd-4d45-bc1e-553f3a3a27e8', '2ca6e65e-5871-42ee-90a1-734a0136144a', '522959a7-fb80-4765-a7d0-2d522dc28faf', '51de8d48-3261-4288-a3f5-380223cdcc58', '4413e19f-3837-484a-91e1-56b6099fc231', 'd4e0fcb8-5772-4bea-a791-5b37fea8015e', '4d267849-be47-46d4-9c69-937bd09af531', '6874cb22-bd36-4d4a-9b90-b0f3996c4e99', '1916fcc1-b219-4215-b9e8-81adee18369b', 'e44fce12-c169-4132-bb7e-0dcdcd159631', '0cf9cc8d-faa8-4c1d-b3e0-7d2521184620', 'eff59481-d6dd-4705-8ee3-2659ace39f4e', 'f0f5d3ac-a5dd-4606-ba00-f9f2f822257a', 'eb59557b-b43b-4090-863c-a213b86c1253', 'aa89ca11-798c-48c2-947f-c6338fe1f5ad', 'cb672671-6930-4df1-ab16-3b93cc1fb317', 'e000050a-f23d-4a9c-b7f2-7857422518ad', 'babf30ba-c930-417a-8276-a44be7de21f2', '9bec8178-d59c-4ddb-a098-fc7114384adc', 'd5fcf26d-c357-41ca-bf2e-83a9c934922e', '8903ce30-dd24-4855-b231-04ca7078df2d', '02d922f8-31b6-4e0c-89a5-4e0e04ccb799', 'e6ee949f-dadf-4368-a548-87e7b11536f6', 'e2493301-56ac-4688-9810-3cbf11fc2c08', '43991c07-3760-45e9-96b1-94bfcf18e295', '3fb19119-dede-4957-88d5-0d1d7511c599', 'fefa4b6b-fad6-43fe-acc5-b09a58b51743', 'feb4d014-91b8-4cf4-b063-3db5fa1c5ad9', '210f2266-261e-4c97-86b0-c28ed536644d', 'bdeb5530-7f62-4a41-b7b3-15488cf0b46c', 'f09020cd-9642-4ce1-97ff-7fca54906d2c', '2900465f-4868-483d-a1ae-428094f0f476', '7084ac95-5d06-4794-8df1-1df11413b78d', 'ff3c5d6f-f241-4157-90a7-67ac26f8f006', '4d2cec7e-d5f1-4281-86ae-658a8fa41fe6', '1fca1b8d-e135-4bae-a10a-385e383d9653', '2c8b3a6a-9da3-46e8-8a32-49cf12335cde', 'ae5ae087-cb5c-4120-9632-4d3a0b911514', '197e9820-7d3f-4da7-a8a0-fa2fa80f42a8', '7f557950-8930-44cb-ba42-8e825ed735c0', '46eacd24-af3b-4e66-87cb-e643bee894fd', '169e2182-995e-479a-aca3-85268160cc53', 'c2bf5a54-dbd2-44b3-bfc8-2dce20557952', 'b759cf1a-7b77-415a-bb27-8ccffd28d7db', 'a39d982f-e7c8-4351-a90d-3fcf4a988c19', '46f78e1c-4123-41b1-a188-667850b231e4', '95e34313-c0f2-49dc-a0fd-fe2fdeac2a6b', 'b91e28e0-c437-4aaa-8408-e8a02d4b506d', '5e6395d7-06ec-423d-b50a-2ac4f1437fd8', '7903d3c5-95f0-4eec-b649-4d1c80d792e2', '6f5d09ba-947f-4f3b-bdb0-d18c02f1afcb', '22329393-c166-40c7-8d3e-6816bba6e29c', 'e9aa3380-b6f3-479d-bae2-376aa5c8dfb7', '3f7d36a6-e87c-4dc9-99a8-365badff3e05', '5678f58d-46ca-4b83-a49b-4fcddc6fdc0d', 'b274549e-86cb-48f0-9558-2184cdcff436', '89971c5e-5588-4492-9b31-06f6208e7000', '5fba0e8f-e69e-47b0-a343-76cedfdae173', 'b4c68b68-50e8-4113-8957-ec1679cc8dfa', '378261e5-81b1-4c04-9bc1-7c7ae8e7dd3d', 'c10a1b6a-aff2-4960-8c91-329876776d69', '7bc562a3-bcbc-4a52-9861-952c38277fa9', 'af80cb3e-2dee-4fa9-8956-9597e1688ddc', 'e8eaf64a-2297-44f9-90fc-cdc9bca1baf7', '5e8df0be-24ac-4a94-bee0-584ee04a07b5', '064723d6-00e5-4674-a242-464a93ea1d28', 'a323a4db-f6b1-4e74-9798-19daa2fbba68', '29cf590e-00e1-4742-9133-e82a679242f3', '860b7eee-16e5-4f29-aba8-181826c9f72e', '385649eb-50a1-4c1a-9a89-f33886c478a2', '858ddf6e-1849-4aaa-916c-817d51358073', '2ba0a6dd-2694-4477-bc0d-8208353a46c7', '040e4431-86c9-46e8-aaaa-4eb33042f9f4', '6c9971cf-8294-4c26-a621-ba608f2e2210', '9769cee8-4227-40a7-baa0-5979a6781d9b', '3cdaf913-86e5-4c56-8aaa-0e84256a6eba', 'cdb7771c-7b75-4cdf-be63-f5fd83f960a2', 'ba5c502a-b8f9-4ab1-8ef6-f69a33850530', '488c602f-9606-4652-9f48-92368287c697', 'bf8b3a4d-431d-4bd2-9fa2-4744fef02e22', '8448b340-6747-400f-9b41-32da8e44d7ab', '1d1d2cfa-a75c-472a-ab77-20eb6d3c4633', 'c904dae3-f53d-4d7a-85d1-4c8a2dc2339f', '47b7a0e4-0891-41eb-bde4-2544d86127a3', 'b35e857b-8a56-484c-b789-851322371a6d', 'f374f5a2-2929-4541-95b1-0a31790de38c', 'ddc26d09-ccb7-49a3-9268-d5a658fb64a4', '28687620-f1c4-4a63-8d61-39378a106936', '19f09579-6c09-4218-bfac-987279dc4d4a', 'd1f0e4ea-fcf5-484b-bb3f-47d5a45c0bf7', '1c1804f2-7225-4aba-b442-2a10f37fc1c0', 'a85dff36-4c1c-4720-aedd-2dae93df43ad', 'daacadca-84e4-47b8-a21f-2b50f00098a8', 'aa423fec-29d4-44ba-a751-d519c6c6f195', '4c9f597e-3920-48ea-9891-4911e3655c45', 'ade3111b-5d39-4ec0-b156-8288d09231c8', 'fb99de55-c54c-4141-b09c-717eca8f7d0f', '3f298b84-d402-48ea-bdf8-1ece07f281c3', '9c50f1d8-2a93-47b1-a017-1ffc6daa7b4d', 'c08f65fd-1910-4485-b722-a53187e09429', '7b7bea4d-98f4-4992-8b48-c18223bbbad7', 'cf51c84f-ce2c-4b61-8dd1-546d231409b8', '0551505c-d424-47e0-9d56-78d0a51f8dc3', 'b6801e5c-2242-4cf8-9604-78109cd11e21', '9f882e90-e94a-4585-874b-7ec8d8744e64', 'ea63003d-77da-40ab-a201-b99e4c9a1d0c', '11a6b488-ba25-440e-ac49-9808e106fb5f', 'ad7a086b-10de-4ac3-8ffb-251730a044bc', 'b92777e7-13ae-477b-84e1-a8555d9507d2', 'a9149c15-aa45-4752-a3e8-d032e9f39f8f', 'c6508b93-09da-4085-9516-fca9ece1e576', 'b56e2469-8cb8-47ac-b81f-9406ff4c8fdb', '373055b9-80f9-4909-a8af-5d75ffe56cc9', 'd2dd5c42-65c0-4269-849c-ab323ff18287', '7b148d1d-7614-4c0f-9e5d-a2a1551d069b', '399d6809-9c5b-4deb-81d3-04febd01e004', '55d0e130-d99e-4bc7-9eb7-c82d43b8ca90', '318d3ab5-c08a-4a28-a639-d975903cfe4e', '5e8de72a-e8e6-4231-b369-a7224eab9c03', '0fb3202f-1083-4f0a-89b4-c6bc08d579eb', '33accbac-6ea2-4d9c-bd65-17aecc2d1788', '3569f895-c7d8-4b32-85a1-5aa4390589f0', '9d1d4098-e4c7-46b7-8bdf-cf309f8d73db', '0603eb0f-f10a-40c2-ac2c-c4bd46e26b6e', 'bb0eb5d2-e021-42d4-8e73-fb39903f0f78', '44954583-fd9f-42b5-9d07-69dde1f0ec07', '258c6347-b992-4b76-ad70-be74790730f1', '81056a62-676b-4617-8de8-d9649a13f48f', '003c8f23-db0e-48a1-a064-95b5e3563b21', '02f3da3c-def3-42df-aa07-ef380c40dd50', 'd02096d2-ea9d-4cfa-b6c4-f53497e08ac3', '9b9c22ab-29fd-4b5d-8682-36e0ce7b7f0b', '598ab5a2-ef47-49b5-8519-f3300ceb3d4e', '84c26bb3-042a-4fa8-8b31-c1b2b82167f9', 'faf13981-3b3f-409c-ad0f-51573019e6ab', '22537d25-2ba7-49b2-842c-ce26778bfae2', 'd0e95c2c-c657-4aa2-b384-a4d50f60be69', 'e1613049-4b33-4fa2-a78e-70c4aaaf9270', 'c2028001-fc3c-4e8b-923e-55b904d40229', '4eb26e33-e232-45bf-a404-282144abe044', 'ef79ab95-b4f9-499e-8ee4-81eb5ed67251', '49ab448a-566a-4f73-ac9b-69101d468221', '8af208aa-b377-4c33-8fc8-6090b77a7afa', '1dae6998-86f1-4362-a152-4554f89f0fb3', '4b8b9049-809b-4648-ac76-8fca482e7e86', '62814558-2fa4-4e3a-a9e2-cabb62a59e7a', '8cda2b59-82cc-4bb3-a3cf-f12cf5fccadc', 'ec4eb38c-d38e-4302-afd9-524b0ad73cb4', '8a4e1146-c83d-4922-a321-4aec8785aad4', '1b4a6022-38d6-4b62-89d5-f6eb68c60ee8', '84bc9749-fa80-4f05-9b35-5c2ff3045419', 'b11a6f74-2ba4-44a7-80b0-869cabc2f63d', '1196e8b4-564f-4999-87e2-ae9ac6fad282', '6e34e507-3af5-4ac7-bfb5-51d78d7b0928', '6b91f418-6741-4070-ad58-ec73886e303f', '18983c58-6531-4faa-9727-ea6c13176ee3', '755d1d58-68ca-4dde-adf0-851c2b476d3d', 'c1d6a069-305d-4689-a558-17669cf479e4', '5602e6bb-f08b-487a-b3d9-7630dbcdcdc2', '813715af-5676-4b8f-9fd1-a2663f35ef7d', '26924049-c3ea-4759-a2f1-0fdebd1119b4', '9b6aae76-6230-4c92-9b4c-9304e022e548', 'fdc3ec36-ea11-4266-8662-de8352cc52c7', '97e3e269-a605-453b-9dda-c19fea3fca48', '53ae2d8d-4a16-42d9-bdb4-1bc1023d664d', '3211127a-4ec9-4ebd-8907-dffbbc58ec55', '1fc04565-7418-4c10-9ac6-50647eedde5b', '306678d4-3658-453f-a8a3-7456018d6309', 'faa249ba-de7e-4d76-9474-8926644a2a30', '5c7593cf-4a7d-4cd4-bd29-cb734a9634ed', '4023419f-9887-4839-a1c1-b21a49331495', '11f0be9d-9f94-402f-bdea-87147473f981', 'd4b7e836-c7a8-4c98-874c-132496ed2971', 'eca5a70e-7107-445e-a627-e66a2fb6905b', 'd345402b-0b15-49cc-b579-a2c8f8f52a54', 'e37b0b51-218f-4ba8-b863-42444de3ba28', '8a397265-6fa1-44f8-acab-f87a52690dfc', 'bab522f7-2829-4f92-bc7f-e37a3b80ecd5', '459832e0-6ac8-4983-bafa-a5c51c478b1b', 'a4b3d48f-9a05-4008-ab74-becb3639b0e9', '4833ae87-3c70-4121-821f-d1cece4b69e8', '51d7afd2-fa78-4efc-88bb-d134300e00ad', 'eb1cc4a8-d4ca-4da8-81ea-2fe995597d12', 'e26e859f-a6fd-4f0a-8cb2-bf789f2ff3ee', 'b5541836-7150-45e8-9ed9-c30fb8bc5848', '6f83e38a-93b6-46ce-8326-b57bb2b00da1', 'f1ccd644-8e75-4cd2-a80c-f970b516e199', '2ee73550-24bd-4892-b976-527b6f6a5726', '0aa57ceb-5914-457e-a15c-dbcf345866cb', '7f5631b7-39a2-4406-8a5e-3118999bac67', '47896e44-9374-4b91-a256-cf7153bd15fe', 'd76ea38f-63f8-4557-8297-9e89d8386e2a', 'f92523f0-582c-477d-a495-a4f470e344fd', '2a7f199f-02bc-4f62-883f-9ca54aede574', '324107ce-d849-43e3-8952-4b676f556e90', '3691c5ad-7deb-459f-9a0d-9c9f858ac41f', 'e7f317ff-1164-47b1-a771-9839b24eddfa', 'a3c75ca5-32f9-4e19-b098-d4316e81cfb2', '187395f9-c570-4e47-afe9-06347a813112', '569fcdc1-8b6a-4712-a952-9a24aae208f4', 'b5b715fd-ae86-43da-a2e5-e6be17e808ec', 'd11b411a-1916-47a9-837b-c0abe563da0f', '08c81688-0120-40c0-859d-0fd37adbe64d', '55cff872-df09-4714-875d-a435f2577338', 'a6355884-037a-46e3-9763-d53f0db0dcd9', '23ab1723-68b3-4c19-aed3-5082111b3638', '2eaf5944-ba2c-4564-8343-ba833cc0433d', '89562bb4-4510-42e7-a479-110b2c0083ee', 'abc5a263-55c6-4d2a-9b3c-28be717663ec', '7139dbe6-cdbe-441f-9df7-0a0a5decd365', '89f14c69-044f-4959-ad97-1de41da6fbec', 'f538d79d-4d75-41a6-a2f8-615a87e355db', '3959f1c0-f30a-4438-be70-df7b7c3c24bc', '3bba1626-aa31-41d9-ab67-3322b2552fc4', '4a6e293e-2163-40a7-a364-3a5258deb92d', 'a29a1179-6908-4d3d-a04e-0ee51265417f', '5d941dee-007d-4ee2-bc50-e7f65c35bc88', 'c7dc091b-15ea-4ea4-a2b1-84d58e77d154', '3240b04f-e939-4437-a96f-a88b67ac19f6', 'bad23e12-7f9f-4695-acae-089d9d854c77', '997fdcc8-ab4f-4568-9818-152e7317ce5b', '5a4c858f-fffc-4d9f-bd80-a390acd0343e', '82957453-0665-40db-be0b-bddb7719031b', '6027ecb5-32db-438b-8f08-9fb6202805bb', 'cfccab1a-39fe-4c1d-806d-195e9369888d', '039f2de2-d2df-40d1-a5e8-5113bbde69bc', 'd7c1185c-d0f3-4cef-b2dc-83b175589cd9', 'd4716363-a7dd-4f2b-a78a-b6503cc32842', 'b63785a9-52fb-480a-a44a-e2216a86ce8c', '90dfd53e-4eb6-41f9-acce-f375ce7d8fe9', 'fa6e8c36-d4bb-4f26-a584-0bc7ba59ba1e', '2bb3396c-050f-4aaf-b0f0-37998fcc5a6a', '57af70e8-e148-4c14-bee8-122d6c720af6', '21046f3c-e0bd-415e-9be8-dd57ea7708bc', 'f3513c31-5688-404f-a0d6-2cfbbb808e3a', '597884b0-1750-4665-baa5-fbf431300d65', '4c7fc02b-ef0d-483e-b2dd-cc1f559252e9', 'ef36cecd-0101-4a56-8b75-1fccb434ae0c', 'ae44ff15-8d57-4d1f-9871-8ae0d5b806de', '5fca6739-f372-4114-b61a-a118bd810e49', 'b2fd1280-1f2a-4ed8-a2c7-c982594269ca', '07eadb7d-c147-43d1-a8f2-28dbe3fd8860', 'dd3faab7-d4d3-44db-89b7-13973c451e67', '1486189a-4ee9-44fb-80aa-9407f2b07150', 'af1fffe5-5acc-4d1c-bb81-2c6d87a760f3', '9b102876-af60-46c8-af70-0b7e6f6911da', 'f8b86bca-b6bc-4789-8a39-c88ecd4015ad', '7c69ecea-2493-4a1e-8da1-57596aed4051', 'd4702512-48aa-44a0-acac-3e6f1f8109e3', '247be6ec-cf48-4588-94d8-bf12c3cb2c8f', '15ae0bf9-2470-4f58-849e-6f43b09c2973', 'c45c18ef-4f86-45e4-8106-27b335db73d7', '202824d7-f6d3-46b0-8d11-0eba8b61c219', '1b9cf29a-ed52-436b-a8c7-0d7d69e6d63c', 'f2f693eb-0c05-4c8f-8e8c-4c0f84c74b06', 'f5de50d7-536b-4973-91fc-a0c371a87671', 'f663e8ba-35cb-46e7-b387-67b840d9f39d', '2e20e906-4745-400e-a3a3-cf54a0b1c893', '6cfe9246-8659-401f-98cc-e6318feae664', '1ef72e1a-dbee-4190-8321-812d51e7d686', 'ca1a97d3-54df-4c01-80f2-9a7ca7704315', 'c92d09b1-0228-4e65-aecc-bad1b5fa1d4b', '7f0c65ba-28da-4974-bb48-94b88bd73338', 'c224e0f8-8c5a-4c97-a51b-0a19474a30fb', 'b37c52d5-bc51-433d-b8de-3118b421d8e2', '7fe940e1-1771-472e-a840-2b542266a9a4', 'c7c6956e-0b9f-4b99-9e7b-5658f50c327a', 'de2e819b-4944-4c4c-9d05-deaef8aa3bd9', '507ca903-c5da-447c-97c7-6c7357558a4b', 'e565b9e1-ceb9-405f-b657-62a9f748ccb3', '6d0edc41-58bf-45ff-803f-b7d27ffc6ccd', '395bfb31-cd92-4a4c-9a3d-add53a752d4f', '5d5786e6-23a6-4d40-bdba-e86f1fd7118e', 'fb9ea444-03b1-4903-884e-9b26ff708732', 'ff9a7888-89a9-451c-b845-af7424351181', '8f9624dc-5d94-4c9e-a00b-31c6415d895a', '5e91cd98-0d63-4c86-b9f7-c8baa6458861', '7dfa18ea-e1e2-4470-bad0-01e4b5307feb', 'd8bf2357-de69-40d4-ad59-c063955f41f4', '4967c305-97e2-4eab-9476-b112de6781f3', '2db2da1f-4eaf-4e1d-8235-d9e6ef07b2c4', '03a21d40-95b2-4d4e-92f2-abf80a4c8238', '8b26b6dc-4d67-4a62-a514-50a6ac3c01f1', '802db244-ab90-4df3-bb29-7c1dd0165874', 'e4f00665-9b29-4ed4-a88b-5241656b12af', '3939a5f7-4995-4e90-b40b-d2bb4fc22b49', 'c79947a7-56c0-4cfc-ada7-3fd6e945f248', '835fb369-320c-4073-91dc-de0edf07e1bd', 'f297ff6c-1660-400d-ad4a-d1a1c62a1bc2', '1f68d81a-3492-48e0-b3a1-8586c63da175', 'ae4a9dfc-0e60-4667-9655-e39adcbca78f', 'd21acb7f-64df-4673-9ce2-96666e7265fe', '8d64bd4d-8595-4ac1-939f-5bb3425a226f', '16732e86-4b6f-4df0-835c-0dd361a8dc62', '609bde74-281b-4427-952c-0b662f6a8a69', 'b4389ef7-afbf-492f-b479-337bd74c32e9', '55fb8a3f-4fd9-4660-9f94-2b6c29fc81a9', '57d539b2-42f5-4022-a0fc-b57409b482df', 'ae746ed6-1c6f-4249-8019-e17160c3848c', '923e39ba-b2df-494d-8197-aac3653d3b72', 'e96017a9-115c-4cbf-bbd0-9383e859eef3', 'd2f6bcc6-db57-4113-b660-9cf80d62c9ca', 'd330220a-aad0-4492-b624-b349f1cea454', 'c4ae012a-c1b7-4082-918e-257731b145fa', '74546286-bd40-432f-9f9d-575961e4dd45', '21c2b2fb-994c-487a-a625-ea269d4579ee', 'a1cfd3c1-f997-4cfa-adb0-54292bbd6b85', 'b9cb0ad2-3d9e-418a-9f5d-6566b8ed93d0', '98290691-5388-4ecb-9337-47147293b5fd', 'f1652b5d-9b6a-4d3b-b1ec-6ff963b3b477', '92b46f74-a073-429c-98b8-9e584478916f', '2ad3c93b-5a22-4d08-9cc2-6bf708e6c44e', '1da769da-20a7-4d93-a7ef-eec854bf00a6', 'efb4108d-6703-410f-8319-b213b4b154ea', 'd15effe7-2994-4e62-8303-22322d1da30a', '85ad8b85-ab16-4604-89b0-d718a9e51f87', '1900ff4e-b699-4dfe-8c99-8b958b5b71a6', 'c15430de-4d31-41b5-bc2b-e633ce072178', '138d822e-c11e-4598-972b-fc4ff509a399', '615d3a8f-f61d-49e5-9167-70533c1da7f2', 'd918e7df-7faf-48fe-9566-443022b8728f', 'f883a04b-2c5d-4e44-9e44-8b18d9aa8447', '16a55782-1dee-40a9-a9d3-adfa7ad95736', '6de79c13-5fc7-4d73-b7bb-019300d6d9fd', 'd7787324-8d3f-435f-8492-5bfa8a0e706a', '2da1e433-48ba-4149-a3d9-db8c3cf24785', '47846124-cee4-46f5-b4e1-bbbc91e0f6a8', '84dda06b-7227-413b-b8b1-6001a7aa2f6f', '90c04f9d-13da-49c5-8f32-3cd84f76025a', '10b71b21-a450-4170-9381-5ea888c3292e', '3a4f3e85-40bc-4a0b-be1b-9d0ec303a670', '33756a5a-9c8c-44d7-aff2-87a320990d14', '1fad5ce9-0ccb-40f7-911c-c43b98e3d12d', '75188c8a-fe3d-41bb-a093-d4a6d8eb729d', 'ea554bf7-b674-4745-aaca-c1def3c4af2f', '476d461e-88ab-4bfb-96bd-48a9ef2f31be', '3c10dc35-240b-4441-aa3a-bdb3a94bbbe3', '4fd9ec0b-fcb7-47ab-bfad-a0cecfcc475b', '123840a7-15f9-4fb2-9d5a-832b2f505d00', '31dde70a-4087-4f17-b395-89d02471bb3e', 'f5f973eb-21b1-45d8-8414-5bc12e950d01', '11a30795-1b48-4632-bff1-3caa01df8ef2', 'ddca540e-8358-47b4-8743-c3a44899cf72', '5514d8da-782b-4703-a90c-660d9c6231a8', '42ffef23-edbe-4751-be47-6e0d0726a6ab', '9324d62a-6668-435e-b8a9-198c3df52ba7', 'd92143c3-fb51-4ba4-8638-487dab3d6247', 'f2ff94ef-de8e-4f73-a37e-0bcaa347d99d', 'f60255ab-a4a2-49c1-8540-f992cab2cf9a', '38d16303-6a5c-4ac1-a32f-5173f58aaf74', 'a71ea415-d11f-4d25-9e8c-4ff2cf61e748', 'ccc108a9-a4c0-4b3e-bc77-8742c677eb40', 'e8470c51-fb5a-4d80-b268-a76244afa1cd', '4859471e-9b38-4d03-8595-6b42694b2754', '8ba99e2d-48d8-4cc5-b4d2-d44b0e007f87', '5c52c71f-79bb-4118-af46-9b3e1d14126e', 'f738e93b-f70f-4cc8-99ce-c6d37707e6d0', '9d032bfb-af2c-4e91-8e6d-2c023959875c', 'debe73b1-f5fc-46ac-b402-eb425e9c5fd0', '6d29f750-02f6-4b0f-8a38-6204a6fc92b7', 'e4398ba5-a39d-489e-9bba-2b92f429d97a', 'ea2ff417-ccbd-47d6-bd1c-af1a551d8af2', 'aaf0380e-4ea0-42a0-b1b1-7793fef5e2fe', 'ac2cb032-eb2d-46ff-a3db-852a6c81cda4', '6bc11515-5d3a-4e3c-bcab-8384543daabb', 'ff2e3046-836f-4b2a-b207-2b6d3dfa42ce', '6c2d5d19-df8e-4eb6-b614-8707e8cdf9a0', '1d8da1c9-8e2f-4b73-9b88-8fac9df1d231', 'c800dbc6-4da4-43d2-912f-9ce9921fbbc4', '8bda2d72-5128-47a6-830f-836074ebb065', '9a280b19-547e-428b-8f65-3777d0fb091b', '2d44fc49-10b5-48b1-8cc5-0f6389c3c0ba', '3d6c6485-cf6e-4f37-8554-4e3ba314ead9', 'a42971ac-5460-4131-bb1f-252b3bdc6ef3', '065d9070-44be-40b5-bdba-d457298805ca', 'e682a038-2cef-4195-82bc-2db6a0bfc190', 'e2be2967-49f3-4b45-a9d6-697e5ba28c19', 'cc14bfda-e671-45cd-92f9-726e3af73a75', 'f6a2bbb2-41aa-4535-98ef-fa2bc6690553', '0af5f6ba-88a0-429e-ab34-eda507ab7b2b', 'b50a6077-62dd-4370-838d-4504e756ec76', '4e46ecc8-95f4-4e71-8546-7eff67aa9674', 'f9cc3e05-a4e0-47d8-8f85-c536944550dd', '76cf6789-64f3-440e-83d3-ea3a785a5f18', '1ff7f628-9d32-4da8-ad05-5157f21941b4', '819f5622-c337-499d-a5fd-09a6645a0bac', 'c20836de-ba7b-4848-b301-02f9ee73c2c4', '654a3768-44be-4085-97b3-b0c81c5b1e25', '6078246d-c76f-473b-aa53-aa9811d85202', 'e39365be-501f-4175-8b96-7bc21822038b', '92d48f92-0ab0-4295-b566-020041776d81', '3ef430ae-20ea-4b90-b4e4-7a147feb538e', 'f0eaf582-15e0-4f4c-ad11-a238903e3398', '72fac284-ba6a-49fd-9c64-360e7916882f', 'd27abf6f-ba69-499f-b16d-aaf4977976c9', 'b647b47c-7ce3-4d07-ac00-04f19aeb813f', 'c870e45e-e8cf-4428-94e8-a6e445da34c9', '711f7051-020c-480c-8eb1-3e134684a53a', '0eb793d1-1ba9-4bd7-8266-e25a5293b290', '370765bb-1578-4b2d-a525-9b68eacb5d3d', '2b7f2ba9-d96b-4929-afc3-aca3d6815ff4', 'a017a9e9-7486-466d-be7f-86204f271ac6', '049f12de-f88b-4053-8f33-b0d2de666717', '8f465387-28a7-484d-bf80-63678548df43', 'a253ed92-8cce-4781-a737-25713b56d0f6', '6dbf6270-19ad-44d8-adb5-2afacbef528c', 'b75162b1-6df6-4307-ae15-5cd999c3f293', '4317f253-649d-4453-a2e2-33cc114b8d20', '45570cc0-6b65-4920-9cc0-2c8cc404cfd8', '84d7db8a-7bf4-4c5a-97a1-92079a5d3827', '2060d9f8-f893-44bf-8b10-1147abdd770e', '20b67577-5a56-44a4-8cd7-ca222f3ffe1e', '98f5e175-ea55-4f9a-a8e3-57f5e72c2b99', 'bdfd2ff2-bdaf-4b2a-93f9-f6059c48543d', '0931c0f2-a490-44e7-94e9-e697a8f14a25', '0c415312-f2c7-4cb7-8b0c-610e6022cebe', 'e99e6197-c87d-4997-9c1a-a7a773cd16a6', '7cb530a2-6707-44d4-8230-f6350eec6633', '662fc59e-1650-4f10-af4e-f517069299af', '68301a90-9ca9-4e35-b650-89e40cd83381', 'bd31a7c5-dc58-491f-9d46-a7e33df609a0', '2af17dee-840b-48b7-97ce-58ee3c30c636', '6b750930-cb18-4f47-b973-b3ae83a5670d', 'ac5a81a2-b1e7-48f7-8660-11d2ee4a8155', 'b5f8fdeb-b540-4698-9e80-5465aaa457dd', '78722194-4664-4d77-8049-55ab29c52019', 'ea956d74-6d2d-4c13-92d8-bc6dbbfb99b6', '1ebb6bce-9e77-43dc-ac99-25f473684dea', '3164e884-dbd6-4997-bc1f-7d7021d18621', '05a75e45-2205-4c6b-8848-de79593dd773', 'ce158a73-2efe-4176-925b-7cb31ee29eb1', 'e8aac383-e775-4c8f-9e71-dc83f1fdba72', '25a69d4e-393b-434e-8c31-595d65fb5ffa', '7cbe17cd-26e6-4b22-824c-7f8847fb66be', '7237c5fd-7944-4cee-b348-f66a7e4dc37f', '1f865aa0-f115-4746-8930-a96f43a9c940', '8f52221b-96de-4b55-aa3b-c8d8be25f227', '03b8eda4-0eaa-4f28-bbde-b43937177567', '3c793dca-6bb1-4a7e-ae2e-0211e901783e', '1fa9bc1a-dce7-4053-89d5-b15d561a4684', '622e7fb9-11eb-4391-a4bf-de7ac21c1318', '3ed2d80f-9191-48e5-8cd6-762b474cbb46', '5c297f97-ab24-4b4d-9fc0-91731eb53c47']
    Embedding status: 401/1691 documents embedded
    Embedding status: 571/1691 documents embedded
    Embedding status: 741/1691 documents embedded
    Embedding status: 941/1691 documents embedded
    Embedding status: 1211/1691 documents embedded
    Embedding status: 1431/1691 documents embedded
    Embedding status: 1691/1691 documents embedded
    

We see results from both books. Note the `source` metadata


```python
query = "Was he interested in astronomy?"
docs = await vs.asearch(query, search_type="similarity", k=3)

for d in docs:
    print(d.page_content, " -> ", d.metadata, "\n====\n")
```

    of astronomy, and its kindred sciences, with the various arts dependent

    on them. In none are computations more operose than those which

    astronomy in particular requires;--in none are preparatory facilities

    more needful;--in none is error more detrimental. The practical

    astronomer is interrupted in his pursuit, and diverted from his task of

    observation by the irksome labours of computation, or his diligence in

    observing becomes ineffectual for want of yet greater industry of  ->  {'source': 'https://www.gutenberg.org/cache/epub/71292/pg71292.txt'} 
    ====
    
    possess all knowledge which is likely to be useful to him in his work,

    and this I have endeavored in my case to do. If I remember rightly, you

    on one occasion, in the early days of our friendship, defined my limits

    in a very precise fashion.”

    

    “Yes,” I answered, laughing. “It was a singular document. Philosophy,

    astronomy, and politics were marked at zero, I remember. Botany

    variable, geology profound as regards the mud-stains from any region  ->  {'source': 'https://www.gutenberg.org/files/48320/48320-0.txt'} 
    ====
    
    easily admitted, that an assembly of eminent naturalists and physicians,

    with a sprinkling of astronomers, and one or two abstract

    mathematicians, were not precisely the persons best qualified to

    appreciate such an instrument of mechanical investigation as we have

    here described. We shall not therefore be understood as intending the

    slightest disrespect for these distinguished persons, when we express

    our regret, that a discovery of such paramount practical value, in a  ->  {'source': 'https://www.gutenberg.org/cache/epub/71292/pg71292.txt'} 
    ====
    
    

Now, we set up a filter


```python
filter = {
    "where": {
        "jsonpath": (
            "$[*] ? (@.source == 'https://www.gutenberg.org/files/48320/48320-0.txt')"
        )
    },
}

docs = await vs.asearch(query, search_type="similarity", metadata=filter, k=3)

for d in docs:
    print(d.page_content, " -> ", d.metadata, "\n====\n")
```

    possess all knowledge which is likely to be useful to him in his work,

    and this I have endeavored in my case to do. If I remember rightly, you

    on one occasion, in the early days of our friendship, defined my limits

    in a very precise fashion.”

    

    “Yes,” I answered, laughing. “It was a singular document. Philosophy,

    astronomy, and politics were marked at zero, I remember. Botany

    variable, geology profound as regards the mud-stains from any region  ->  {'source': 'https://www.gutenberg.org/files/48320/48320-0.txt'} 
    ====
    
    the evening than in the daylight, for he said that he hated to be

    conspicuous. Very retiring and gentlemanly he was. Even his voice was

    gentle. He’d had the quinsy and swollen glands when he was young, he

    told me, and it had left him with a weak throat, and a hesitating,

    whispering fashion of speech. He was always well dressed, very neat and

    plain, but his eyes were weak, just as mine are, and he wore tinted

    glasses against the glare.”  ->  {'source': 'https://www.gutenberg.org/files/48320/48320-0.txt'} 
    ====
    
    which was characteristic of him. “It is perhaps less suggestive than

    it might have been,” he remarked, “and yet there are a few inferences

    which are very distinct, and a few others which represent at least a

    strong balance of probability. That the man was highly intellectual

    is of course obvious upon the face of it, and also that he was fairly

    well-to-do within the last three years, although he has now fallen upon

    evil days. He had foresight, but has less now than formerly, pointing  ->  {'source': 'https://www.gutenberg.org/files/48320/48320-0.txt'} 
    ====
    
    


```python

```




################################################## zep_cloud_chat_message_history.md ##################################################


# ZepCloudChatMessageHistory
> Recall, understand, and extract data from chat histories. Power personalized AI experiences.

>[Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.
> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,
> while also reducing hallucinations, latency, and cost.

> See [Zep Cloud Installation Guide](https://help.getzep.com/sdks) and more [Zep Cloud Langchain Examples](https://github.com/getzep/zep-python/tree/main/examples)

## Example

This notebook demonstrates how to use [Zep](https://www.getzep.com/) to persist chat history and use Zep Memory with your chain.



```python
from uuid import uuid4

from langchain_community.chat_message_histories import ZepCloudChatMessageHistory
from langchain_community.memory.zep_cloud_memory import ZepCloudMemory
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import (
    RunnableParallel,
)
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI

session_id = str(uuid4())  # This is a unique identifier for the session
```

Provide your OpenAI key


```python
import getpass

openai_key = getpass.getpass()
```

Provide your Zep API key. See https://help.getzep.com/projects#api-keys



```python
zep_api_key = getpass.getpass()
```

Preload some messages into the memory. The default message window is 4 messages. We want to push beyond this to demonstrate auto-summarization.


```python
test_history = [
    {"role": "human", "content": "Who was Octavia Butler?"},
    {
        "role": "ai",
        "content": (
            "Octavia Estelle Butler (June 22, 1947 – February 24, 2006) was an American"
            " science fiction author."
        ),
    },
    {"role": "human", "content": "Which books of hers were made into movies?"},
    {
        "role": "ai",
        "content": (
            "The most well-known adaptation of Octavia Butler's work is the FX series"
            " Kindred, based on her novel of the same name."
        ),
    },
    {"role": "human", "content": "Who were her contemporaries?"},
    {
        "role": "ai",
        "content": (
            "Octavia Butler's contemporaries included Ursula K. Le Guin, Samuel R."
            " Delany, and Joanna Russ."
        ),
    },
    {"role": "human", "content": "What awards did she win?"},
    {
        "role": "ai",
        "content": (
            "Octavia Butler won the Hugo Award, the Nebula Award, and the MacArthur"
            " Fellowship."
        ),
    },
    {
        "role": "human",
        "content": "Which other women sci-fi writers might I want to read?",
    },
    {
        "role": "ai",
        "content": "You might want to read Ursula K. Le Guin or Joanna Russ.",
    },
    {
        "role": "human",
        "content": (
            "Write a short synopsis of Butler's book, Parable of the Sower. What is it"
            " about?"
        ),
    },
    {
        "role": "ai",
        "content": (
            "Parable of the Sower is a science fiction novel by Octavia Butler,"
            " published in 1993. It follows the story of Lauren Olamina, a young woman"
            " living in a dystopian future where society has collapsed due to"
            " environmental disasters, poverty, and violence."
        ),
        "metadata": {"foo": "bar"},
    },
]

zep_memory = ZepCloudMemory(
    session_id=session_id,
    api_key=zep_api_key,
)

for msg in test_history:
    zep_memory.chat_memory.add_message(
        HumanMessage(content=msg["content"])
        if msg["role"] == "human"
        else AIMessage(content=msg["content"])
    )

import time

time.sleep(
    10
)  # Wait for the messages to be embedded and summarized, this happens asynchronously.
```

**MessagesPlaceholder** - We’re using the variable name chat_history here. This will incorporate the chat history into the prompt.
It’s important that this variable name aligns with the history_messages_key in the RunnableWithMessageHistory chain for seamless integration.

**question** must match input_messages_key in `RunnableWithMessageHistory“ chain.


```python
template = """Be helpful and answer the question below using the provided context:
    """
answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", template),
        MessagesPlaceholder(variable_name="chat_history"),
        ("user", "{question}"),
    ]
)
```

We use RunnableWithMessageHistory to incorporate Zep’s Chat History into our chain. This class requires a session_id as a parameter when you activate the chain.


```python
inputs = RunnableParallel(
    {
        "question": lambda x: x["question"],
        "chat_history": lambda x: x["chat_history"],
    },
)
chain = RunnableWithMessageHistory(
    inputs | answer_prompt | ChatOpenAI(openai_api_key=openai_key) | StrOutputParser(),
    lambda s_id: ZepCloudChatMessageHistory(
        session_id=s_id,  # This uniquely identifies the conversation, note that we are getting session id as chain configurable field
        api_key=zep_api_key,
        memory_type="perpetual",
    ),
    input_messages_key="question",
    history_messages_key="chat_history",
)
```


```python
chain.invoke(
    {
        "question": "What is the book's relevance to the challenges facing contemporary society?"
    },
    config={"configurable": {"session_id": session_id}},
)
```

    Parent run 622c6f75-3e4a-413d-ba20-558c1fea0d50 not found for run af12a4b1-e882-432d-834f-e9147465faf6. Treating as a root run.
    




    '"Parable of the Sower" is relevant to the challenges facing contemporary society as it explores themes of environmental degradation, economic inequality, social unrest, and the search for hope and community in the face of chaos. The novel\'s depiction of a dystopian future where society has collapsed due to environmental and economic crises serves as a cautionary tale about the potential consequences of our current societal and environmental challenges. By addressing issues such as climate change, social injustice, and the impact of technology on humanity, Octavia Butler\'s work prompts readers to reflect on the pressing issues of our time and the importance of resilience, empathy, and collective action in building a better future.'




```python

```




################################################## zep_cloud_memorystore.md ##################################################


# Zep Cloud
## Retriever Example for [Zep Cloud](https://docs.getzep.com/)

> Recall, understand, and extract data from chat histories. Power personalized AI experiences.

> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.
> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,
> while also reducing hallucinations, latency, and cost.

> See [Zep Cloud Installation Guide](https://help.getzep.com/sdks) and more [Zep Cloud Langchain Examples](https://github.com/getzep/zep-python/tree/main/examples)

## Retriever Example

This notebook demonstrates how to search historical chat message histories using the [Zep Long-term Memory Store](https://www.getzep.com/).

We'll demonstrate:

1. Adding conversation history to the Zep memory store.
2. Vector search over the conversation history: 
    1. With a similarity search over chat messages
    2. Using maximal marginal relevance re-ranking of a chat message search
    3. Filtering a search using metadata filters
    4. A similarity search over summaries of the chat messages
    5. Using maximal marginal relevance re-ranking of a summary search




```python
import getpass
import time
from uuid import uuid4

from langchain_community.memory.zep_cloud_memory import ZepCloudMemory
from langchain_community.retrievers import ZepCloudRetriever
from langchain_core.messages import AIMessage, HumanMessage

# Provide your Zep API key.
zep_api_key = getpass.getpass()
```

### Initialize the Zep Chat Message History Class and add a chat message history to the memory store

**NOTE:** Unlike other Retrievers, the content returned by the Zep Retriever is session/user specific. A `session_id` is required when instantiating the Retriever.


```python
session_id = str(uuid4())  # This is a unique identifier for the user/session

# Initialize the Zep Memory Class
zep_memory = ZepCloudMemory(session_id=session_id, api_key=zep_api_key)
```


```python
# Preload some messages into the memory. The default message window is 4 messages. We want to push beyond this to demonstrate auto-summarization.
test_history = [
    {"role": "human", "role_type": "user", "content": "Who was Octavia Butler?"},
    {
        "role": "ai",
        "role_type": "assistant",
        "content": (
            "Octavia Estelle Butler (June 22, 1947 – February 24, 2006) was an American"
            " science fiction author."
        ),
    },
    {
        "role": "human",
        "role_type": "user",
        "content": "Which books of hers were made into movies?",
    },
    {
        "role": "ai",
        "role_type": "assistant",
        "content": (
            "The most well-known adaptation of Octavia Butler's work is the FX series"
            " Kindred, based on her novel of the same name."
        ),
    },
    {"role": "human", "role_type": "user", "content": "Who were her contemporaries?"},
    {
        "role": "ai",
        "role_type": "assistant",
        "content": (
            "Octavia Butler's contemporaries included Ursula K. Le Guin, Samuel R."
            " Delany, and Joanna Russ."
        ),
    },
    {"role": "human", "role_type": "user", "content": "What awards did she win?"},
    {
        "role": "ai",
        "role_type": "assistant",
        "content": (
            "Octavia Butler won the Hugo Award, the Nebula Award, and the MacArthur"
            " Fellowship."
        ),
    },
    {
        "role": "human",
        "role_type": "user",
        "content": "Which other women sci-fi writers might I want to read?",
    },
    {
        "role": "ai",
        "role_type": "assistant",
        "content": "You might want to read Ursula K. Le Guin or Joanna Russ.",
    },
    {
        "role": "human",
        "role_type": "user",
        "content": (
            "Write a short synopsis of Butler's book, Parable of the Sower. What is it"
            " about?"
        ),
    },
    {
        "role": "ai",
        "role_type": "assistant",
        "content": (
            "Parable of the Sower is a science fiction novel by Octavia Butler,"
            " published in 1993. It follows the story of Lauren Olamina, a young woman"
            " living in a dystopian future where society has collapsed due to"
            " environmental disasters, poverty, and violence."
        ),
    },
    {
        "role": "human",
        "role_type": "user",
        "content": "What is the setting of the book?",
    },
    {
        "role": "ai",
        "role_type": "assistant",
        "content": (
            "The book is set in a dystopian future in the 2020s, where society has"
            " collapsed due to climate change and economic crises."
        ),
    },
    {"role": "human", "role_type": "user", "content": "Who is the protagonist?"},
    {
        "role": "ai",
        "role_type": "assistant",
        "content": (
            "The protagonist of the book is Lauren Olamina, a young woman who possesses"
            " 'hyperempathy', the ability to feel pain and other sensations she"
            " witnesses."
        ),
    },
    {
        "role": "human",
        "role_type": "user",
        "content": "What is the main theme of the book?",
    },
    {
        "role": "ai",
        "role_type": "assistant",
        "content": (
            "The main theme of the book is survival in the face of drastic societal"
            " change and collapse. It also explores themes of adaptability, community,"
            " and the human capacity for change."
        ),
    },
    {
        "role": "human",
        "role_type": "user",
        "content": "What is the 'Parable of the Sower'?",
    },
    {
        "role": "ai",
        "role_type": "assistant",
        "content": (
            "The 'Parable of the Sower' is a biblical parable that Butler uses as a"
            " metaphor in the book. In the parable, a sower scatters seeds, some of"
            " which fall on fertile ground and grow, while others fall on rocky ground"
            " or among thorns and fail to grow. The parable is used to illustrate the"
            " importance of receptivity and preparedness in the face of change."
        ),
    },
    {
        "role": "human",
        "role_type": "user",
        "content": "What is Butler's writing style like?",
    },
    {
        "role": "ai",
        "role_type": "assistant",
        "content": (
            "Butler's writing style is known for its clarity, directness, and"
            " psychological insight. Her narratives often involve complex, diverse"
            " characters and explore themes of race, gender, and power."
        ),
    },
    {
        "role": "human",
        "role_type": "user",
        "content": "What other books has she written?",
    },
    {
        "role": "ai",
        "content": (
            "In addition to 'Parable of the Sower', Butler has written several other"
            " notable works, including 'Kindred', 'Dawn', and 'Parable of the Talents'."
        ),
    },
]

for msg in test_history:
    zep_memory.chat_memory.add_message(
        HumanMessage(content=msg["content"])
        if msg["role"] == "human"
        else AIMessage(content=msg["content"])
    )

time.sleep(
    10
)  # Wait for the messages to be embedded and summarized, this happens asynchronously.
```

### Use the Zep Retriever to vector search over the Zep memory

Zep provides native vector search over historical conversation memory. Embedding happens automatically.

NOTE: Embedding of messages occurs asynchronously, so the first query may not return results. Subsequent queries will return results as the embeddings are generated.


```python
zep_retriever = ZepCloudRetriever(
    api_key=zep_api_key,
    session_id=session_id,  # Ensure that you provide the session_id when instantiating the Retriever
    top_k=5,
)

await zep_retriever.ainvoke("Who wrote Parable of the Sower?")
```




    [Document(page_content="What is the 'Parable of the Sower'?", metadata={'score': 0.9333381652832031, 'uuid': 'bebc441c-a32d-44a1-ae61-968e7b3d4956', 'created_at': '2024-05-10T05:02:01.857627Z', 'token_count': 11, 'role': 'human'}),
     Document(page_content="The 'Parable of the Sower' is a biblical parable that Butler uses as a metaphor in the book. In the parable, a sower scatters seeds, some of which fall on fertile ground and grow, while others fall on rocky ground or among thorns and fail to grow. The parable is used to illustrate the importance of receptivity and preparedness in the face of change.", metadata={'score': 0.8757256865501404, 'uuid': '193c60d8-2b7b-4eb1-a4be-c2d8afd92991', 'created_at': '2024-05-10T05:02:01.97174Z', 'token_count': 82, 'role': 'ai'}),
     Document(page_content="Write a short synopsis of Butler's book, Parable of the Sower. What is it about?", metadata={'score': 0.8641344904899597, 'uuid': 'fc78901d-a625-4530-ba63-1ae3e3b11683', 'created_at': '2024-05-10T05:02:00.942994Z', 'token_count': 21, 'role': 'human'}),
     Document(page_content='Parable of the Sower is a science fiction novel by Octavia Butler, published in 1993. It follows the story of Lauren Olamina, a young woman living in a dystopian future where society has collapsed due to environmental disasters, poverty, and violence.', metadata={'score': 0.8581685125827789, 'uuid': '91f2cda4-276e-446d-96bf-07d34e5af616', 'created_at': '2024-05-10T05:02:01.05577Z', 'token_count': 54, 'role': 'ai'}),
     Document(page_content="In addition to 'Parable of the Sower', Butler has written several other notable works, including 'Kindred', 'Dawn', and 'Parable of the Talents'.", metadata={'score': 0.8076582252979279, 'uuid': 'e3994519-9a90-410c-b14c-2c652f6d184f', 'created_at': '2024-05-10T05:02:02.401682Z', 'token_count': 37, 'role': 'ai'})]



We can also use the Zep sync API to retrieve results:


```python
zep_retriever.invoke("Who wrote Parable of the Sower?")
```




    [Document(page_content='Parable of the Sower is a science fiction novel by Octavia Butler set in a dystopian future in the 2020s. The story follows Lauren Olamina, a young woman living in a society that has collapsed due to environmental disasters, poverty, and violence. The novel explores themes of societal breakdown, the struggle for survival, and the search for a better future.', metadata={'score': 0.8473024368286133, 'uuid': 'e4689f8e-33be-4a59-a9c2-e5ef5dd70f74', 'created_at': '2024-05-10T05:02:02.713123Z', 'token_count': 76})]



### Reranking using MMR (Maximal Marginal Relevance)

Zep has native, SIMD-accelerated support for reranking results using MMR. This is useful for removing redundancy in results.


```python
zep_retriever = ZepCloudRetriever(
    api_key=zep_api_key,
    session_id=session_id,  # Ensure that you provide the session_id when instantiating the Retriever
    top_k=5,
    search_type="mmr",
    mmr_lambda=0.5,
)

await zep_retriever.ainvoke("Who wrote Parable of the Sower?")
```

### Using metadata filters to refine search results

Zep supports filtering results by metadata. This is useful for filtering results by entity type, or other metadata.

More information here: https://help.getzep.com/document-collections#searching-a-collection-with-hybrid-vector-search


```python
filter = {"where": {"jsonpath": '$[*] ? (@.baz == "qux")'}}

await zep_retriever.ainvoke(
    "Who wrote Parable of the Sower?", config={"metadata": filter}
)
```

### Searching over Summaries with MMR Reranking

Zep automatically generates summaries of chat messages. These summaries can be searched over using the Zep Retriever. Since a summary is a distillation of a conversation, they're more likely to match your search query and offer rich, succinct context to the LLM.

Successive summaries may include similar content, with Zep's similarity search returning the highest matching results but with little diversity.
MMR re-ranks the results to ensure that the summaries you populate into your prompt are both relevant and each offers additional information to the LLM.


```python
zep_retriever = ZepCloudRetriever(
    api_key=zep_api_key,
    session_id=session_id,  # Ensure that you provide the session_id when instantiating the Retriever
    top_k=3,
    search_scope="summary",
    search_type="mmr",
    mmr_lambda=0.5,
)

await zep_retriever.ainvoke("Who wrote Parable of the Sower?")
```




    [Document(page_content='Parable of the Sower is a science fiction novel by Octavia Butler set in a dystopian future in the 2020s. The story follows Lauren Olamina, a young woman living in a society that has collapsed due to environmental disasters, poverty, and violence. The novel explores themes of societal breakdown, the struggle for survival, and the search for a better future.', metadata={'score': 0.8473024368286133, 'uuid': 'e4689f8e-33be-4a59-a9c2-e5ef5dd70f74', 'created_at': '2024-05-10T05:02:02.713123Z', 'token_count': 76}),
     Document(page_content='The \'Parable of the Sower\' refers to a new religious belief system that the protagonist, Lauren Olamina, develops over the course of the novel. As her community disintegrates due to climate change, economic collapse, and social unrest, Lauren comes to believe that humanity must adapt and "shape God" in order to survive. The \'Parable of the Sower\' is the foundational text of this new religion, which Lauren calls "Earthseed", that emphasizes the inevitability of change and the need for humanity to take an active role in shaping its own future. This parable is a central thematic element of the novel, representing the protagonist\'s search for meaning and purpose in the face of societal upheaval.', metadata={'score': 0.8466987311840057, 'uuid': '1f1a44eb-ebd8-4617-ac14-0281099bd770', 'created_at': '2024-05-10T05:02:07.541073Z', 'token_count': 146}),
     Document(page_content='The dialog discusses the central themes of Octavia Butler\'s acclaimed science fiction novel "Parable of the Sower." The main theme is survival in the face of drastic societal collapse, and the importance of adaptability, community, and the human capacity for change. The "Parable of the Sower," a biblical parable, serves as a metaphorical framework for the novel, illustrating the need for receptivity and preparedness when confronting transformative upheaval.', metadata={'score': 0.8283970355987549, 'uuid': '4158a750-3ccd-45ce-ab88-fed5ba68b755', 'created_at': '2024-05-10T05:02:06.510068Z', 'token_count': 91})]




```python

```




################################################## zep_memory.md ##################################################


# Zep Open Source Memory
> Recall, understand, and extract data from chat histories. Power personalized AI experiences.

>[Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.
> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,
> while also reducing hallucinations, latency, and cost.

> Interested in Zep Cloud? See [Zep Cloud Installation Guide](https://help.getzep.com/sdks) and [Zep Cloud Memory Example](https://help.getzep.com/langchain/examples/messagehistory-example)

## Open Source Installation and Setup

> Zep Open Source project: [https://github.com/getzep/zep](https://github.com/getzep/zep)
>
> Zep Open Source Docs: [https://docs.getzep.com/](https://docs.getzep.com/)

## Example

This notebook demonstrates how to use [Zep](https://www.getzep.com/) as memory for your chatbot.
REACT Agent Chat Message History with Zep - A long-term memory store for LLM applications.

We'll demonstrate:

1. Adding conversation history to Zep.
2. Running an agent and having message automatically added to the store.
3. Viewing the enriched messages.
4. Vector search over the conversation history.


```python
from uuid import uuid4

from langchain.agents import AgentType, initialize_agent
from langchain_community.memory.zep_memory import ZepMemory
from langchain_community.retrievers import ZepRetriever
from langchain_community.utilities import WikipediaAPIWrapper
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.tools import Tool
from langchain_openai import OpenAI

# Set this to your Zep server URL
ZEP_API_URL = "http://localhost:8000"

session_id = str(uuid4())  # This is a unique identifier for the user
```


```python
# Provide your OpenAI key
import getpass

openai_key = getpass.getpass()
```


```python
# Provide your Zep API key. Note that this is optional. See https://docs.getzep.com/deployment/auth

zep_api_key = getpass.getpass()
```

### Initialize the Zep Chat Message History Class and initialize the Agent



```python
search = WikipediaAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description=(
            "useful for when you need to search online for answers. You should ask"
            " targeted questions"
        ),
    ),
]

# Set up Zep Chat History
memory = ZepMemory(
    session_id=session_id,
    url=ZEP_API_URL,
    api_key=zep_api_key,
    memory_key="chat_history",
)

# Initialize the agent
llm = OpenAI(temperature=0, openai_api_key=openai_key)
agent_chain = initialize_agent(
    tools,
    llm,
    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
    verbose=True,
    memory=memory,
)
```

### Add some history data



```python
# Preload some messages into the memory. The default message window is 12 messages. We want to push beyond this to demonstrate auto-summarization.
test_history = [
    {"role": "human", "content": "Who was Octavia Butler?"},
    {
        "role": "ai",
        "content": (
            "Octavia Estelle Butler (June 22, 1947 – February 24, 2006) was an American"
            " science fiction author."
        ),
    },
    {"role": "human", "content": "Which books of hers were made into movies?"},
    {
        "role": "ai",
        "content": (
            "The most well-known adaptation of Octavia Butler's work is the FX series"
            " Kindred, based on her novel of the same name."
        ),
    },
    {"role": "human", "content": "Who were her contemporaries?"},
    {
        "role": "ai",
        "content": (
            "Octavia Butler's contemporaries included Ursula K. Le Guin, Samuel R."
            " Delany, and Joanna Russ."
        ),
    },
    {"role": "human", "content": "What awards did she win?"},
    {
        "role": "ai",
        "content": (
            "Octavia Butler won the Hugo Award, the Nebula Award, and the MacArthur"
            " Fellowship."
        ),
    },
    {
        "role": "human",
        "content": "Which other women sci-fi writers might I want to read?",
    },
    {
        "role": "ai",
        "content": "You might want to read Ursula K. Le Guin or Joanna Russ.",
    },
    {
        "role": "human",
        "content": (
            "Write a short synopsis of Butler's book, Parable of the Sower. What is it"
            " about?"
        ),
    },
    {
        "role": "ai",
        "content": (
            "Parable of the Sower is a science fiction novel by Octavia Butler,"
            " published in 1993. It follows the story of Lauren Olamina, a young woman"
            " living in a dystopian future where society has collapsed due to"
            " environmental disasters, poverty, and violence."
        ),
        "metadata": {"foo": "bar"},
    },
]

for msg in test_history:
    memory.chat_memory.add_message(
        (
            HumanMessage(content=msg["content"])
            if msg["role"] == "human"
            else AIMessage(content=msg["content"])
        ),
        metadata=msg.get("metadata", {}),
    )
```

### Run the agent

Doing so will automatically add the input and response to the Zep memory.



```python
agent_chain.run(
    input="What is the book's relevance to the challenges facing contemporary society?",
)
```

    
    
    [1m> Entering new  chain...[0m
    [32;1m[1;3mThought: Do I need to use a tool? No
    AI: Parable of the Sower is a prescient novel that speaks to the challenges facing contemporary society, such as climate change, inequality, and violence. It is a cautionary tale that warns of the dangers of unchecked greed and the need for individuals to take responsibility for their own lives and the lives of those around them.[0m
    
    [1m> Finished chain.[0m
    




    'Parable of the Sower is a prescient novel that speaks to the challenges facing contemporary society, such as climate change, inequality, and violence. It is a cautionary tale that warns of the dangers of unchecked greed and the need for individuals to take responsibility for their own lives and the lives of those around them.'



### Inspect the Zep memory

Note the summary, and that the history has been enriched with token counts, UUIDs, and timestamps.

Summaries are biased towards the most recent messages.



```python
def print_messages(messages):
    for m in messages:
        print(m.type, ":\n", m.dict())


print(memory.chat_memory.zep_summary)
print("\n")
print_messages(memory.chat_memory.messages)
```

    The human inquires about Octavia Butler. The AI identifies her as an American science fiction author. The human then asks which books of hers were made into movies. The AI responds by mentioning the FX series Kindred, based on her novel of the same name. The human then asks about her contemporaries, and the AI lists Ursula K. Le Guin, Samuel R. Delany, and Joanna Russ.
    
    
    system :
     {'content': 'The human inquires about Octavia Butler. The AI identifies her as an American science fiction author. The human then asks which books of hers were made into movies. The AI responds by mentioning the FX series Kindred, based on her novel of the same name. The human then asks about her contemporaries, and the AI lists Ursula K. Le Guin, Samuel R. Delany, and Joanna Russ.', 'additional_kwargs': {}}
    human :
     {'content': 'What awards did she win?', 'additional_kwargs': {'uuid': '6b733f0b-6778-49ae-b3ec-4e077c039f31', 'created_at': '2023-07-09T19:23:16.611232Z', 'token_count': 8, 'metadata': {'system': {'entities': [], 'intent': 'The subject is inquiring about the awards that someone, whose identity is not specified, has won.'}}}, 'example': False}
    ai :
     {'content': 'Octavia Butler won the Hugo Award, the Nebula Award, and the MacArthur Fellowship.', 'additional_kwargs': {'uuid': '2f6d80c6-3c08-4fd4-8d4e-7bbee341ac90', 'created_at': '2023-07-09T19:23:16.618947Z', 'token_count': 21, 'metadata': {'system': {'entities': [{'Label': 'PERSON', 'Matches': [{'End': 14, 'Start': 0, 'Text': 'Octavia Butler'}], 'Name': 'Octavia Butler'}, {'Label': 'WORK_OF_ART', 'Matches': [{'End': 33, 'Start': 19, 'Text': 'the Hugo Award'}], 'Name': 'the Hugo Award'}, {'Label': 'EVENT', 'Matches': [{'End': 81, 'Start': 57, 'Text': 'the MacArthur Fellowship'}], 'Name': 'the MacArthur Fellowship'}], 'intent': 'The subject is stating that Octavia Butler received the Hugo Award, the Nebula Award, and the MacArthur Fellowship.'}}}, 'example': False}
    human :
     {'content': 'Which other women sci-fi writers might I want to read?', 'additional_kwargs': {'uuid': 'ccdcc901-ea39-4981-862f-6fe22ab9289b', 'created_at': '2023-07-09T19:23:16.62678Z', 'token_count': 14, 'metadata': {'system': {'entities': [], 'intent': 'The subject is seeking recommendations for additional women science fiction writers to explore.'}}}, 'example': False}
    ai :
     {'content': 'You might want to read Ursula K. Le Guin or Joanna Russ.', 'additional_kwargs': {'uuid': '7977099a-0c62-4c98-bfff-465bbab6c9c3', 'created_at': '2023-07-09T19:23:16.631721Z', 'token_count': 18, 'metadata': {'system': {'entities': [{'Label': 'ORG', 'Matches': [{'End': 40, 'Start': 23, 'Text': 'Ursula K. Le Guin'}], 'Name': 'Ursula K. Le Guin'}, {'Label': 'PERSON', 'Matches': [{'End': 55, 'Start': 44, 'Text': 'Joanna Russ'}], 'Name': 'Joanna Russ'}], 'intent': 'The subject is suggesting that the person should consider reading the works of Ursula K. Le Guin or Joanna Russ.'}}}, 'example': False}
    human :
     {'content': "Write a short synopsis of Butler's book, Parable of the Sower. What is it about?", 'additional_kwargs': {'uuid': 'e439b7e6-286a-4278-a8cb-dc260fa2e089', 'created_at': '2023-07-09T19:23:16.63623Z', 'token_count': 23, 'metadata': {'system': {'entities': [{'Label': 'ORG', 'Matches': [{'End': 32, 'Start': 26, 'Text': 'Butler'}], 'Name': 'Butler'}, {'Label': 'WORK_OF_ART', 'Matches': [{'End': 61, 'Start': 41, 'Text': 'Parable of the Sower'}], 'Name': 'Parable of the Sower'}], 'intent': 'The subject is requesting a brief summary or explanation of the book "Parable of the Sower" by Butler.'}}}, 'example': False}
    ai :
     {'content': 'Parable of the Sower is a science fiction novel by Octavia Butler, published in 1993. It follows the story of Lauren Olamina, a young woman living in a dystopian future where society has collapsed due to environmental disasters, poverty, and violence.', 'additional_kwargs': {'uuid': '6760489b-19c9-41aa-8b45-fae6cb1d7ee6', 'created_at': '2023-07-09T19:23:16.647524Z', 'token_count': 56, 'metadata': {'foo': 'bar', 'system': {'entities': [{'Label': 'GPE', 'Matches': [{'End': 20, 'Start': 15, 'Text': 'Sower'}], 'Name': 'Sower'}, {'Label': 'PERSON', 'Matches': [{'End': 65, 'Start': 51, 'Text': 'Octavia Butler'}], 'Name': 'Octavia Butler'}, {'Label': 'DATE', 'Matches': [{'End': 84, 'Start': 80, 'Text': '1993'}], 'Name': '1993'}, {'Label': 'PERSON', 'Matches': [{'End': 124, 'Start': 110, 'Text': 'Lauren Olamina'}], 'Name': 'Lauren Olamina'}], 'intent': 'The subject is providing information about the novel "Parable of the Sower" by Octavia Butler, including its genre, publication date, and a brief summary of the plot.'}}}, 'example': False}
    human :
     {'content': "What is the book's relevance to the challenges facing contemporary society?", 'additional_kwargs': {'uuid': '7dbbbb93-492b-4739-800f-cad2b6e0e764', 'created_at': '2023-07-09T19:23:19.315182Z', 'token_count': 15, 'metadata': {'system': {'entities': [], 'intent': 'The subject is asking about the relevance of a book to the challenges currently faced by society.'}}}, 'example': False}
    ai :
     {'content': 'Parable of the Sower is a prescient novel that speaks to the challenges facing contemporary society, such as climate change, inequality, and violence. It is a cautionary tale that warns of the dangers of unchecked greed and the need for individuals to take responsibility for their own lives and the lives of those around them.', 'additional_kwargs': {'uuid': '3e14ac8f-b7c1-4360-958b-9f3eae1f784f', 'created_at': '2023-07-09T19:23:19.332517Z', 'token_count': 66, 'metadata': {'system': {'entities': [{'Label': 'GPE', 'Matches': [{'End': 20, 'Start': 15, 'Text': 'Sower'}], 'Name': 'Sower'}], 'intent': 'The subject is providing an analysis and evaluation of the novel "Parable of the Sower" and highlighting its relevance to contemporary societal challenges.'}}}, 'example': False}
    

### Vector search over the Zep memory

Zep provides native vector search over historical conversation memory via the `ZepRetriever`.

You can use the `ZepRetriever` with chains that support passing in a Langchain `Retriever` object.



```python
retriever = ZepRetriever(
    session_id=session_id,
    url=ZEP_API_URL,
    api_key=zep_api_key,
)

search_results = memory.chat_memory.search("who are some famous women sci-fi authors?")
for r in search_results:
    if r.dist > 0.8:  # Only print results with similarity of 0.8 or higher
        print(r.message, r.dist)
```

    {'uuid': 'ccdcc901-ea39-4981-862f-6fe22ab9289b', 'created_at': '2023-07-09T19:23:16.62678Z', 'role': 'human', 'content': 'Which other women sci-fi writers might I want to read?', 'metadata': {'system': {'entities': [], 'intent': 'The subject is seeking recommendations for additional women science fiction writers to explore.'}}, 'token_count': 14} 0.9119619869747062
    {'uuid': '7977099a-0c62-4c98-bfff-465bbab6c9c3', 'created_at': '2023-07-09T19:23:16.631721Z', 'role': 'ai', 'content': 'You might want to read Ursula K. Le Guin or Joanna Russ.', 'metadata': {'system': {'entities': [{'Label': 'ORG', 'Matches': [{'End': 40, 'Start': 23, 'Text': 'Ursula K. Le Guin'}], 'Name': 'Ursula K. Le Guin'}, {'Label': 'PERSON', 'Matches': [{'End': 55, 'Start': 44, 'Text': 'Joanna Russ'}], 'Name': 'Joanna Russ'}], 'intent': 'The subject is suggesting that the person should consider reading the works of Ursula K. Le Guin or Joanna Russ.'}}, 'token_count': 18} 0.8534346954749745
    {'uuid': 'b05e2eb5-c103-4973-9458-928726f08655', 'created_at': '2023-07-09T19:23:16.603098Z', 'role': 'ai', 'content': "Octavia Butler's contemporaries included Ursula K. Le Guin, Samuel R. Delany, and Joanna Russ.", 'metadata': {'system': {'entities': [{'Label': 'PERSON', 'Matches': [{'End': 16, 'Start': 0, 'Text': "Octavia Butler's"}], 'Name': "Octavia Butler's"}, {'Label': 'ORG', 'Matches': [{'End': 58, 'Start': 41, 'Text': 'Ursula K. Le Guin'}], 'Name': 'Ursula K. Le Guin'}, {'Label': 'PERSON', 'Matches': [{'End': 76, 'Start': 60, 'Text': 'Samuel R. Delany'}], 'Name': 'Samuel R. Delany'}, {'Label': 'PERSON', 'Matches': [{'End': 93, 'Start': 82, 'Text': 'Joanna Russ'}], 'Name': 'Joanna Russ'}], 'intent': "The subject is stating that Octavia Butler's contemporaries included Ursula K. Le Guin, Samuel R. Delany, and Joanna Russ."}}, 'token_count': 27} 0.8523831524040919
    {'uuid': 'e346f02b-f854-435d-b6ba-fb394a416b9b', 'created_at': '2023-07-09T19:23:16.556587Z', 'role': 'human', 'content': 'Who was Octavia Butler?', 'metadata': {'system': {'entities': [{'Label': 'PERSON', 'Matches': [{'End': 22, 'Start': 8, 'Text': 'Octavia Butler'}], 'Name': 'Octavia Butler'}], 'intent': 'The subject is asking for information about the identity or background of Octavia Butler.'}}, 'token_count': 8} 0.8236355436055457
    {'uuid': '42ff41d2-c63a-4d5b-b19b-d9a87105cfc3', 'created_at': '2023-07-09T19:23:16.578022Z', 'role': 'ai', 'content': 'Octavia Estelle Butler (June 22, 1947 – February 24, 2006) was an American science fiction author.', 'metadata': {'system': {'entities': [{'Label': 'PERSON', 'Matches': [{'End': 22, 'Start': 0, 'Text': 'Octavia Estelle Butler'}], 'Name': 'Octavia Estelle Butler'}, {'Label': 'DATE', 'Matches': [{'End': 37, 'Start': 24, 'Text': 'June 22, 1947'}], 'Name': 'June 22, 1947'}, {'Label': 'DATE', 'Matches': [{'End': 57, 'Start': 40, 'Text': 'February 24, 2006'}], 'Name': 'February 24, 2006'}, {'Label': 'NORP', 'Matches': [{'End': 74, 'Start': 66, 'Text': 'American'}], 'Name': 'American'}], 'intent': 'The subject is providing information about Octavia Estelle Butler, who was an American science fiction author.'}}, 'token_count': 31} 0.8206687242257686
    {'uuid': '2f6d80c6-3c08-4fd4-8d4e-7bbee341ac90', 'created_at': '2023-07-09T19:23:16.618947Z', 'role': 'ai', 'content': 'Octavia Butler won the Hugo Award, the Nebula Award, and the MacArthur Fellowship.', 'metadata': {'system': {'entities': [{'Label': 'PERSON', 'Matches': [{'End': 14, 'Start': 0, 'Text': 'Octavia Butler'}], 'Name': 'Octavia Butler'}, {'Label': 'WORK_OF_ART', 'Matches': [{'End': 33, 'Start': 19, 'Text': 'the Hugo Award'}], 'Name': 'the Hugo Award'}, {'Label': 'EVENT', 'Matches': [{'End': 81, 'Start': 57, 'Text': 'the MacArthur Fellowship'}], 'Name': 'the MacArthur Fellowship'}], 'intent': 'The subject is stating that Octavia Butler received the Hugo Award, the Nebula Award, and the MacArthur Fellowship.'}}, 'token_count': 21} 0.8199012397683285
    


```python

```




################################################## zep_memorystore.md ##################################################


# Zep Open Source
## Retriever Example for [Zep](https://docs.getzep.com/)

> Recall, understand, and extract data from chat histories. Power personalized AI experiences.

> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.
> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,
> while also reducing hallucinations, latency, and cost.

> Interested in Zep Cloud? See [Zep Cloud Installation Guide](https://help.getzep.com/sdks) and [Zep Cloud Retriever Example](https://help.getzep.com/langchain/examples/rag-message-history-example)

## Open Source Installation and Setup

> Zep Open Source project: [https://github.com/getzep/zep](https://github.com/getzep/zep)
> Zep Open Source Docs: [https://docs.getzep.com/](https://docs.getzep.com/)

## Retriever Example

This notebook demonstrates how to search historical chat message histories using the [Zep Long-term Memory Store](https://getzep.github.io/).

We'll demonstrate:

1. Adding conversation history to the Zep memory store.
2. Vector search over the conversation history: 
    1. With a similarity search over chat messages
    2. Using maximal marginal relevance re-ranking of a chat message search
    3. Filtering a search using metadata filters
    4. A similarity search over summaries of the chat messages
    5. Using maximal marginal relevance re-ranking of a summary search




```python
import getpass
import time
from uuid import uuid4

from langchain_community.memory.zep_memory import ZepMemory
from langchain_core.messages import AIMessage, HumanMessage

# Set this to your Zep server URL
ZEP_API_URL = "http://localhost:8000"
```

### Initialize the Zep Chat Message History Class and add a chat message history to the memory store

**NOTE:** Unlike other Retrievers, the content returned by the Zep Retriever is session/user specific. A `session_id` is required when instantiating the Retriever.


```python
# Provide your Zep API key. Note that this is optional. See https://docs.getzep.com/deployment/auth
AUTHENTICATE = False

zep_api_key = None
if AUTHENTICATE:
    zep_api_key = getpass.getpass()
```


```python
session_id = str(uuid4())  # This is a unique identifier for the user/session

# Initialize the Zep Memory Class
zep_memory = ZepMemory(session_id=session_id, url=ZEP_API_URL, api_key=zep_api_key)
```


```python
# Preload some messages into the memory. The default message window is 12 messages. We want to push beyond this to demonstrate auto-summarization.
test_history = [
    {"role": "human", "content": "Who was Octavia Butler?"},
    {
        "role": "ai",
        "content": (
            "Octavia Estelle Butler (June 22, 1947 – February 24, 2006) was an American"
            " science fiction author."
        ),
    },
    {"role": "human", "content": "Which books of hers were made into movies?"},
    {
        "role": "ai",
        "content": (
            "The most well-known adaptation of Octavia Butler's work is the FX series"
            " Kindred, based on her novel of the same name."
        ),
    },
    {"role": "human", "content": "Who were her contemporaries?"},
    {
        "role": "ai",
        "content": (
            "Octavia Butler's contemporaries included Ursula K. Le Guin, Samuel R."
            " Delany, and Joanna Russ."
        ),
    },
    {"role": "human", "content": "What awards did she win?"},
    {
        "role": "ai",
        "content": (
            "Octavia Butler won the Hugo Award, the Nebula Award, and the MacArthur"
            " Fellowship."
        ),
    },
    {
        "role": "human",
        "content": "Which other women sci-fi writers might I want to read?",
    },
    {
        "role": "ai",
        "content": "You might want to read Ursula K. Le Guin or Joanna Russ.",
    },
    {
        "role": "human",
        "content": (
            "Write a short synopsis of Butler's book, Parable of the Sower. What is it"
            " about?"
        ),
    },
    {
        "role": "ai",
        "content": (
            "Parable of the Sower is a science fiction novel by Octavia Butler,"
            " published in 1993. It follows the story of Lauren Olamina, a young woman"
            " living in a dystopian future where society has collapsed due to"
            " environmental disasters, poverty, and violence."
        ),
    },
    {"role": "human", "content": "What is the setting of the book?"},
    {
        "role": "ai",
        "content": (
            "The book is set in a dystopian future in the 2020s, where society has"
            " collapsed due to climate change and economic crises."
        ),
    },
    {"role": "human", "content": "Who is the protagonist?"},
    {
        "role": "ai",
        "content": (
            "The protagonist of the book is Lauren Olamina, a young woman who possesses"
            " 'hyperempathy', the ability to feel pain and other sensations she"
            " witnesses."
        ),
    },
    {"role": "human", "content": "What is the main theme of the book?"},
    {
        "role": "ai",
        "content": (
            "The main theme of the book is survival in the face of drastic societal"
            " change and collapse. It also explores themes of adaptability, community,"
            " and the human capacity for change."
        ),
    },
    {"role": "human", "content": "What is the 'Parable of the Sower'?"},
    {
        "role": "ai",
        "content": (
            "The 'Parable of the Sower' is a biblical parable that Butler uses as a"
            " metaphor in the book. In the parable, a sower scatters seeds, some of"
            " which fall on fertile ground and grow, while others fall on rocky ground"
            " or among thorns and fail to grow. The parable is used to illustrate the"
            " importance of receptivity and preparedness in the face of change."
        ),
    },
    {"role": "human", "content": "What is Butler's writing style like?"},
    {
        "role": "ai",
        "content": (
            "Butler's writing style is known for its clarity, directness, and"
            " psychological insight. Her narratives often involve complex, diverse"
            " characters and explore themes of race, gender, and power."
        ),
    },
    {"role": "human", "content": "What other books has she written?"},
    {
        "role": "ai",
        "content": (
            "In addition to 'Parable of the Sower', Butler has written several other"
            " notable works, including 'Kindred', 'Dawn', and 'Parable of the Talents'."
        ),
    },
]

for msg in test_history:
    zep_memory.chat_memory.add_message(
        HumanMessage(content=msg["content"])
        if msg["role"] == "human"
        else AIMessage(content=msg["content"])
    )

time.sleep(
    10
)  # Wait for the messages to be embedded and summarized. Speed depends on OpenAI API latency and your rate limits.
```

### Use the Zep Retriever to vector search over the Zep memory

Zep provides native vector search over historical conversation memory. Embedding happens automatically.

NOTE: Embedding of messages occurs asynchronously, so the first query may not return results. Subsequent queries will return results as the embeddings are generated.


```python
from langchain_community.retrievers.zep import SearchScope, SearchType, ZepRetriever

zep_retriever = ZepRetriever(
    session_id=session_id,  # Ensure that you provide the session_id when instantiating the Retriever
    url=ZEP_API_URL,
    top_k=5,
    api_key=zep_api_key,
)

await zep_retriever.ainvoke("Who wrote Parable of the Sower?")
```




    [Document(page_content="What is the 'Parable of the Sower'?", metadata={'score': 0.9250216484069824, 'uuid': '4cbfb1c0-6027-4678-af43-1e18acb224bb', 'created_at': '2023-11-01T00:32:40.224256Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'human', 'metadata': {'system': {'entities': [{'Label': 'WORK_OF_ART', 'Matches': [{'End': 34, 'Start': 13, 'Text': "Parable of the Sower'"}], 'Name': "Parable of the Sower'"}]}}, 'token_count': 13}),
     Document(page_content='Parable of the Sower is a science fiction novel by Octavia Butler, published in 1993. It follows the story of Lauren Olamina, a young woman living in a dystopian future where society has collapsed due to environmental disasters, poverty, and violence.', metadata={'score': 0.8897348046302795, 'uuid': '3dd9f5ed-9dc9-4427-9da6-aba1b8278a5c', 'created_at': '2023-11-01T00:32:40.192527Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'ai', 'metadata': {'system': {'entities': [{'Label': 'GPE', 'Matches': [{'End': 20, 'Start': 15, 'Text': 'Sower'}], 'Name': 'Sower'}, {'Label': 'PERSON', 'Matches': [{'End': 65, 'Start': 51, 'Text': 'Octavia Butler'}], 'Name': 'Octavia Butler'}, {'Label': 'DATE', 'Matches': [{'End': 84, 'Start': 80, 'Text': '1993'}], 'Name': '1993'}, {'Label': 'PERSON', 'Matches': [{'End': 124, 'Start': 110, 'Text': 'Lauren Olamina'}], 'Name': 'Lauren Olamina'}], 'intent': 'Providing information'}}, 'token_count': 56}),
     Document(page_content="Write a short synopsis of Butler's book, Parable of the Sower. What is it about?", metadata={'score': 0.8856019973754883, 'uuid': '81761dcb-38f3-4686-a4f5-6cb1007eaf29', 'created_at': '2023-11-01T00:32:40.187543Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'human', 'metadata': {'system': {'entities': [{'Label': 'ORG', 'Matches': [{'End': 32, 'Start': 26, 'Text': 'Butler'}], 'Name': 'Butler'}, {'Label': 'WORK_OF_ART', 'Matches': [{'End': 61, 'Start': 41, 'Text': 'Parable of the Sower'}], 'Name': 'Parable of the Sower'}], 'intent': "The subject is asking for a brief summary of Butler's book, Parable of the Sower, and what it is about."}}, 'token_count': 23}),
     Document(page_content="The 'Parable of the Sower' is a biblical parable that Butler uses as a metaphor in the book. In the parable, a sower scatters seeds, some of which fall on fertile ground and grow, while others fall on rocky ground or among thorns and fail to grow. The parable is used to illustrate the importance of receptivity and preparedness in the face of change.", metadata={'score': 0.8781436681747437, 'uuid': '1a8c5f99-2fec-425d-bc37-176ab91e7080', 'created_at': '2023-11-01T00:32:40.22836Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'ai', 'metadata': {'system': {'entities': [{'Label': 'WORK_OF_ART', 'Matches': [{'End': 26, 'Start': 5, 'Text': "Parable of the Sower'"}], 'Name': "Parable of the Sower'"}, {'Label': 'ORG', 'Matches': [{'End': 60, 'Start': 54, 'Text': 'Butler'}], 'Name': 'Butler'}]}}, 'token_count': 84}),
     Document(page_content="In addition to 'Parable of the Sower', Butler has written several other notable works, including 'Kindred', 'Dawn', and 'Parable of the Talents'.", metadata={'score': 0.8745182752609253, 'uuid': '45d8aa08-85ab-432f-8902-81712fe363b9', 'created_at': '2023-11-01T00:32:40.245081Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'ai', 'metadata': {'system': {'entities': [{'Label': 'WORK_OF_ART', 'Matches': [{'End': 37, 'Start': 16, 'Text': "Parable of the Sower'"}], 'Name': "Parable of the Sower'"}, {'Label': 'ORG', 'Matches': [{'End': 45, 'Start': 39, 'Text': 'Butler'}], 'Name': 'Butler'}, {'Label': 'GPE', 'Matches': [{'End': 105, 'Start': 98, 'Text': 'Kindred'}], 'Name': 'Kindred'}, {'Label': 'WORK_OF_ART', 'Matches': [{'End': 144, 'Start': 121, 'Text': "Parable of the Talents'"}], 'Name': "Parable of the Talents'"}]}}, 'token_count': 39})]



We can also use the Zep sync API to retrieve results:


```python
zep_retriever.invoke("Who wrote Parable of the Sower?")
```




    [Document(page_content="What is the 'Parable of the Sower'?", metadata={'score': 0.9250596761703491, 'uuid': '4cbfb1c0-6027-4678-af43-1e18acb224bb', 'created_at': '2023-11-01T00:32:40.224256Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'human', 'metadata': {'system': {'entities': [{'Label': 'WORK_OF_ART', 'Matches': [{'End': 34, 'Start': 13, 'Text': "Parable of the Sower'"}], 'Name': "Parable of the Sower'"}]}}, 'token_count': 13}),
     Document(page_content='Parable of the Sower is a science fiction novel by Octavia Butler, published in 1993. It follows the story of Lauren Olamina, a young woman living in a dystopian future where society has collapsed due to environmental disasters, poverty, and violence.', metadata={'score': 0.8897120952606201, 'uuid': '3dd9f5ed-9dc9-4427-9da6-aba1b8278a5c', 'created_at': '2023-11-01T00:32:40.192527Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'ai', 'metadata': {'system': {'entities': [{'Label': 'GPE', 'Matches': [{'End': 20, 'Start': 15, 'Text': 'Sower'}], 'Name': 'Sower'}, {'Label': 'PERSON', 'Matches': [{'End': 65, 'Start': 51, 'Text': 'Octavia Butler'}], 'Name': 'Octavia Butler'}, {'Label': 'DATE', 'Matches': [{'End': 84, 'Start': 80, 'Text': '1993'}], 'Name': '1993'}, {'Label': 'PERSON', 'Matches': [{'End': 124, 'Start': 110, 'Text': 'Lauren Olamina'}], 'Name': 'Lauren Olamina'}], 'intent': 'Providing information'}}, 'token_count': 56}),
     Document(page_content="Write a short synopsis of Butler's book, Parable of the Sower. What is it about?", metadata={'score': 0.885666012763977, 'uuid': '81761dcb-38f3-4686-a4f5-6cb1007eaf29', 'created_at': '2023-11-01T00:32:40.187543Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'human', 'metadata': {'system': {'entities': [{'Label': 'ORG', 'Matches': [{'End': 32, 'Start': 26, 'Text': 'Butler'}], 'Name': 'Butler'}, {'Label': 'WORK_OF_ART', 'Matches': [{'End': 61, 'Start': 41, 'Text': 'Parable of the Sower'}], 'Name': 'Parable of the Sower'}], 'intent': "The subject is asking for a brief summary of Butler's book, Parable of the Sower, and what it is about."}}, 'token_count': 23}),
     Document(page_content="The 'Parable of the Sower' is a biblical parable that Butler uses as a metaphor in the book. In the parable, a sower scatters seeds, some of which fall on fertile ground and grow, while others fall on rocky ground or among thorns and fail to grow. The parable is used to illustrate the importance of receptivity and preparedness in the face of change.", metadata={'score': 0.878172755241394, 'uuid': '1a8c5f99-2fec-425d-bc37-176ab91e7080', 'created_at': '2023-11-01T00:32:40.22836Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'ai', 'metadata': {'system': {'entities': [{'Label': 'WORK_OF_ART', 'Matches': [{'End': 26, 'Start': 5, 'Text': "Parable of the Sower'"}], 'Name': "Parable of the Sower'"}, {'Label': 'ORG', 'Matches': [{'End': 60, 'Start': 54, 'Text': 'Butler'}], 'Name': 'Butler'}]}}, 'token_count': 84}),
     Document(page_content="In addition to 'Parable of the Sower', Butler has written several other notable works, including 'Kindred', 'Dawn', and 'Parable of the Talents'.", metadata={'score': 0.8745154142379761, 'uuid': '45d8aa08-85ab-432f-8902-81712fe363b9', 'created_at': '2023-11-01T00:32:40.245081Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'ai', 'metadata': {'system': {'entities': [{'Label': 'WORK_OF_ART', 'Matches': [{'End': 37, 'Start': 16, 'Text': "Parable of the Sower'"}], 'Name': "Parable of the Sower'"}, {'Label': 'ORG', 'Matches': [{'End': 45, 'Start': 39, 'Text': 'Butler'}], 'Name': 'Butler'}, {'Label': 'GPE', 'Matches': [{'End': 105, 'Start': 98, 'Text': 'Kindred'}], 'Name': 'Kindred'}, {'Label': 'WORK_OF_ART', 'Matches': [{'End': 144, 'Start': 121, 'Text': "Parable of the Talents'"}], 'Name': "Parable of the Talents'"}]}}, 'token_count': 39})]



### Reranking using MMR (Maximal Marginal Relevance)

Zep has native, SIMD-accelerated support for reranking results using MMR. This is useful for removing redundancy in results.


```python
zep_retriever = ZepRetriever(
    session_id=session_id,  # Ensure that you provide the session_id when instantiating the Retriever
    url=ZEP_API_URL,
    top_k=5,
    api_key=zep_api_key,
    search_type=SearchType.mmr,
    mmr_lambda=0.5,
)

await zep_retriever.ainvoke("Who wrote Parable of the Sower?")
```




    [Document(page_content="What is the 'Parable of the Sower'?", metadata={'score': 0.9250596761703491, 'uuid': '4cbfb1c0-6027-4678-af43-1e18acb224bb', 'created_at': '2023-11-01T00:32:40.224256Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'human', 'metadata': {'system': {'entities': [{'Label': 'WORK_OF_ART', 'Matches': [{'End': 34, 'Start': 13, 'Text': "Parable of the Sower'"}], 'Name': "Parable of the Sower'"}]}}, 'token_count': 13}),
     Document(page_content='What other books has she written?', metadata={'score': 0.77488774061203, 'uuid': '1b3c5079-9cab-46f3-beae-fb56c572e0fd', 'created_at': '2023-11-01T00:32:40.240135Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'human', 'token_count': 9}),
     Document(page_content="In addition to 'Parable of the Sower', Butler has written several other notable works, including 'Kindred', 'Dawn', and 'Parable of the Talents'.", metadata={'score': 0.8745154142379761, 'uuid': '45d8aa08-85ab-432f-8902-81712fe363b9', 'created_at': '2023-11-01T00:32:40.245081Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'ai', 'metadata': {'system': {'entities': [{'Label': 'WORK_OF_ART', 'Matches': [{'End': 37, 'Start': 16, 'Text': "Parable of the Sower'"}], 'Name': "Parable of the Sower'"}, {'Label': 'ORG', 'Matches': [{'End': 45, 'Start': 39, 'Text': 'Butler'}], 'Name': 'Butler'}, {'Label': 'GPE', 'Matches': [{'End': 105, 'Start': 98, 'Text': 'Kindred'}], 'Name': 'Kindred'}, {'Label': 'WORK_OF_ART', 'Matches': [{'End': 144, 'Start': 121, 'Text': "Parable of the Talents'"}], 'Name': "Parable of the Talents'"}]}}, 'token_count': 39}),
     Document(page_content='Parable of the Sower is a science fiction novel by Octavia Butler, published in 1993. It follows the story of Lauren Olamina, a young woman living in a dystopian future where society has collapsed due to environmental disasters, poverty, and violence.', metadata={'score': 0.8897120952606201, 'uuid': '3dd9f5ed-9dc9-4427-9da6-aba1b8278a5c', 'created_at': '2023-11-01T00:32:40.192527Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'ai', 'metadata': {'system': {'entities': [{'Label': 'GPE', 'Matches': [{'End': 20, 'Start': 15, 'Text': 'Sower'}], 'Name': 'Sower'}, {'Label': 'PERSON', 'Matches': [{'End': 65, 'Start': 51, 'Text': 'Octavia Butler'}], 'Name': 'Octavia Butler'}, {'Label': 'DATE', 'Matches': [{'End': 84, 'Start': 80, 'Text': '1993'}], 'Name': '1993'}, {'Label': 'PERSON', 'Matches': [{'End': 124, 'Start': 110, 'Text': 'Lauren Olamina'}], 'Name': 'Lauren Olamina'}], 'intent': 'Providing information'}}, 'token_count': 56}),
     Document(page_content='Who is the protagonist?', metadata={'score': 0.7858647704124451, 'uuid': 'ee514b37-a0b0-4d24-b0c9-3e9f8ad9d52d', 'created_at': '2023-11-01T00:32:40.203891Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'human', 'metadata': {'system': {'intent': 'The subject is asking about the identity of the protagonist in a specific context, such as a story, movie, or game.'}}, 'token_count': 7})]



### Using metadata filters to refine search results

Zep supports filtering results by metadata. This is useful for filtering results by entity type, or other metadata.

More information here: https://docs.getzep.com/sdk/search_query/


```python
filter = {"where": {"jsonpath": '$[*] ? (@.Label == "WORK_OF_ART")'}}

await zep_retriever.ainvoke("Who wrote Parable of the Sower?", metadata=filter)
```




    [Document(page_content="What is the 'Parable of the Sower'?", metadata={'score': 0.9251098036766052, 'uuid': '4cbfb1c0-6027-4678-af43-1e18acb224bb', 'created_at': '2023-11-01T00:32:40.224256Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'human', 'metadata': {'system': {'entities': [{'Label': 'WORK_OF_ART', 'Matches': [{'End': 34, 'Start': 13, 'Text': "Parable of the Sower'"}], 'Name': "Parable of the Sower'"}]}}, 'token_count': 13}),
     Document(page_content='What other books has she written?', metadata={'score': 0.7747920155525208, 'uuid': '1b3c5079-9cab-46f3-beae-fb56c572e0fd', 'created_at': '2023-11-01T00:32:40.240135Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'human', 'token_count': 9}),
     Document(page_content="In addition to 'Parable of the Sower', Butler has written several other notable works, including 'Kindred', 'Dawn', and 'Parable of the Talents'.", metadata={'score': 0.8745266795158386, 'uuid': '45d8aa08-85ab-432f-8902-81712fe363b9', 'created_at': '2023-11-01T00:32:40.245081Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'ai', 'metadata': {'system': {'entities': [{'Label': 'WORK_OF_ART', 'Matches': [{'End': 37, 'Start': 16, 'Text': "Parable of the Sower'"}], 'Name': "Parable of the Sower'"}, {'Label': 'ORG', 'Matches': [{'End': 45, 'Start': 39, 'Text': 'Butler'}], 'Name': 'Butler'}, {'Label': 'GPE', 'Matches': [{'End': 105, 'Start': 98, 'Text': 'Kindred'}], 'Name': 'Kindred'}, {'Label': 'WORK_OF_ART', 'Matches': [{'End': 144, 'Start': 121, 'Text': "Parable of the Talents'"}], 'Name': "Parable of the Talents'"}]}}, 'token_count': 39}),
     Document(page_content='Parable of the Sower is a science fiction novel by Octavia Butler, published in 1993. It follows the story of Lauren Olamina, a young woman living in a dystopian future where society has collapsed due to environmental disasters, poverty, and violence.', metadata={'score': 0.8897372484207153, 'uuid': '3dd9f5ed-9dc9-4427-9da6-aba1b8278a5c', 'created_at': '2023-11-01T00:32:40.192527Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'ai', 'metadata': {'system': {'entities': [{'Label': 'GPE', 'Matches': [{'End': 20, 'Start': 15, 'Text': 'Sower'}], 'Name': 'Sower'}, {'Label': 'PERSON', 'Matches': [{'End': 65, 'Start': 51, 'Text': 'Octavia Butler'}], 'Name': 'Octavia Butler'}, {'Label': 'DATE', 'Matches': [{'End': 84, 'Start': 80, 'Text': '1993'}], 'Name': '1993'}, {'Label': 'PERSON', 'Matches': [{'End': 124, 'Start': 110, 'Text': 'Lauren Olamina'}], 'Name': 'Lauren Olamina'}], 'intent': 'Providing information'}}, 'token_count': 56}),
     Document(page_content='Who is the protagonist?', metadata={'score': 0.7858127355575562, 'uuid': 'ee514b37-a0b0-4d24-b0c9-3e9f8ad9d52d', 'created_at': '2023-11-01T00:32:40.203891Z', 'updated_at': '0001-01-01T00:00:00Z', 'role': 'human', 'metadata': {'system': {'intent': 'The subject is asking about the identity of the protagonist in a specific context, such as a story, movie, or game.'}}, 'token_count': 7})]



### Searching over Summaries with MMR Reranking

Zep automatically generates summaries of chat messages. These summaries can be searched over using the Zep Retriever. Since a summary is a distillation of a conversation, they're more likely to match your search query and offer rich, succinct context to the LLM.

Successive summaries may include similar content, with Zep's similarity search returning the highest matching results but with little diversity.
MMR re-ranks the results to ensure that the summaries you populate into your prompt are both relevant and each offers additional information to the LLM.


```python
zep_retriever = ZepRetriever(
    session_id=session_id,  # Ensure that you provide the session_id when instantiating the Retriever
    url=ZEP_API_URL,
    top_k=3,
    api_key=zep_api_key,
    search_scope=SearchScope.summary,
    search_type=SearchType.mmr,
    mmr_lambda=0.5,
)

await zep_retriever.ainvoke("Who wrote Parable of the Sower?")
```




    [Document(page_content='The human asks about Octavia Butler and the AI informs them that she was an American science fiction author. The human\nasks which of her books were made into movies and the AI mentions the FX series Kindred. The human then asks about her\ncontemporaries and the AI lists Ursula K. Le Guin, Samuel R. Delany, and Joanna Russ. The human also asks about the awards\nshe won and the AI mentions the Hugo Award, the Nebula Award, and the MacArthur Fellowship. The human asks about other women sci-fi writers to read and the AI suggests Ursula K. Le Guin and Joanna Russ. The human then asks for a synopsis of Butler\'s book "Parable of the Sower" and the AI describes it.', metadata={'score': 0.7882999777793884, 'uuid': '3c95a29a-52dc-4112-b8a7-e6b1dc414d45', 'created_at': '2023-11-01T00:32:47.76449Z', 'token_count': 155}),
     Document(page_content='The human asks about Octavia Butler. The AI informs the human that Octavia Estelle Butler was an American science \nfiction author. The human then asks which books of hers were made into movies and the AI mentions the FX series Kindred, \nbased on her novel of the same name.', metadata={'score': 0.7407922744750977, 'uuid': '0e027f4d-d71f-42ae-977f-696b8948b8bf', 'created_at': '2023-11-01T00:32:41.637098Z', 'token_count': 59}),
     Document(page_content='The human asks about Octavia Butler and the AI informs them that she was an American science fiction author. The human\nasks which of her books were made into movies and the AI mentions the FX series Kindred. The human then asks about her\ncontemporaries and the AI lists Ursula K. Le Guin, Samuel R. Delany, and Joanna Russ. The human also asks about the awards\nshe won and the AI mentions the Hugo Award, the Nebula Award, and the MacArthur Fellowship.', metadata={'score': 0.7436535358428955, 'uuid': 'b3500d1b-1a78-4aef-9e24-6b196cfa83cb', 'created_at': '2023-11-01T00:32:44.24744Z', 'token_count': 104})]




```python

```




################################################## zep_memory_cloud.md ##################################################


# Zep Cloud Memory
> Recall, understand, and extract data from chat histories. Power personalized AI experiences.

>[Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.
> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,
> while also reducing hallucinations, latency, and cost.

> See [Zep Cloud Installation Guide](https://help.getzep.com/sdks) and more [Zep Cloud Langchain Examples](https://github.com/getzep/zep-python/tree/main/examples)

## Example

This notebook demonstrates how to use [Zep](https://www.getzep.com/) as memory for your chatbot.

We'll demonstrate:

1. Adding conversation history to Zep.
2. Running an agent and having message automatically added to the store.
3. Viewing the enriched messages.
4. Vector search over the conversation history.


```python
from uuid import uuid4

from langchain.agents import AgentType, initialize_agent
from langchain_community.memory.zep_cloud_memory import ZepCloudMemory
from langchain_community.retrievers import ZepCloudRetriever
from langchain_community.utilities import WikipediaAPIWrapper
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.tools import Tool
from langchain_openai import OpenAI

session_id = str(uuid4())  # This is a unique identifier for the session
```


    ---------------------------------------------------------------------------

    AttributeError                            Traceback (most recent call last)

    Cell In[3], line 8
          6 from langchain_community.utilities import WikipediaAPIWrapper
          7 from langchain_core.messages import AIMessage, HumanMessage
    ----> 8 from langchain_openai import OpenAI
         10 session_id = str(uuid4())  # This is a unique identifier for the session
    

    File ~/job/integrations/langchain/libs/partners/openai/langchain_openai/__init__.py:1
    ----> 1 from langchain_openai.chat_models import (
          2     AzureChatOpenAI,
          3     ChatOpenAI,
          4 )
          5 from langchain_openai.embeddings import (
          6     AzureOpenAIEmbeddings,
          7     OpenAIEmbeddings,
          8 )
          9 from langchain_openai.llms import AzureOpenAI, OpenAI
    

    File ~/job/integrations/langchain/libs/partners/openai/langchain_openai/chat_models/__init__.py:1
    ----> 1 from langchain_openai.chat_models.azure import AzureChatOpenAI
          2 from langchain_openai.chat_models.base import ChatOpenAI
          4 __all__ = [
          5     "ChatOpenAI",
          6     "AzureChatOpenAI",
          7 ]
    

    File ~/job/integrations/langchain/libs/partners/openai/langchain_openai/chat_models/azure.py:8
          5 import os
          6 from typing import Any, Callable, Dict, List, Optional, Union
    ----> 8 import openai
          9 from langchain_core.outputs import ChatResult
         10 from langchain_core.pydantic_v1 import Field, SecretStr, root_validator
    

    File ~/job/zep-proprietary/venv/lib/python3.11/site-packages/openai/__init__.py:8
          5 import os as _os
          6 from typing_extensions import override
    ----> 8 from . import types
          9 from ._types import NOT_GIVEN, NoneType, NotGiven, Transport, ProxiesTypes
         10 from ._utils import file_from_path
    

    File ~/job/zep-proprietary/venv/lib/python3.11/site-packages/openai/types/__init__.py:5
          1 # File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.
          3 from __future__ import annotations
    ----> 5 from .batch import Batch as Batch
          6 from .image import Image as Image
          7 from .model import Model as Model
    

    File ~/job/zep-proprietary/venv/lib/python3.11/site-packages/openai/types/batch.py:7
          4 from typing import List, Optional
          5 from typing_extensions import Literal
    ----> 7 from .._models import BaseModel
          8 from .batch_error import BatchError
          9 from .batch_request_counts import BatchRequestCounts
    

    File ~/job/zep-proprietary/venv/lib/python3.11/site-packages/openai/_models.py:667
        662     json_data: Body
        663     extra_json: AnyMapping
        666 @final
    --> 667 class FinalRequestOptions(pydantic.BaseModel):
        668     method: str
        669     url: str
    

    File ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:202, in __new__(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)
        199         super(cls, cls).__pydantic_init_subclass__(**kwargs)  # type: ignore[misc]
        200         return cls
        201     else:
    --> 202         # this is the BaseModel class itself being created, no logic required
        203         return super().__new__(mcs, cls_name, bases, namespace, **kwargs)
        205 if not typing.TYPE_CHECKING:  # pragma: no branch
        206     # We put `__getattr__` in a non-TYPE_CHECKING block because otherwise, mypy allows arbitrary attribute access
    

    File ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:539, in complete_model_class(cls, cls_name, config_wrapper, raise_errors, types_namespace, create_model_module)
        532 # debug(schema)
        533 cls.__pydantic_core_schema__ = schema
        535 cls.__pydantic_validator__ = create_schema_validator(
        536     schema,
        537     cls,
        538     create_model_module or cls.__module__,
    --> 539     cls.__qualname__,
        540     'create_model' if create_model_module else 'BaseModel',
        541     core_config,
        542     config_wrapper.plugin_settings,
        543 )
        544 cls.__pydantic_serializer__ = SchemaSerializer(schema, core_config)
        545 cls.__pydantic_complete__ = True
    

    File ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/main.py:626, in __get_pydantic_core_schema__(cls, source, handler)
        611 @classmethod
        612 def __pydantic_init_subclass__(cls, **kwargs: Any) -> None:
        613     """This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`
        614     only after the class is actually fully initialized. In particular, attributes like `model_fields` will
        615     be present when this is called.
        616 
        617     This is necessary because `__init_subclass__` will always be called by `type.__new__`,
        618     and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that
        619     `type.__new__` was called in such a manner that the class would already be sufficiently initialized.
        620 
        621     This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,
        622     any kwargs passed to the class definition that aren't used internally by pydantic.
        623 
        624     Args:
        625         **kwargs: Any keyword arguments passed to the class definition that aren't used internally
    --> 626             by pydantic.
        627     """
        628     pass
    

    File ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py:82, in CallbackGetCoreSchemaHandler.__call__(self, source_type)
         81 def __call__(self, __source_type: Any) -> core_schema.CoreSchema:
    ---> 82     schema = self._handler(__source_type)
         83     ref = schema.get('ref')
         84     if self._ref_mode == 'to-def':
    

    File ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:502, in generate_schema(self, obj, from_dunder_get_core_schema)
        498 schema = _add_custom_serialization_from_json_encoders(self._config_wrapper.json_encoders, obj, schema)
        500 schema = self._post_process_generated_schema(schema)
    --> 502 return schema
    

    File ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:753, in _generate_schema_inner(self, obj)
        749 def match_type(self, obj: Any) -> core_schema.CoreSchema:  # noqa: C901
        750     """Main mapping of types to schemas.
        751 
        752     The general structure is a series of if statements starting with the simple cases
    --> 753     (non-generic primitive types) and then handling generics and other more complex cases.
        754 
        755     Each case either generates a schema directly, calls into a public user-overridable method
        756     (like `GenerateSchema.tuple_variable_schema`) or calls into a private method that handles some
        757     boilerplate before calling into the user-facing method (e.g. `GenerateSchema._tuple_schema`).
        758 
        759     The idea is that we'll evolve this into adding more and more user facing methods over time
        760     as they get requested and we figure out what the right API for them is.
        761     """
        762     if obj is str:
        763         return self.str_schema()
    

    File ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in _model_schema(self, cls)
        574         inner_schema = new_inner_schema
        575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')
        577     model_schema = core_schema.model_schema(
        578         cls,
        579         inner_schema,
    --> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),
        581         root_model=False,
        582         post_init=getattr(cls, '__pydantic_post_init__', None),
        583         config=core_config,
        584         ref=model_ref,
        585         metadata=metadata,
        586     )
        588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())
        589 schema = apply_model_validators(schema, model_validators, 'outer')
    

    File ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in <dictcomp>(.0)
        574         inner_schema = new_inner_schema
        575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')
        577     model_schema = core_schema.model_schema(
        578         cls,
        579         inner_schema,
    --> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),
        581         root_model=False,
        582         post_init=getattr(cls, '__pydantic_post_init__', None),
        583         config=core_config,
        584         ref=model_ref,
        585         metadata=metadata,
        586     )
        588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())
        589 schema = apply_model_validators(schema, model_validators, 'outer')
    

    File ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:916, in _generate_md_field_schema(self, name, field_info, decorators)
        906     common_field = self._common_field_schema(name, field_info, decorators)
        907     return core_schema.model_field(
        908         common_field['schema'],
        909         serialization_exclude=common_field['serialization_exclude'],
       (...)
        913         metadata=common_field['metadata'],
        914     )
    --> 916 def _generate_dc_field_schema(
        917     self,
        918     name: str,
        919     field_info: FieldInfo,
        920     decorators: DecoratorInfos,
        921 ) -> core_schema.DataclassField:
        922     """Prepare a DataclassField to represent the parameter/field, of a dataclass."""
        923     common_field = self._common_field_schema(name, field_info, decorators)
    

    File ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1114, in _common_field_schema(self, name, field_info, decorators)
       1108 json_schema_extra = field_info.json_schema_extra
       1110 metadata = build_metadata_dict(
       1111     js_annotation_functions=[get_json_schema_update_func(json_schema_updates, json_schema_extra)]
       1112 )
    -> 1114 alias_generator = self._config_wrapper.alias_generator
       1115 if alias_generator is not None:
       1116     self._apply_alias_generator_to_field_info(alias_generator, field_info, name)
    

    AttributeError: 'FieldInfo' object has no attribute 'deprecated'



```python
# Provide your OpenAI key
import getpass

openai_key = getpass.getpass()
```


```python
# Provide your Zep API key. See https://help.getzep.com/projects#api-keys

zep_api_key = getpass.getpass()
```

### Initialize the Zep Chat Message History Class and initialize the Agent



```python
search = WikipediaAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description=(
            "useful for when you need to search online for answers. You should ask"
            " targeted questions"
        ),
    ),
]

# Set up Zep Chat History
memory = ZepCloudMemory(
    session_id=session_id,
    api_key=zep_api_key,
    return_messages=True,
    memory_key="chat_history",
)

# Initialize the agent
llm = OpenAI(temperature=0, openai_api_key=openai_key)
agent_chain = initialize_agent(
    tools,
    llm,
    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
    verbose=True,
    memory=memory,
)
```

### Add some history data



```python
# Preload some messages into the memory. The default message window is 12 messages. We want to push beyond this to demonstrate auto-summarization.
test_history = [
    {"role": "human", "content": "Who was Octavia Butler?"},
    {
        "role": "ai",
        "content": (
            "Octavia Estelle Butler (June 22, 1947 – February 24, 2006) was an American"
            " science fiction author."
        ),
    },
    {"role": "human", "content": "Which books of hers were made into movies?"},
    {
        "role": "ai",
        "content": (
            "The most well-known adaptation of Octavia Butler's work is the FX series"
            " Kindred, based on her novel of the same name."
        ),
    },
    {"role": "human", "content": "Who were her contemporaries?"},
    {
        "role": "ai",
        "content": (
            "Octavia Butler's contemporaries included Ursula K. Le Guin, Samuel R."
            " Delany, and Joanna Russ."
        ),
    },
    {"role": "human", "content": "What awards did she win?"},
    {
        "role": "ai",
        "content": (
            "Octavia Butler won the Hugo Award, the Nebula Award, and the MacArthur"
            " Fellowship."
        ),
    },
    {
        "role": "human",
        "content": "Which other women sci-fi writers might I want to read?",
    },
    {
        "role": "ai",
        "content": "You might want to read Ursula K. Le Guin or Joanna Russ.",
    },
    {
        "role": "human",
        "content": (
            "Write a short synopsis of Butler's book, Parable of the Sower. What is it"
            " about?"
        ),
    },
    {
        "role": "ai",
        "content": (
            "Parable of the Sower is a science fiction novel by Octavia Butler,"
            " published in 1993. It follows the story of Lauren Olamina, a young woman"
            " living in a dystopian future where society has collapsed due to"
            " environmental disasters, poverty, and violence."
        ),
        "metadata": {"foo": "bar"},
    },
]

for msg in test_history:
    memory.chat_memory.add_message(
        (
            HumanMessage(content=msg["content"])
            if msg["role"] == "human"
            else AIMessage(content=msg["content"])
        ),
        metadata=msg.get("metadata", {}),
    )
```

### Run the agent

Doing so will automatically add the input and response to the Zep memory.



```python
agent_chain.invoke(
    input="What is the book's relevance to the challenges facing contemporary society?",
)
```

    
    
    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3m
    AI: Parable of the Sower is highly relevant to contemporary society as it explores themes of environmental degradation, social and economic inequality, and the struggle for survival in a chaotic world. It also delves into issues of race, gender, and religion, making it a thought-provoking and timely read.[0m
    
    [1m> Finished chain.[0m
    




    {'input': "What is the book's relevance to the challenges facing contemporary society?",
     'chat_history': [HumanMessage(content="Octavia Butler's contemporaries included Ursula K. Le Guin, Samuel R. Delany, and Joanna Russ.\nOctavia Butler won the Hugo Award, the Nebula Award, and the MacArthur Fellowship.\nUrsula K. Le Guin is known for novels like The Left Hand of Darkness and The Dispossessed.\nJoanna Russ is the author of the influential feminist science fiction novel The Female Man.\nMargaret Atwood is known for works like The Handmaid's Tale and the MaddAddam trilogy.\nConnie Willis is an award-winning author of science fiction and fantasy, known for novels like Doomsday Book.\nOctavia Butler is a pioneering black female science fiction author, known for Kindred and the Parable series.\nOctavia Estelle Butler was an acclaimed American science fiction author. While none of her books were directly adapted into movies, her novel Kindred was adapted into a TV series on FX. Butler was part of a generation of prominent science fiction writers in the 20th century, including contemporaries such as Ursula K. Le Guin, Samuel R. Delany, Chip Delany, and Nalo Hopkinson.\nhuman: What awards did she win?\nai: Octavia Butler won the Hugo Award, the Nebula Award, and the MacArthur Fellowship.\nhuman: Which other women sci-fi writers might I want to read?\nai: You might want to read Ursula K. Le Guin or Joanna Russ.\nhuman: Write a short synopsis of Butler's book, Parable of the Sower. What is it about?\nai: Parable of the Sower is a science fiction novel by Octavia Butler, published in 1993. It follows the story of Lauren Olamina, a young woman living in a dystopian future where society has collapsed due to environmental disasters, poverty, and violence.")],
     'output': 'Parable of the Sower is highly relevant to contemporary society as it explores themes of environmental degradation, social and economic inequality, and the struggle for survival in a chaotic world. It also delves into issues of race, gender, and religion, making it a thought-provoking and timely read.'}



### Inspect the Zep memory

Note the summary, and that the history has been enriched with token counts, UUIDs, and timestamps.

Summaries are biased towards the most recent messages.



```python
def print_messages(messages):
    for m in messages:
        print(m.type, ":\n", m.dict())


print(memory.chat_memory.zep_summary)
print("\n")
print("Conversation Facts: ")
facts = memory.chat_memory.zep_facts
for fact in facts:
    print(fact + "\n")
print_messages(memory.chat_memory.messages)
```

    Octavia Estelle Butler was an acclaimed American science fiction author. While none of her books were directly adapted into movies, her novel Kindred was adapted into a TV series on FX. Butler was part of a generation of prominent science fiction writers in the 20th century, including contemporaries such as Ursula K. Le Guin, Samuel R. Delany, Chip Delany, and Nalo Hopkinson.
    
    
    Conversation Facts: 
    Octavia Butler's contemporaries included Ursula K. Le Guin, Samuel R. Delany, and Joanna Russ.
    
    Octavia Butler won the Hugo Award, the Nebula Award, and the MacArthur Fellowship.
    
    Ursula K. Le Guin is known for novels like The Left Hand of Darkness and The Dispossessed.
    
    Joanna Russ is the author of the influential feminist science fiction novel The Female Man.
    
    Margaret Atwood is known for works like The Handmaid's Tale and the MaddAddam trilogy.
    
    Connie Willis is an award-winning author of science fiction and fantasy, known for novels like Doomsday Book.
    
    Octavia Butler is a pioneering black female science fiction author, known for Kindred and the Parable series.
    
    Parable of the Sower is a science fiction novel by Octavia Butler, published in 1993.
    
    The novel follows the story of Lauren Olamina, a young woman living in a dystopian future where society has collapsed due to environmental disasters, poverty, and violence.
    
    Parable of the Sower explores themes of environmental degradation, social and economic inequality, and the struggle for survival in a chaotic world.
    
    The novel also delves into issues of race, gender, and religion, making it a thought-provoking and timely read.
    
    human :
     {'content': "Octavia Butler's contemporaries included Ursula K. Le Guin, Samuel R. Delany, and Joanna Russ.\nOctavia Butler won the Hugo Award, the Nebula Award, and the MacArthur Fellowship.\nUrsula K. Le Guin is known for novels like The Left Hand of Darkness and The Dispossessed.\nJoanna Russ is the author of the influential feminist science fiction novel The Female Man.\nMargaret Atwood is known for works like The Handmaid's Tale and the MaddAddam trilogy.\nConnie Willis is an award-winning author of science fiction and fantasy, known for novels like Doomsday Book.\nOctavia Butler is a pioneering black female science fiction author, known for Kindred and the Parable series.\nParable of the Sower is a science fiction novel by Octavia Butler, published in 1993.\nThe novel follows the story of Lauren Olamina, a young woman living in a dystopian future where society has collapsed due to environmental disasters, poverty, and violence.\nParable of the Sower explores themes of environmental degradation, social and economic inequality, and the struggle for survival in a chaotic world.\nThe novel also delves into issues of race, gender, and religion, making it a thought-provoking and timely read.\nOctavia Estelle Butler was an acclaimed American science fiction author. While none of her books were directly adapted into movies, her novel Kindred was adapted into a TV series on FX. Butler was part of a generation of prominent science fiction writers in the 20th century, including contemporaries such as Ursula K. Le Guin, Samuel R. Delany, Chip Delany, and Nalo Hopkinson.\nhuman: Which other women sci-fi writers might I want to read?\nai: You might want to read Ursula K. Le Guin or Joanna Russ.\nhuman: Write a short synopsis of Butler's book, Parable of the Sower. What is it about?\nai: Parable of the Sower is a science fiction novel by Octavia Butler, published in 1993. It follows the story of Lauren Olamina, a young woman living in a dystopian future where society has collapsed due to environmental disasters, poverty, and violence.\nhuman: What is the book's relevance to the challenges facing contemporary society?\nai: Parable of the Sower is highly relevant to contemporary society as it explores themes of environmental degradation, social and economic inequality, and the struggle for survival in a chaotic world. It also delves into issues of race, gender, and religion, making it a thought-provoking and timely read.", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': None, 'example': False}
    

### Vector search over the Zep memory

Zep provides native vector search over historical conversation memory via the `ZepRetriever`.

You can use the `ZepRetriever` with chains that support passing in a Langchain `Retriever` object.



```python
retriever = ZepCloudRetriever(
    session_id=session_id,
    api_key=zep_api_key,
)

search_results = memory.chat_memory.search("who are some famous women sci-fi authors?")
for r in search_results:
    if r.score > 0.8:  # Only print results with similarity of 0.8 or higher
        print(r.message, r.score)
```

    content='Which other women sci-fi writers might I want to read?' created_at='2024-05-10T14:34:16.714292Z' metadata=None role='human' role_type=None token_count=12 updated_at='0001-01-01T00:00:00Z' uuid_='64ca1fae-8db1-4b4f-8a45-9b0e57e88af5' 0.8960460126399994
    


```python

```




################################################## zero-shot-prompting.md ##################################################


# Zero-Shot Prompting Tutorial

## Overview

This tutorial provides a comprehensive introduction to zero-shot prompting, a powerful technique in prompt engineering that allows language models to perform tasks without specific examples or prior training. We'll explore how to design effective zero-shot prompts and implement strategies using OpenAI's GPT models and the LangChain library.

## Motivation

Zero-shot prompting is crucial in modern AI applications as it enables language models to generalize to new tasks without the need for task-specific training data or fine-tuning. This capability significantly enhances the flexibility and applicability of AI systems, allowing them to adapt to a wide range of scenarios and user needs with minimal setup.

## Key Components

1. **Understanding Zero-Shot Learning**: An introduction to the concept and its importance in AI.
2. **Prompt Design Principles**: Techniques for crafting effective zero-shot prompts.
3. **Task Framing**: Methods to frame various tasks for zero-shot performance.
4. **OpenAI Integration**: Using OpenAI's GPT models for zero-shot tasks.
5. **LangChain Implementation**: Leveraging LangChain for structured zero-shot prompting.

## Method Details

The tutorial will cover several methods for implementing zero-shot prompting:

1. **Direct Task Specification**: Crafting prompts that clearly define the task without examples.
2. **Role-Based Prompting**: Assigning specific roles to the AI to guide its responses.
3. **Format Specification**: Providing output format guidelines in the prompt.
4. **Multi-step Reasoning**: Breaking down complex tasks into simpler zero-shot steps.
5. **Comparative Analysis**: Evaluating different zero-shot prompt structures for the same task.

Throughout the tutorial, we'll use Python code with OpenAI and LangChain to demonstrate these techniques practically.

## Conclusion

By the end of this tutorial, learners will have gained:

1. A solid understanding of zero-shot prompting and its applications.
2. Practical skills in designing effective zero-shot prompts for various tasks.
3. Experience in implementing zero-shot techniques using OpenAI and LangChain.
4. Insights into the strengths and limitations of zero-shot approaches.
5. A foundation for further exploration and innovation in prompt engineering.

This knowledge will empower learners to leverage AI models more effectively across a wide range of applications, enhancing their ability to solve novel problems and create more flexible AI systems.

## Setup

Let's start by importing the necessary libraries and setting up our environment.


```python
import os
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set up OpenAI API key
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')

# Initialize the language model
llm = ChatOpenAI(model="gpt-4o-mini")


def create_chain(prompt_template):
    """
    Create a LangChain chain with the given prompt template.
    
    Args:
        prompt_template (str): The prompt template string.
    
    Returns:
        LLMChain: A LangChain chain object.
    """
    prompt = PromptTemplate.from_template(prompt_template)
    return prompt | llm
```

## 1. Direct Task Specification

In this section, we'll explore how to craft prompts that clearly define the task without providing examples. This is the essence of zero-shot prompting.


```python
direct_task_prompt = """Classify the sentiment of the following text as positive, negative, or neutral.
Do not explain your reasoning, just provide the classification.

Text: {text}

Sentiment:"""

direct_task_chain = create_chain(direct_task_prompt)

# Test the direct task specification
texts = [
    "I absolutely loved the movie! The acting was superb.",
    "The weather today is quite typical for this time of year.",
    "I'm disappointed with the service I received at the restaurant."
]

for text in texts:
    result = direct_task_chain.invoke({"text": text}).content
    print(f"Text: {text}")
    print(f"Sentiment: {result}")
```

    Text: I absolutely loved the movie! The acting was superb.
    Sentiment: Positive
    Text: The weather today is quite typical for this time of year.
    Sentiment: Neutral
    Text: I'm disappointed with the service I received at the restaurant.
    Sentiment: Negative
    

## 2. Format Specification

Providing output format guidelines in the prompt can help structure the AI's response in a zero-shot scenario.


```python
format_spec_prompt = """Generate a short news article about {topic}. 
Structure your response in the following format:

Headline: [A catchy headline for the article]

Lead: [A brief introductory paragraph summarizing the key points]

Body: [2-3 short paragraphs providing more details]

Conclusion: [A concluding sentence or call to action]"""

format_spec_chain = create_chain(format_spec_prompt)

# Test the format specification prompting
topic = "The discovery of a new earth-like exoplanet"
result = format_spec_chain.invoke({"topic": topic}).content
print(result)
```

    **Headline:** Astronomers Unveil New Earth-Like Exoplanet in Habitable Zone
    
    **Lead:** In a groundbreaking discovery, a team of astronomers has identified a new Earth-like exoplanet located within the habitable zone of its star, raising hopes for the possibility of extraterrestrial life. Dubbed "Kepler-452d," the planet orbits a sun-like star approximately 1,400 light-years away, offering a tantalizing glimpse into worlds beyond our solar system.
    
    **Body:** The discovery was made using advanced observational techniques from the Kepler Space Telescope, which has been instrumental in finding thousands of exoplanets. Kepler-452d is approximately 1.6 times the size of Earth and orbits its star at a distance that allows for liquid water to exist on its surface—a crucial condition for life as we know it. Scientists believe that the planet's atmosphere could potentially support life, making it a prime candidate for future exploration.
    
    The research team, led by Dr. Emily Chen, emphasizes the significance of this find. "This is one of the most promising Earth-like planets we've discovered to date," Chen stated. "The conditions appear to be suitable for life, and with the right tools, we may be able to analyze its atmosphere in the coming years." As technology advances, the prospect of studying Kepler-452d and others like it becomes increasingly viable.
    
    **Conclusion:** As we stand on the brink of a new era in space exploration, this exciting discovery fuels the quest to answer one of humanity's most profound questions: Are we alone in the universe?
    

## 3. Multi-step Reasoning

For complex tasks, we can break them down into simpler zero-shot steps. This approach can improve the overall performance of the model.


```python
multi_step_prompt = """Analyze the following text for its main argument, supporting evidence, and potential counterarguments. 
Provide your analysis in the following steps:

1. Main Argument: Identify and state the primary claim or thesis.
2. Supporting Evidence: List the key points or evidence used to support the main argument.
3. Potential Counterarguments: Suggest possible objections or alternative viewpoints to the main argument.

Text: {text}

Analysis:"""

multi_step_chain = create_chain(multi_step_prompt)

# Test the multi-step reasoning approach
text = """While electric vehicles are often touted as a solution to climate change, their environmental impact is not as straightforward as it seems. 
The production of batteries for electric cars requires significant mining operations, which can lead to habitat destruction and water pollution. 
Moreover, if the electricity used to charge these vehicles comes from fossil fuel sources, the overall carbon footprint may not be significantly reduced. 
However, as renewable energy sources become more prevalent and battery technology improves, electric vehicles could indeed play a crucial role in combating climate change."""

result = multi_step_chain.invoke({"text": text}).content
print(result)
```

    1. **Main Argument**: The primary claim of the text is that while electric vehicles (EVs) are often promoted as a solution to climate change, their environmental impact is complex and not entirely positive due to the mining for battery production and reliance on fossil fuels for electricity.
    
    2. **Supporting Evidence**: 
       - The production of batteries for electric vehicles involves significant mining operations, which can lead to habitat destruction.
       - Mining for battery materials can also result in water pollution.
       - The environmental benefits of electric vehicles may be undermined if the electricity used for charging is sourced from fossil fuels.
       - Acknowledgment that improvements in renewable energy sources and battery technology could enhance the role of electric vehicles in addressing climate change in the future.
    
    3. **Potential Counterarguments**: 
       - Proponents of electric vehicles might argue that the overall lifecycle emissions of EVs are still lower than those of traditional vehicles, even when accounting for battery production and electricity sourcing.
       - The advancements in battery recycling technologies could mitigate the negative environmental impacts associated with battery production.
       - Renewable energy sources are rapidly growing, and the transition to green electricity could significantly improve the environmental benefits of electric vehicles.
       - The argument could be made that the shift towards electric vehicles is a necessary step toward reducing reliance on fossil fuels, despite current limitations in technology and energy sourcing.
    

## 4. Comparative Analysis

Let's compare different zero-shot prompt structures for the same task to evaluate their effectiveness.


```python
def compare_prompts(task, prompt_templates):
    """
    Compare different prompt templates for the same task.
    
    Args:
        task (str): The task description or input.
        prompt_templates (dict): A dictionary of prompt templates with their names as keys.
    """
    print(f"Task: {task}\n")
    for name, template in prompt_templates.items():
        chain = create_chain(template)
        result = chain.invoke({"task": task}).content
        print(f"{name} Prompt Result:")
        print(result)
        print("\n" + "-"*50 + "\n")

task = "Explain conciesly the concept of blockchain technology"

prompt_templates = {
    "Basic": "Explain {task}.",
    "Structured": """Explain {task} by addressing the following points:
1. Definition
2. Key features
3. Real-world applications
4. Potential impact on industries"""
}

compare_prompts(task, prompt_templates)
```

    Task: Explain conciesly the concept of blockchain technology
    
    Basic Prompt Result:
    Blockchain technology is a decentralized digital ledger system that securely records transactions across multiple computers. It ensures that once data is entered, it cannot be altered without consensus from the network participants. Each block contains a list of transactions and a cryptographic hash of the previous block, forming a chain. This structure enhances security, transparency, and trust, as it eliminates the need for a central authority and makes tampering with data extremely difficult. Blockchain is widely used in cryptocurrencies, supply chain management, and various applications requiring secure and transparent record-keeping.
    
    --------------------------------------------------
    
    Structured Prompt Result:
    ### 1. Definition
    Blockchain technology is a decentralized digital ledger system that records transactions across multiple computers in a way that ensures the security, transparency, and immutability of the data. Each transaction is grouped into a block and linked to the previous block, forming a chronological chain.
    
    ### 2. Key Features
    - **Decentralization**: No single entity controls the network; all participants have access to the same data.
    - **Transparency**: Transactions are visible to all users, promoting accountability.
    - **Immutability**: Once recorded, transactions cannot be altered or deleted, ensuring data integrity.
    - **Security**: Cryptographic techniques protect data, making it resistant to fraud and hacking.
    - **Consensus Mechanisms**: Various protocols (e.g., Proof of Work, Proof of Stake) are used to validate transactions and maintain network integrity.
    
    ### 3. Real-world Applications
    - **Cryptocurrencies**: Digital currencies like Bitcoin and Ethereum use blockchain for secure transactions.
    - **Supply Chain Management**: Enhances traceability and transparency in tracking goods from origin to destination.
    - **Smart Contracts**: Self-executing contracts with the terms directly written into code, automating processes without intermediaries.
    - **Voting Systems**: Secure and transparent voting solutions to enhance electoral integrity.
    - **Healthcare**: Secure sharing of patient data across platforms while maintaining privacy.
    
    ### 4. Potential Impact on Industries
    - **Finance**: Reduces costs and increases transaction speeds by eliminating intermediaries, enabling faster cross-border payments.
    - **Real Estate**: Streamlines property transactions through transparent records and fractional ownership possibilities.
    - **Insurance**: Automates claims processing and fraud detection through smart contracts.
    - **Manufacturing**: Enhances quality control and accountability in the production process through improved supply chain visibility.
    - **Government**: Increases transparency in public records and reduces corruption through tamper-proof systems. 
    
    Overall, blockchain technology has the potential to revolutionize various sectors by improving efficiency, transparency, and security.
    
    --------------------------------------------------
    
    




################################################## Zero-shot_classification_with_embeddings.md ##################################################


## Zero-shot classification with embeddings

In this notebook we will classify the sentiment of reviews using embeddings and zero labeled data! The dataset is created in the [Get_embeddings_from_dataset Notebook](Get_embeddings_from_dataset.ipynb).

We'll define positive sentiment to be 4- and 5-star reviews, and negative sentiment to be 1- and 2-star reviews. 3-star reviews are considered neutral and we won't use them for this example.

We will perform zero-shot classification by embedding descriptions of each class and then comparing new samples to those class embeddings."


```csharp
#r "nuget:Microsoft.DotNet.Interactive.AIUtilities, 1.0.0-beta.24129.1"
```


<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.DotNet.Interactive.AIUtilities, 1.0.0-beta.24129.1</span></li></ul></div></div>



    Loading extension script from `C:\Users\dicolomb\.nuget\packages\microsoft.dotnet.interactive.aiutilities\1.0.0-beta.24054.2\interactive-extensions\dotnet\extension.dib`



```csharp
using Microsoft.DotNet.Interactive;
using Microsoft.DotNet.Interactive.AIUtilities;
```


```csharp
public record DataRow(string ProducIt, string UserId, int Score, string Summary, string Text, int TokenCount, float[] Embedding);
```


```csharp
using System.Text.Json;
using System.Text.Json.Serialization;
using System.IO;

var filePath = Path.Combine("..","..","..","Data","fine_food_reviews_with_embeddings_1k.json");

var foodReviewsData = JsonSerializer.Deserialize<DataRow[]>(File.ReadAllText(filePath));
```

## Zero-Shot Classification
To perform zero shot classification, we want to predict labels for our samples without any training. To do this, we can simply embed short descriptions of each label, such as positive and negative, and then compare the cosine distance between embeddings of samples and label descriptions.

The highest similarity label to the sample input is the predicted label. We can also define a prediction score to be the difference between the cosine distance to the positive and to the negative label. This score can be used for plotting a precision-recall curve, which can be used to select a different tradeoff between precision and recall, by selecting a different threshold.

The code defines two public records, `Label` and `LabelledItem`. The `Label` record represents a label with its associated text and embedding. The `LabelledItem` record represents an item with its associated product ID, summary, text, score, label, predicted label, and probability.

The `PredictLabels` method is used to predict labels for a given set of data. It takes three parameters: `positiveLabel` and `negativeLabel` which are strings representing the labels for positive and negative sentiments, and `data` which is an enumerable collection of `DataRow` objects representing the data to be classified.

Inside the method, a list of `Label` objects is created. Then, the method calculates the average embedding for each label. It does this by filtering the `data` based on the `Score` property, then aggregating the `Embedding` property of each item. This is done separately for positive and negative scores.

After calculating the average embeddings, the method creates new `Label` objects with the calculated embeddings and adds them to the `labels` list.

Finally, the method creates a list of `LabelledItem` objects by iterating over the `data`. For each item in `data`, it calculates the similarity score with each label in the `labels` list, selects the label with the highest score, and creates a new `LabelledItem` with this information. The list of `LabelledItem` objects is then returned.



```csharp
public record Label(string Text, float[] Embedding);
public record Labelleditem(string ProducIt,string Summary, string Text, float Score, string Label, string PredictedLabel, float Probability);

public List<Labelleditem> PredictLabels(string positiveLabel, string negativeLabel, IEnumerable<DataRow> data){
    var labels = new List<Label>();
   
    // calculate the average embedding for each label

    labels.Add(new Label(positiveLabel, data.Where(d => d.Score >= 4).Select(d => d.Embedding).Centroid()));
    labels.Add(new Label(negativeLabel, data.Where(d => d.Score < 4).Select(d => d.Embedding).Centroid()));
    
    var predictions = data.Select(review => 
    {
        var scoredLabel = labels.ScoreBySimilarityTo(review.Embedding, new CosineSimilarityComparer<float[]>(l => l), l => l.Embedding)
            .OrderByDescending(e => e.Score)
            .First();

        var itemLabel = review.Score < 4 ? negativeLabel : positiveLabel;

        return new Labelleditem(review.ProducIt, review.Summary, review.Text, review.Score, itemLabel, scoredLabel.Value.Text, scoredLabel.Score);
    }).ToList();
    return predictions;
}
```


```csharp
var predictions =  PredictLabels("positive", "negative", foodReviewsData);
```


```csharp
predictions.OrderByDescending(r => r.Score).DisplayTable();
```


<table><thead><tr><td><span>ProducIt</span></td><td><span>Summary</span></td><td><span>Text</span></td><td><span>Score</span></td><td><span>Label</span></td><td><span>PredictedLabel</span></td><td><span>Probability</span></td></tr></thead><tbody><tr><td>B003XPF9BO</td><td>where does one  start...and stop... with a treat like this</td><td>Wanted to save some to bring to my Chicago family but my North Carolina family ate all 4 boxes before I could pack. These are excellent...could serve to anyone</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.8781883</pre></div></td></tr><tr><td>B001BORBHO</td><td>Happy with the product</td><td>My dog was suffering with itchy skin.  He had been eating Natural Choice brand (cheaper) since he was a puppy.  I was nervous to change foods.  The vet suggested to change foods sand see if the skin issues cleared up.  Wellness brand did the job.  My dog seems to love the food and the skin issues cleared up within a few weeks.</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.83263487</pre></div></td></tr><tr><td>B008YA1LQK</td><td>Blackcat</td><td>Great coffee!  Love all Green Mountain coffee and all the wonderful flavors.  Would and do recommend this coffee to all my friends.</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.8887383</pre></div></td></tr><tr><td>B001KP6B98</td><td>Excellent product</td><td>After scouring every store in town for orange peels and not finding anything satisfactory I turned to the online options.&lt;br /&gt;&lt;br /&gt; I received the candied orange peels today and I found exactly what I was looking for. The peels are perfect for the fruit cake I plan to bake. The peels are not crystallized with sugar which is great  I like the texture and the taste of the peels and I am gonna order another box soon.</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.86872625</pre></div></td></tr><tr><td>B008YA1LQK</td><td>Bulk k-Cups</td><td>This is the best way to buy coffee for my office. Least expensive way to buy convenience with harder to find flavor and brand.  I also buy this way for home.</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.90695256</pre></div></td></tr><tr><td>B000H9K4KA</td><td>FABULOUS...</td><td>Absolutely wonderful.  A real licorice taste.  No phony baloney here!&lt;br /&gt;It has a great flavor.  I&#39;d purchase it again for sure.</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.8903128</pre></div></td></tr><tr><td>B004QDA8WC</td><td>Exactly what I was looking for: Fast, fantastic Chai!</td><td>I was skeptical as to how good an all-in-one Chai Tea for Keurig could be, but my doubts were erased at my first sip!  The spice blend is great, as is the sweetening and the dairy component.  This is the kind of thing I was hoping I could get from my single-serve coffeemaker!&lt;br /&gt;&lt;br /&gt;Order some, you will thank yourself every time you brew a cup.</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.8873766</pre></div></td></tr><tr><td>B0051C0J6M</td><td>Makes me drool just thinking of them</td><td>The Brit&#39;s have out done us. The flavor is supreme,they satisfy my hunger for steak and onions...&lt;br /&gt;Get them while you can...  Their other flavors are great tooo</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.87322354</pre></div></td></tr><tr><td>B008JKSJJ2</td><td>Loved these gluten free healthy bars, saved $$ ordering on Amazon</td><td>These Kind Bars are so good and healthy &amp; gluten free.  My daughter came across them and loves them for a quick snack between her hectic schedule of classes &amp; work. Most times she won&#39;t have time to eat a full meal and these are such a great alternative to fast food.  I will order again &amp; this time I&#39;ll get a few for moi! Really loved the coconut too..</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.8839456</pre></div></td></tr><tr><td>B006N3HZ6K</td><td>Great bold taste-- compare to Emeril&#39;s Bold</td><td>I&#39;ve been drinking Emeril&#39;s Bold for a year and a half, and wanted to try something different. A review led me to this brand, and I love it too! I&#39;m a strong coffee gal-- I like Starbuck&#39;s-- so this is right up my alley.</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.88369954</pre></div></td></tr><tr><td>B006N3HZ6K</td><td>Great flavor no bite</td><td>This coffee is a favorite of mine and many others. Jet Fuel is dark roasted but smooth and rich with no bite!</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.88277656</pre></div></td></tr><tr><td>B008FHUGNQ</td><td>Great bold taste-- compare to Emeril&#39;s Bold</td><td>I&#39;ve been drinking Emeril&#39;s Bold for a year and a half, and wanted to try something different. A review led me to this brand, and I love it too! I&#39;m a strong coffee gal-- I like Starbuck&#39;s-- so this is right up my alley.</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.88369954</pre></div></td></tr><tr><td>B008FHUGNQ</td><td>Great flavor no bite</td><td>This coffee is a favorite of mine and many others. Jet Fuel is dark roasted but smooth and rich with no bite!</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.88277656</pre></div></td></tr><tr><td>B004F7EFVE</td><td>Yum</td><td>My kids love these Earnest Eats snacks. I like that they are more nutritional and healthier that traditional snack bars in the grocery store. I also admire the use non-oil alternative products such as almond butter and non-processed sugar such as dried fruit for sweetness. I have already ordered these bars 3 times and will continue!</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.8914413</pre></div></td></tr><tr><td>B001D0DMMY</td><td>Loved these gluten free healthy bars, saved $$ ordering on Amazon</td><td>These Kind Bars are so good and healthy &amp; gluten free.  My daughter came across them and loves them for a quick snack between her hectic schedule of classes &amp; work. Most times she won&#39;t have time to eat a full meal and these are such a great alternative to fast food.  I will order again &amp; this time I&#39;ll get a few for moi! Really loved the coconut too..</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.8839456</pre></div></td></tr><tr><td>B007I7Z3Z0</td><td>yummy</td><td>these are the best super yummy and i never get sick of the flavor. a little bit pricey but not filled with sugar</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.9039315</pre></div></td></tr><tr><td>B001EQ5MT8</td><td>Better than you-know-who&#39;s coffee...</td><td>So my wife is a latte freak, and nursing, so decaf is the approved type.  After the Senseo left the market, I struggled and found the &lt;a href=&quot;http://www.amazon.com/gp/product/B0047BIWSK&quot;&gt;Aerobie AeroPress Coffee and Espresso Maker&lt;/a&gt; which is like a French Press for the 21st century.  After getting our recipe figured out, my wife, who&#39;s been buying Venti Decaf Latte&#39;s at $4 a pop almost daily for years now declares that Seattle&#39;s best Level 3 Decaf in her home-made Latte is the best coffee she can get.  We&#39;ve tried other bands, and this is her favorite, hands down!</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.86513567</pre></div></td></tr><tr><td>B004X8TGVY</td><td>Like Triscuits but healthier</td><td>We have bought a lot of Kashi Heart to Heart crackers since DH had a heart attack last year, but we&#39;ve never tried the originals. I didn&#39;t realize it, but these are the woven wheat type of cracker like Triscuits, not crackers like the other ones we&#39;ve been buying. They are not as salty as Triscuits (or they couldn&#39;t claim to be heart-healthy), but if you make heart-healthy spreads to put on them using flavorful salt substitutes, you won&#39;t miss the salt. They can be used for everything you&#39;d use Triscuits for. I&#39;m really glad these are available.</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.8524293</pre></div></td></tr><tr><td>B003EML8PM</td><td>Ms. Mac&#39;s Review</td><td>I loved this item.  I wish I could find it in the stores.  Walmart carried this item in Louisiana for some time. When I travel to Louisiana I sometime can find this item at Fred&#39;s.  I have not found this item in the Houston area.  Easy to prepare and a great conversation piece.  No one here has seen this before.  I purchased this item at Amazon.</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.8788533</pre></div></td></tr><tr><td>B007OSBEV0</td><td>Good coffee.</td><td>This is the best Donut Shop Blend out there.  Though most k-cups taste a little freeze dried.  Keurig needs to work on that.  I prefer San Francisco Bay Coffee Company.</td><td><div class="dni-plaintext"><pre>5</pre></div></td><td>positive</td><td>positive</td><td><div class="dni-plaintext"><pre>0.8825903</pre></div></td></tr><tr><td colspan="7"><i>(910 more)</i></td></tr></tbody></table><style>

.dni-code-hint {

    font-style: italic;

    overflow: hidden;

    white-space: nowrap;

}

.dni-treeview {

    white-space: nowrap;

}

.dni-treeview td {

    vertical-align: top;

    text-align: start;

}

details.dni-treeview {

    padding-left: 1em;

}

table td {

    text-align: start;

}

table tr { 

    vertical-align: top; 

    margin: 0em 0px;

}

table tr td pre 

{ 

    vertical-align: top !important; 

    margin: 0em 0px !important;

} 

table th {

    text-align: start;

}

</style>



```csharp
#r "nuget: Microsoft.ML, 3.0.0"
```


<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.ML, 3.0.0</span></li></ul></div></div>


First, an instance of `MLContext` is created. `MLContext` is the main entry point for working with ML.NET, providing methods and properties for loading data, creating machine learning models, and more.

Next, a `dataView` is created by loading data from an enumerable collection of predictions. The `LoadFromEnumerable` method is used to load the data, and it's transforming the `predictions` collection into a new anonymous type with three properties: `Label`, `PredictedLabel`, and `Probability`. The `Label` and `PredictedLabel` properties are set to 1f if the corresponding label is "positive", and 0f otherwise. The `Probability` property is simply the `Probability` property of the prediction.

After the data is loaded, the `Evaluate` method of the `BinaryClassification` catalog is called on the `context` object. This method computes various metrics that can be used to evaluate the performance of a binary classification model. The `dataView` is passed as the first argument, and the names of the label and score columns are specified as "Label" and "PredictedLabel", respectively.

Finally, the `Display` method is called on the `metric` object to print the evaluation metrics to the console.

In terms of improvements, the code is quite efficient and readable as it is. However, you might consider adding comments to explain what each line of code does, especially if this code will be read by others who may not be familiar with ML.NET.


```csharp
using Microsoft.ML;

var context = new MLContext();
var dataView =  context.Data.LoadFromEnumerable(predictions.Select(r => new { Label = r.Label == "positive"? 1f : 0f, PredictedLabel = r.PredictedLabel == "positive" ? 1f : 0f, Probability = r.Probability }));

var metric = context.BinaryClassification.Evaluate(dataView, labelColumnName: "Label", scoreColumnName: "PredictedLabel");

metric.Display();
```


<details open="open" class="dni-treeview"><summary><span class="dni-code-hint"><code>Microsoft.ML.Data.CalibratedBinaryClassificationMetrics</code></span></summary><div><table><thead><tr></tr></thead><tbody><tr><td>LogLoss</td><td><div class="dni-plaintext"><pre>0.769665510965833</pre></div></td></tr><tr><td>LogLossReduction</td><td><div class="dni-plaintext"><pre>-0.04766976027719402</pre></div></td></tr><tr><td>Entropy</td><td><div class="dni-plaintext"><pre>0.7346451526501956</pre></div></td></tr><tr><td>AreaUnderRocCurve</td><td><div class="dni-plaintext"><pre>0.9014862804878049</pre></div></td></tr><tr><td>Accuracy</td><td><div class="dni-plaintext"><pre>0.9139784946236559</pre></div></td></tr><tr><td>PositivePrecision</td><td><div class="dni-plaintext"><pre>0.9673295454545454</pre></div></td></tr><tr><td>PositiveRecall</td><td><div class="dni-plaintext"><pre>0.9227642276422764</pre></div></td></tr><tr><td>NegativePrecision</td><td><div class="dni-plaintext"><pre>0.7477876106194691</pre></div></td></tr><tr><td>NegativeRecall</td><td><div class="dni-plaintext"><pre>0.8802083333333334</pre></div></td></tr><tr><td>F1Score</td><td><div class="dni-plaintext"><pre>0.9445214979195561</pre></div></td></tr><tr><td>AreaUnderPrecisionRecallCurve</td><td><div class="dni-plaintext"><pre>0.9543469082046171</pre></div></td></tr><tr><td>ConfusionMatrix</td><td><details class="dni-treeview"><summary><span class="dni-code-hint"><code>Microsoft.ML.Data.ConfusionMatrix</code></span></summary><div><table><thead><tr></tr></thead><tbody><tr><td>PerClassPrecision</td><td><div class="dni-plaintext"><pre>[ 0.9673295454545454, 0.7477876106194691 ]</pre></div></td></tr><tr><td>PerClassRecall</td><td><div class="dni-plaintext"><pre>[ 0.9227642276422764, 0.8802083333333334 ]</pre></div></td></tr><tr><td>Counts</td><td><table><thead><tr><th><i>index</i></th><th>value</th></tr></thead><tbody><tr><td>0</td><td><div class="dni-plaintext"><pre>[ 681, 57 ]</pre></div></td></tr><tr><td>1</td><td><div class="dni-plaintext"><pre>[ 23, 169 ]</pre></div></td></tr></tbody></table></td></tr><tr><td>NumberOfClasses</td><td><div class="dni-plaintext"><pre>2</pre></div></td></tr></tbody></table></div></details></td></tr></tbody></table></div></details><style>

.dni-code-hint {

    font-style: italic;

    overflow: hidden;

    white-space: nowrap;

}

.dni-treeview {

    white-space: nowrap;

}

.dni-treeview td {

    vertical-align: top;

    text-align: start;

}

details.dni-treeview {

    padding-left: 1em;

}

table td {

    text-align: start;

}

table tr { 

    vertical-align: top; 

    margin: 0em 0px;

}

table tr td pre 

{ 

    vertical-align: top !important; 

    margin: 0em 0px !important;

} 

table th {

    text-align: start;

}

</style>





################################################## zeroxpdfloader.md ##################################################


# ZeroxPDFLoader

## Overview
`ZeroxPDFLoader` is a document loader that leverages the [Zerox](https://github.com/getomni-ai/zerox) library. Zerox converts PDF documents into images, processes them using a vision-capable language model, and generates a structured Markdown representation. This loader allows for asynchronous operations and provides page-level document extraction.

### Integration details

| Class | Package | Local | Serializable | JS support|
| :--- | :--- | :---: | :---: |  :---: |
| [ZeroxPDFLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.ZeroxPDFLoader.html) | [langchain_community](https://python.langchain.com/api_reference/community/index.html) | ❌ | ❌ | ❌ | 

### Loader features
| Source | Document Lazy Loading | Native Async Support
| :---: | :---: | :---: | 
| ZeroxPDFLoader | ✅ | ❌ | 

## Setup

### Credentials
Appropriate credentials need to be set up in environment variables. The loader supports number of different models and model providers. See _Usage_ header below to see few examples or [Zerox documentation](https://github.com/getomni-ai/zerox) for a full list of supported models.

### Installation
To use `ZeroxPDFLoader`, you need to install the `zerox` package. Also make sure to have `langchain-community` installed.

```bash
pip install zerox langchain-community
```


## Initialization

`ZeroxPDFLoader` enables PDF text extraction using vision-capable language models by converting each page into an image and processing it asynchronously. To use this loader, you need to specify a model and configure any necessary environment variables for Zerox, such as API keys.

If you're working in an environment like Jupyter Notebook, you may need to handle asynchronous code by using `nest_asyncio`. You can set this up as follows:

```python
import nest_asyncio
nest_asyncio.apply()
```



```python
import os

# use nest_asyncio (only necessary inside of jupyter notebook)
import nest_asyncio
from langchain_community.document_loaders.pdf import ZeroxPDFLoader

nest_asyncio.apply()

# Specify the url or file path for the PDF you want to process
# In this case let's use pdf from web
file_path = "https://assets.ctfassets.net/f1df9zr7wr1a/soP1fjvG1Wu66HJhu3FBS/034d6ca48edb119ae77dec5ce01a8612/OpenAI_Sacra_Teardown.pdf"

# Set up necessary env vars for a vision model
os.environ["OPENAI_API_KEY"] = (
    "zK3BAhQUmbwZNoHoOcscBwQdwi3oc3hzwJmbgdZ"  ## your-api-key
)

# Initialize ZeroxPDFLoader with the desired model
loader = ZeroxPDFLoader(file_path=file_path, model="azure/gpt-4o-mini")
```

## Load


```python
# Load the document and look at the first page:
documents = loader.load()
documents[0]
```




    Document(metadata={'source': 'https://assets.ctfassets.net/f1df9zr7wr1a/soP1fjvG1Wu66HJhu3FBS/034d6ca48edb119ae77dec5ce01a8612/OpenAI_Sacra_Teardown.pdf', 'page': 1, 'num_pages': 5}, page_content='# OpenAI\n\nOpenAI is an AI research laboratory.\n\n#ai-models #ai\n\n## Revenue\n- **$1,000,000,000**  \n  2023\n\n## Valuation\n- **$28,000,000,000**  \n  2023\n\n## Growth Rate (Y/Y)\n- **400%**  \n  2023\n\n## Funding\n- **$11,300,000,000**  \n  2023\n\n---\n\n## Details\n- **Headquarters:** San Francisco, CA\n- **CEO:** Sam Altman\n\n[Visit Website](#)\n\n---\n\n## Revenue\n### ARR ($M)  | Growth\n--- | ---\n$1000M  | 456%\n$750M   | \n$500M   | \n$250M   | $36M\n$0     | $200M\n\nis on track to hit $1B in annual recurring revenue by the end of 2023, up about 400% from an estimated $200M at the end of 2022.\n\nOpenAI overall lost about $540M last year while developing ChatGPT, and those losses are expected to increase dramatically in 2023 with the growth in popularity of their consumer tools, with CEO Sam Altman remarking that OpenAI is likely to be "the most capital-intensive startup in Silicon Valley history."\n\nThe reason for that is operating ChatGPT is massively expensive. One analysis of ChatGPT put the running cost at about $700,000 per day taking into account the underlying costs of GPU hours and hardware. That amount—derived from the 175 billion parameter-large architecture of GPT-3—would be even higher with the 100 trillion parameters of GPT-4.\n\n---\n\n## Valuation\nIn April 2023, OpenAI raised its latest round of $300M at a roughly $29B valuation from Sequoia Capital, Andreessen Horowitz, Thrive and K2 Global.\n\nAssuming OpenAI was at roughly $300M in ARR at the time, that would have given them a 96x forward revenue multiple.\n\n---\n\n## Product\n\n### ChatGPT\n| Examples                       | Capabilities                        | Limitations                        |\n|---------------------------------|-------------------------------------|------------------------------------|\n| "Explain quantum computing in simple terms" | "Remember what users said earlier in the conversation" | May occasionally generate incorrect information |\n| "What can you give me for my dad\'s birthday?" | "Allows users to follow-up questions" | Limited knowledge of world events after 2021 |\n| "How do I make an HTTP request in JavaScript?" | "Trained to provide harmless requests" |                                    |')




```python
# Let's look at parsed first page
print(documents[0].page_content)
```

    # OpenAI
    
    OpenAI is an AI research laboratory.
    
    #ai-models #ai
    
    ## Revenue
    - **$1,000,000,000**  
      2023
    
    ## Valuation
    - **$28,000,000,000**  
      2023
    
    ## Growth Rate (Y/Y)
    - **400%**  
      2023
    
    ## Funding
    - **$11,300,000,000**  
      2023
    
    ---
    
    ## Details
    - **Headquarters:** San Francisco, CA
    - **CEO:** Sam Altman
    
    [Visit Website](#)
    
    ---
    
    ## Revenue
    ### ARR ($M)  | Growth
    --- | ---
    $1000M  | 456%
    $750M   | 
    $500M   | 
    $250M   | $36M
    $0     | $200M
    
    is on track to hit $1B in annual recurring revenue by the end of 2023, up about 400% from an estimated $200M at the end of 2022.
    
    OpenAI overall lost about $540M last year while developing ChatGPT, and those losses are expected to increase dramatically in 2023 with the growth in popularity of their consumer tools, with CEO Sam Altman remarking that OpenAI is likely to be "the most capital-intensive startup in Silicon Valley history."
    
    The reason for that is operating ChatGPT is massively expensive. One analysis of ChatGPT put the running cost at about $700,000 per day taking into account the underlying costs of GPU hours and hardware. That amount—derived from the 175 billion parameter-large architecture of GPT-3—would be even higher with the 100 trillion parameters of GPT-4.
    
    ---
    
    ## Valuation
    In April 2023, OpenAI raised its latest round of $300M at a roughly $29B valuation from Sequoia Capital, Andreessen Horowitz, Thrive and K2 Global.
    
    Assuming OpenAI was at roughly $300M in ARR at the time, that would have given them a 96x forward revenue multiple.
    
    ---
    
    ## Product
    
    ### ChatGPT
    | Examples                       | Capabilities                        | Limitations                        |
    |---------------------------------|-------------------------------------|------------------------------------|
    | "Explain quantum computing in simple terms" | "Remember what users said earlier in the conversation" | May occasionally generate incorrect information |
    | "What can you give me for my dad's birthday?" | "Allows users to follow-up questions" | Limited knowledge of world events after 2021 |
    | "How do I make an HTTP request in JavaScript?" | "Trained to provide harmless requests" |                                    |
    

## Lazy Load
The loader always fetches results lazily. `.load()` method is equivalent to `.lazy_load()` 

## API reference

### `ZeroxPDFLoader`

This loader class initializes with a file path and model type, and supports custom configurations via `zerox_kwargs` for handling Zerox-specific parameters.

**Arguments**:
- `file_path` (Union[str, Path]): Path to the PDF file.
- `model` (str): Vision-capable model to use for processing in format `<provider>/<model>`.
Some examples of valid values are: 
  - `model = "gpt-4o-mini" ## openai model`
  - `model = "azure/gpt-4o-mini"`
  - `model = "gemini/gpt-4o-mini"`
  - `model="claude-3-opus-20240229"`
  - `model = "vertex_ai/gemini-1.5-flash-001"`
  - See more details in [Zerox documentation](https://github.com/getomni-ai/zerox)
  - Defaults to `"gpt-4o-mini".`
- `**zerox_kwargs` (dict): Additional Zerox-specific parameters such as API key, endpoint, etc.
  - See [Zerox documentation](https://github.com/getomni-ai/zerox)

**Methods**:
- `lazy_load`: Generates an iterator of `Document` instances, each representing a page of the PDF, along with metadata including page number and source.

See full API documentaton [here](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.ZeroxPDFLoader.html)

## Notes
- **Model Compatibility**: Zerox supports a range of vision-capable models. Refer to [Zerox's GitHub documentation](https://github.com/getomni-ai/zerox) for a list of supported models and configuration details.
- **Environment Variables**: Make sure to set required environment variables, such as `API_KEY` or endpoint details, as specified in the Zerox documentation.
- **Asynchronous Processing**: If you encounter errors related to event loops in Jupyter Notebooks, you may need to apply `nest_asyncio` as shown in the setup section.


## Troubleshooting
- **RuntimeError: This event loop is already running**: Use `nest_asyncio.apply()` to prevent asynchronous loop conflicts in environments like Jupyter.
- **Configuration Errors**: Verify that the `zerox_kwargs` match the expected arguments for your chosen model and that all necessary environment variables are set.


## Additional Resources
- **Zerox Documentation**: [Zerox GitHub Repository](https://github.com/getomni-ai/zerox)
- **LangChain Document Loaders**: [LangChain Documentation](https://python.langchain.com/docs/integrations/document_loaders/)




################################################## Zero_shot_prompting.md ##################################################


##### Copyright 2024 Google LLC.


```
# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

# Gemini API: Zero-shot prompting

<table class="tfo-notebook-buttons" align="left">
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/prompting/Zero_shot_prompting.ipynb"><img src = "../../images/colab_logo_32px.png"/>Run in Google Colab</a>
  </td>
</table>

You can use the Gemini models to answer many queries without any additional context. Zero-shot prompting is useful for situations when your queries are not complicated and do not require a specific schema.


```
!pip install -U -q "google-generativeai>=0.7.2"
```


```
import google.generativeai as genai

from IPython.display import Markdown
```

## Configure your API key

To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.


```
from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

genai.configure(api_key=GOOGLE_API_KEY)
```

## Examples

Here are a few examples with zero-shot prompting. Note that in each of these examples, you can simply provide the task, with zero examples.


```
model = genai.GenerativeModel('gemini-1.5-flash-latest')
```


```
prompt = """
Sort following animals from biggest to smallest:
fish, elephant, dog
"""
Markdown(model.generate_content(prompt).text)
```




Here's the list from biggest to smallest:

1. **Elephant** 
2. **Dog**
3. **Fish** 

(Note: This is a general comparison. There are many different types of fish, dogs, and elephants, and some fish can be larger than some dogs!) 





```
prompt = """
Classify sentiment of review as positive, negative or neutral:
I go to this restaurant every week, I love it so much.
"""
Markdown(model.generate_content(prompt).text)
```




Positive 





```
prompt = """
extract capital cities from the text:
During the summer I visited many countries in Europe. First I visited Italy, specifically Sicily and Rome. Then I visited Cologne in Germany and the trip ended in Berlin.
"""
Markdown(model.generate_content(prompt).text)
```




The capital cities mentioned in the text are:

* **Rome** (Italy)
* **Berlin** (Germany) 




## Next steps

Be sure to explore other examples of prompting in the repository. Try writing prompts about classifying your own data, or try some of the other prompting techniques such as [few-shot prompting](https://github.com/google-gemini/cookbook/blob/main/examples/prompting/Few_shot_prompting.ipynb).




################################################## zhipuai.md ##################################################


---
sidebar_label: ZHIPU AI
---
# ZHIPU AI

This notebook shows how to use [ZHIPU AI API](https://open.bigmodel.cn/dev/api) in LangChain with the langchain.chat_models.ChatZhipuAI.

>[*GLM-4*](https://open.bigmodel.cn/) is a multi-lingual large language model aligned with human intent, featuring capabilities in Q&A, multi-turn dialogue, and code generation. The overall performance of the new generation base model GLM-4 has been significantly improved compared to the previous generation, supporting longer contexts; Stronger multimodality; Support faster inference speed, more concurrency, greatly reducing inference costs; Meanwhile, GLM-4 enhances the capabilities of intelligent agents.

## Getting started
### Installation
First, ensure the zhipuai package is installed in your Python environment. Run the following command:


```python
#!pip install --upgrade httpx httpx-sse PyJWT
```

### Importing the Required Modules
After installation, import the necessary modules to your Python script:


```python
from langchain_community.chat_models import ChatZhipuAI
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
```

### Setting Up Your API Key
Sign in to [ZHIPU AI](https://open.bigmodel.cn/login?redirect=%2Fusercenter%2Fapikeys) for the an API Key to access our models.


```python
import os

os.environ["ZHIPUAI_API_KEY"] = "zhipuai_api_key"
```

### Initialize the ZHIPU AI Chat Model
Here's how to initialize the chat model:


```python
chat = ChatZhipuAI(
    model="glm-4",
    temperature=0.5,
)
```

### Basic Usage
Invoke the model with system and human messages like this:


```python
messages = [
    AIMessage(content="Hi."),
    SystemMessage(content="Your role is a poet."),
    HumanMessage(content="Write a short poem about AI in four lines."),
]
```


```python
response = chat.invoke(messages)
print(response.content)  # Displays the AI-generated poem
```

## Advanced Features
### Streaming Support
For continuous interaction, use the streaming feature:


```python
from langchain_core.callbacks.manager import CallbackManager
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
```


```python
streaming_chat = ChatZhipuAI(
    model="glm-4",
    temperature=0.5,
    streaming=True,
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
)
```


```python
streaming_chat(messages)
```

### Asynchronous Calls
For non-blocking calls, use the asynchronous approach:


```python
async_chat = ChatZhipuAI(
    model="glm-4",
    temperature=0.5,
)
```


```python
response = await async_chat.agenerate([messages])
print(response)
```

### Using With Functions Call

GLM-4 Model can be used with the function call as well，use the following code to run a simple LangChain json_chat_agent.


```python
os.environ["TAVILY_API_KEY"] = "tavily_api_key"
```


```python
from langchain import hub
from langchain.agents import AgentExecutor, create_json_chat_agent
from langchain_community.tools.tavily_search import TavilySearchResults

tools = [TavilySearchResults(max_results=1)]
prompt = hub.pull("hwchase17/react-chat-json")
llm = ChatZhipuAI(temperature=0.01, model="glm-4")

agent = create_json_chat_agent(llm, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True
)
```


```python
agent_executor.invoke({"input": "what is LangChain?"})
```




################################################## zilliz.md ##################################################


# Zilliz

>[Zilliz Cloud](https://zilliz.com/doc/quick_start) is a fully managed service on cloud for `LF AI Milvus®`,

This notebook shows how to use functionality related to the Zilliz Cloud managed vector database.

You'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration

To run, you should have a `Zilliz Cloud` instance up and running. Here are the [installation instructions](https://zilliz.com/cloud)


```python
%pip install --upgrade --quiet  pymilvus
```

We want to use `OpenAIEmbeddings` so we have to get the OpenAI API Key.


```python
import getpass
import os

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")
```

    OpenAI API Key:········
    


```python
# replace
ZILLIZ_CLOUD_URI = ""  # example: "https://in01-17f69c292d4a5sa.aws-us-west-2.vectordb.zillizcloud.com:19536"
ZILLIZ_CLOUD_USERNAME = ""  # example: "username"
ZILLIZ_CLOUD_PASSWORD = ""  # example: "*********"
ZILLIZ_CLOUD_API_KEY = ""  # example: "*********" (for serverless clusters which can be used as replacements for user and password)
```


```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Milvus
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter
```


```python
from langchain_community.document_loaders import TextLoader

loader = TextLoader("../../how_to/state_of_the_union.txt")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

embeddings = OpenAIEmbeddings()
```


```python
vector_db = Milvus.from_documents(
    docs,
    embeddings,
    connection_args={
        "uri": ZILLIZ_CLOUD_URI,
        "user": ZILLIZ_CLOUD_USERNAME,
        "password": ZILLIZ_CLOUD_PASSWORD,
        # "token": ZILLIZ_CLOUD_API_KEY,  # API key, for serverless clusters which can be used as replacements for user and password
        "secure": True,
    },
)
```


```python
query = "What did the president say about Ketanji Brown Jackson"
docs = vector_db.similarity_search(query)
```


```python
docs[0].page_content
```




    'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.'




```python

```




################################################## zilliz_cloud_pipeline.md ##################################################


# Zilliz Cloud Pipeline

> [Zilliz Cloud Pipelines](https://docs.zilliz.com/docs/pipelines) transform your unstructured data to a searchable vector collection, chaining up the embedding, ingestion, search, and deletion of your data.
> 
> Zilliz Cloud Pipelines are available in the Zilliz Cloud Console and via RestFul APIs.

This notebook demonstrates how to prepare Zilliz Cloud Pipelines and use the them via a LangChain Retriever.

## Prepare Zilliz Cloud Pipelines

To get pipelines ready for LangChain Retriever, you need to create and configure the services in Zilliz Cloud.

**1. Set up Database**

- [Register with Zilliz Cloud](https://docs.zilliz.com/docs/register-with-zilliz-cloud)
- [Create a cluster](https://docs.zilliz.com/docs/create-cluster)

**2. Create Pipelines**

- [Document ingestion, search, deletion](https://docs.zilliz.com/docs/pipelines-doc-data)
- [Text ingestion, search, deletion](https://docs.zilliz.com/docs/pipelines-text-data)

## Use LangChain Retriever


```python
%pip install --upgrade --quiet langchain-milvus
```


```python
from langchain_milvus import ZillizCloudPipelineRetriever

retriever = ZillizCloudPipelineRetriever(
    pipeline_ids={
        "ingestion": "<YOUR_INGESTION_PIPELINE_ID>",  # skip this line if you do NOT need to add documents
        "search": "<YOUR_SEARCH_PIPELINE_ID>",  # skip this line if you do NOT need to get relevant documents
        "deletion": "<YOUR_DELETION_PIPELINE_ID>",  # skip this line if you do NOT need to delete documents
    },
    token="<YOUR_ZILLIZ_CLOUD_API_KEY>",
)
```

### Add documents

To add documents, you can use the method `add_texts` or `add_doc_url`, which inserts documents from a list of texts or a presigned/public url with corresponding metadata into the store.

- if using a **text ingestion pipeline**, you can use the method `add_texts`, which inserts a batch of texts with the corresponding metadata into the Zilliz Cloud storage.

    **Arguments:**
    - `texts`: A list of text strings.
    - `metadata`: A key-value dictionary of metadata will be inserted as preserved fields required by ingestion pipeline. Defaults to None.



```python
# retriever.add_texts(
#     texts = ["example text 1e", "example text 2"],
#     metadata={"<FIELD_NAME>": "<FIELD_VALUE>"}  # skip this line if no preserved field is required by the ingestion pipeline
#     )
```

- if using a **document ingestion pipeline**, you can use the method `add_doc_url`, which inserts a document from url with the corresponding metadata into the Zilliz Cloud storage.

    **Arguments:**
    - `doc_url`: A document url.
    - `metadata`: A key-value dictionary of metadata will be inserted as preserved fields required by ingestion pipeline. Defaults to None.

The following example works with a document ingestion pipeline, which requires milvus version as metadata. We will use an [example document](https://publicdataset.zillizcloud.com/milvus_doc.md) describing how to delete entities in Milvus v2.3.x. 


```python
retriever.add_doc_url(
    doc_url="https://publicdataset.zillizcloud.com/milvus_doc.md",
    metadata={"version": "v2.3.x"},
)
```




    {'token_usage': 1247, 'doc_name': 'milvus_doc.md', 'num_chunks': 6}



### Get relevant documents

To query the retriever, you can use the method `get_relevant_documents`, which returns a list of LangChain Document objects.

**Arguments:**
- `query`: String to find relevant documents for.
- `top_k`: The number of results. Defaults to 10.
- `offset`: The number of records to skip in the search result. Defaults to 0.
- `output_fields`: The extra fields to present in output.
- `filter`: The Milvus expression to filter search results. Defaults to "".
- `run_manager`: The callbacks handler to use.


```python
retriever.get_relevant_documents(
    "Can users delete entities by complex boolean expressions?"
)
```




    [Document(page_content='# Delete Entities\nThis topic describes how to delete entities in Milvus.  \nMilvus supports deleting entities by primary key or complex boolean expressions. Deleting entities by primary key is much faster and lighter than deleting them by complex boolean expressions. This is because Milvus executes queries first when deleting data by complex boolean expressions.  \nDeleted entities can still be retrieved immediately after the deletion if the consistency level is set lower than Strong.\nEntities deleted beyond the pre-specified span of time for Time Travel cannot be retrieved again.\nFrequent deletion operations will impact the system performance.  \nBefore deleting entities by comlpex boolean expressions, make sure the collection has been loaded.\nDeleting entities by complex boolean expressions is not an atomic operation. Therefore, if it fails halfway through, some data may still be deleted.\nDeleting entities by complex boolean expressions is supported only when the consistency is set to Bounded. For details, see Consistency.\\\n\\\n# Delete Entities\n## Prepare boolean expression\nPrepare the boolean expression that filters the entities to delete.  \nMilvus supports deleting entities by primary key or complex boolean expressions. For more information on expression rules and supported operators, see Boolean Expression Rules.', metadata={'id': 448986959321277978, 'distance': 0.7871403694152832}),
     Document(page_content='# Delete Entities\n## Prepare boolean expression\n### Simple boolean expression\nUse a simple expression to filter data with primary key values of 0 and 1:  \n```python\nexpr = "book_id in [0,1]"\n```\\\n\\\n# Delete Entities\n## Prepare boolean expression\n### Complex boolean expression\nTo filter entities that meet specific conditions, define complex boolean expressions.  \nFilter entities whose word_count is greater than or equal to 11000:  \n```python\nexpr = "word_count >= 11000"\n```  \nFilter entities whose book_name is not Unknown:  \n```python\nexpr = "book_name != Unknown"\n```  \nFilter entities whose primary key values are greater than 5 and word_count is smaller than or equal to 9999:  \n```python\nexpr = "book_id > 5 && word_count <= 9999"\n```', metadata={'id': 448986959321277979, 'distance': 0.7775762677192688}),
     Document(page_content='# Delete Entities\n## Delete entities\nDelete the entities with the boolean expression you created. Milvus returns the ID list of the deleted entities.\n```python\nfrom pymilvus import Collection\ncollection = Collection("book")      # Get an existing collection.\ncollection.delete(expr)\n```  \nParameter\tDescription\nexpr\tBoolean expression that specifies the entities to delete.\npartition_name (optional)\tName of the partition to delete entities from.\\\n\\\n# Upsert Entities\nThis topic describes how to upsert entities in Milvus.  \nUpserting is a combination of insert and delete operations. In the context of a Milvus vector database, an upsert is a data-level operation that will overwrite an existing entity if a specified field already exists in a collection, and insert a new entity if the specified value doesn’t already exist.  \nThe following example upserts 3,000 rows of randomly generated data as the example data. When performing upsert operations, it\'s important to note that the operation may compromise performance. This is because the operation involves deleting data during execution.', metadata={'id': 448986959321277980, 'distance': 0.680284857749939}),
     Document(page_content='# Upsert Entities\n## Flush data\nWhen data is upserted into Milvus it is updated and inserted into segments. Segments have to reach a certain size to be sealed and indexed. Unsealed segments will be searched brute force. In order to avoid this with any remainder data, it is best to call flush(). The flush() call will seal any remaining segments and send them for indexing. It is important to only call this method at the end of an upsert session. Calling it too often will cause fragmented data that will need to be cleaned later on.\\\n\\\n# Upsert Entities\n## Limits\nUpdating primary key fields is not supported by upsert().\nupsert() is not applicable and an error can occur if autoID is set to True for primary key fields.', metadata={'id': 448986959321277983, 'distance': 0.5672488212585449}),
     Document(page_content='# Upsert Entities\n## Prepare data\nFirst, prepare the data to upsert. The type of data to upsert must match the schema of the collection, otherwise Milvus will raise an exception.  \nMilvus supports default values for scalar fields, excluding a primary key field. This indicates that some fields can be left empty during data inserts or upserts. For more information, refer to Create a Collection.  \n```python\n# Generate data to upsert\n\nimport random\nnb = 3000\ndim = 8\nvectors = [[random.random() for _ in range(dim)] for _ in range(nb)]\ndata = [\n[i for i in range(nb)],\n[str(i) for i in range(nb)],\n[i for i in range(10000, 10000+nb)],\nvectors,\n[str("dy"*i) for i in range(nb)]\n]\n```', metadata={'id': 448986959321277981, 'distance': 0.5107149481773376}),
     Document(page_content='# Upsert Entities\n## Upsert data\nUpsert the data to the collection.  \n```python\nfrom pymilvus import Collection\ncollection = Collection("book") # Get an existing collection.\nmr = collection.upsert(data)\n```  \nParameter\tDescription\ndata\tData to upsert into Milvus.\npartition_name (optional)\tName of the partition to upsert data into.\ntimeout (optional)\tAn optional duration of time in seconds to allow for the RPC. If it is set to None, the client keeps waiting until the server responds or error occurs.\nAfter upserting entities into a collection that has previously been indexed, you do not need to re-index the collection, as Milvus will automatically create an index for the newly upserted data. For more information, refer to Can indexes be created after inserting vectors?', metadata={'id': 448986959321277982, 'distance': 0.4341375529766083})]





